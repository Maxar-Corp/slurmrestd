diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.36/jobs.c slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.36/jobs.c
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.36/jobs.c	2022-05-04 21:32:38.000000000 +0200
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.36/jobs.c	2023-01-17 15:46:57.572861418 +0100
@@ -413,8 +413,7 @@
 	data_t *errors = populate_response_format(resp);
 	char *jobid;
 	slurmdb_job_cond_t job_cond = {
-		.flags = (JOBCOND_FLAG_DUP | JOBCOND_FLAG_NO_TRUNC |
-			  JOBCOND_FLAG_WHOLE_HETJOB),
+		.flags = (JOBCOND_FLAG_DUP | JOBCOND_FLAG_NO_TRUNC),
 		.db_flags = SLURMDB_JOB_FLAG_NOTSET,
 	};
 
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.36/parse.c slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.36/parse.c
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.36/parse.c	2022-05-04 21:32:38.000000000 +0200
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.36/parse.c	2023-01-17 15:45:41.119398825 +0100
@@ -358,7 +358,7 @@
 	_add_parse(UINT32, resvid, "reservation/id"),
 	_add_parse(UINT32, resv_name, "reservation/name"),
 	/* skipping show_full */
-	_add_parse(UINT32, eligible, "time/start"),
+	_add_parse(UINT32, start, "time/start"),
 	_add_parse(JOB_STATE, state, "state/current"),
 	_add_parse(JOB_REASON, state_reason_prev, "state/previous"),
 	_add_parse(UINT32, submit, "time/submission"),
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/accounts.c slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/accounts.c
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/accounts.c	1970-01-01 01:00:00.000000000 +0100
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/accounts.c	2022-11-25 15:06:09.223023551 +0100
@@ -0,0 +1,362 @@
+/*****************************************************************************\
+ *  accounts.c - Slurm REST API accounting accounts http operations handlers
+ *****************************************************************************
+ *  Copyright (C) 2020 SchedMD LLC.
+ *  Written by Nathan Rini <nate@schedmd.com>
+ *
+ *  This file is part of Slurm, a resource management program.
+ *  For details, see <https://slurm.schedmd.com/>.
+ *  Please also read the included file: DISCLAIMER.
+ *
+ *  Slurm is free software; you can redistribute it and/or modify it under
+ *  the terms of the GNU General Public License as published by the Free
+ *  Software Foundation; either version 2 of the License, or (at your option)
+ *  any later version.
+ *
+ *  In addition, as a special exception, the copyright holders give permission
+ *  to link the code of portions of this program with the OpenSSL library under
+ *  certain conditions as described in each individual source file, and
+ *  distribute linked combinations including the two. You must obey the GNU
+ *  General Public License in all respects for all of the code used other than
+ *  OpenSSL. If you modify file(s) with this exception, you may extend this
+ *  exception to your version of the file(s), but you are not obligated to do
+ *  so. If you do not wish to do so, delete this exception statement from your
+ *  version.  If you delete this exception statement from all source files in
+ *  the program, then also delete it here.
+ *
+ *  Slurm is distributed in the hope that it will be useful, but WITHOUT ANY
+ *  WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+ *  FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
+ *  details.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with Slurm; if not, write to the Free Software Foundation, Inc.,
+ *  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301  USA.
+\*****************************************************************************/
+
+#include "config.h"
+
+#include <stdint.h>
+
+#include "slurm/slurm.h"
+#include "slurm/slurmdb.h"
+
+#include "src/common/list.h"
+#include "src/common/log.h"
+#include "src/slurmrestd/openapi.h"
+#include "src/common/parse_time.h"
+#include "src/common/ref.h"
+#include "src/common/slurmdbd_defs.h"
+#include "src/common/slurm_protocol_api.h"
+#include "src/common/strlcpy.h"
+#include "src/common/uid.h"
+#include "src/common/xassert.h"
+#include "src/common/xmalloc.h"
+#include "src/common/xstring.h"
+
+#include "src/slurmrestd/operations.h"
+
+#include "src/slurmrestd/plugins/openapi/dbv0.0.38/api.h"
+
+#define MAGIC_FOREACH_ACCOUNT 0xaefefef0
+typedef struct {
+	int magic;
+	data_t *accts;
+	List tres_list;
+	List qos_list;
+} foreach_account_t;
+
+typedef struct {
+	data_t *errors;
+	slurmdb_account_cond_t *account_cond;
+} foreach_query_search_t;
+
+/* Change the account search conditions based on input parameters */
+static data_for_each_cmd_t _foreach_query_search(const char *key,
+						 data_t *data,
+						 void *arg)
+{
+	foreach_query_search_t *args = arg;
+	data_t *errors = args->errors;
+
+	if (!xstrcasecmp("with_deleted", key)) {
+		if (data_convert_type(data, DATA_TYPE_BOOL) != DATA_TYPE_BOOL) {
+			resp_error(errors, ESLURM_REST_INVALID_QUERY,
+				   "must be a Boolean", NULL);
+			return DATA_FOR_EACH_FAIL;
+		}
+
+		if (data->data.bool_u)
+			args->account_cond->with_deleted = true;
+		else
+			args->account_cond->with_deleted = false;
+
+		return DATA_FOR_EACH_CONT;
+	}
+
+	resp_error(errors, ESLURM_REST_INVALID_QUERY, "Unknown query field",
+		   NULL);
+	return DATA_FOR_EACH_FAIL;
+}
+
+static int _parse_other_params(data_t *query,
+			       slurmdb_account_cond_t *cond,
+			       data_t *errors)
+{
+	if (!query || !data_get_dict_length(query))
+		return SLURM_SUCCESS;
+
+	foreach_query_search_t args = {
+		.errors = errors,
+		.account_cond = cond,
+	};
+
+	if (data_dict_for_each(query, _foreach_query_search, &args) < 0)
+		return SLURM_ERROR;
+	else
+		return SLURM_SUCCESS;
+}
+
+static int _foreach_account(void *x, void *arg)
+{
+	parser_env_t penv = { 0 };
+
+	slurmdb_account_rec_t *acct = x;
+	foreach_account_t *args = arg;
+
+	xassert(args->magic == MAGIC_FOREACH_ACCOUNT);
+
+	if (dump(PARSE_ACCOUNT, acct,
+		 data_set_dict(data_list_append(args->accts)), &penv))
+		return DATA_FOR_EACH_FAIL;
+	else
+		return DATA_FOR_EACH_CONT;
+}
+
+/* based on sacctmgr_list_account() */
+static int _dump_accounts(data_t *resp, void *auth,
+			  slurmdb_account_cond_t *acct_cond)
+{
+	int rc = SLURM_SUCCESS;
+	data_t *errors = populate_response_format(resp);
+	slurmdb_qos_cond_t qos_cond = {
+		.with_deleted = 1,
+	};
+	slurmdb_tres_cond_t tres_cond = {
+		.with_deleted = 1,
+	};
+	foreach_account_t args = {
+		.magic = MAGIC_FOREACH_ACCOUNT,
+		.accts = data_set_list(data_key_set(resp, "accounts")),
+	};
+	List acct_list = NULL;
+
+	if (!(rc = db_query_list(errors, auth, &args.tres_list,
+				 slurmdb_tres_get, &tres_cond)) &&
+	    !(rc = db_query_list(errors, auth, &args.qos_list, slurmdb_qos_get,
+				 &qos_cond)) &&
+	    !(rc = db_query_list(errors, auth, &acct_list, slurmdb_accounts_get,
+				 acct_cond)) &&
+	    (list_for_each(acct_list, _foreach_account, &args) < 0))
+		rc = ESLURM_REST_INVALID_QUERY;
+
+	FREE_NULL_LIST(acct_list);
+	FREE_NULL_LIST(args.tres_list);
+	FREE_NULL_LIST(args.qos_list);
+
+	return rc;
+}
+
+#define MAGIC_FOREACH_UP_ACCT 0xefad1a19
+typedef struct {
+	int magic;
+	List acct_list;
+	data_t *errors;
+	rest_auth_context_t *auth;
+} foreach_update_acct_t;
+
+static data_for_each_cmd_t _foreach_update_acct(data_t *data, void *arg)
+{
+	foreach_update_acct_t *args = arg;
+	data_t *errors = args->errors;
+	slurmdb_account_rec_t *acct;
+	parser_env_t penv = {
+		.auth = args->auth,
+	};
+
+	xassert(args->magic == MAGIC_FOREACH_UP_ACCT);
+
+	if (data_get_type(data) != DATA_TYPE_DICT) {
+		resp_error(errors, ESLURM_REST_INVALID_QUERY,
+			   "each account entry must be a dictionary", NULL);
+		return DATA_FOR_EACH_FAIL;
+	}
+
+	acct = xmalloc(sizeof(slurmdb_account_rec_t));
+	acct->assoc_list = list_create(slurmdb_destroy_assoc_rec);
+	acct->coordinators = list_create(slurmdb_destroy_coord_rec);
+
+	if (parse(PARSE_ACCOUNT, acct, data, args->errors, &penv)) {
+		slurmdb_destroy_account_rec(acct);
+		return DATA_FOR_EACH_FAIL;
+	} else {
+		/* sacctmgr will set the org/desc as name if NULL */
+		if (!acct->organization)
+			acct->organization = xstrdup(acct->name);
+		if (!acct->description)
+			acct->description = xstrdup(acct->name);
+
+		(void)list_append(args->acct_list, acct);
+		return DATA_FOR_EACH_CONT;
+	}
+}
+
+static int _update_accts(data_t *query, data_t *resp, void *auth,
+			 bool commit)
+{
+	int rc = SLURM_SUCCESS;
+	data_t *errors = populate_response_format(resp);
+	foreach_update_acct_t args = {
+		.magic = MAGIC_FOREACH_UP_ACCT,
+		.auth = auth,
+		.errors = errors,
+		.acct_list = list_create(slurmdb_destroy_account_rec),
+	};
+	data_t *daccts = get_query_key_list("accounts", errors, query);
+
+	if (daccts &&
+	    (data_list_for_each(daccts, _foreach_update_acct, &args) < 0))
+		rc = ESLURM_REST_INVALID_QUERY;
+
+	if (!rc &&
+	    !(rc = db_query_rc(errors, auth, args.acct_list,
+			       slurmdb_accounts_add)) &&
+	    commit)
+		rc = db_query_commit(errors, auth);
+
+	FREE_NULL_LIST(args.acct_list);
+
+	return rc;
+}
+
+static int _foreach_delete_acct(void *x, void *arg)
+{
+	char *acct = x;
+	data_t *accts = arg;
+
+	data_set_string(data_list_append(accts), acct);
+
+	return DATA_FOR_EACH_CONT;
+}
+
+static int _delete_account(data_t *resp, void *auth, char *account)
+{
+	int rc = SLURM_SUCCESS;
+	data_t *errors = populate_response_format(resp);
+	List removed = NULL;
+	slurmdb_assoc_cond_t assoc_cond = {
+		.acct_list = list_create(NULL),
+		.user_list = list_create(NULL),
+	};
+	slurmdb_account_cond_t acct_cond = {
+		.assoc_cond = &assoc_cond,
+	};
+
+	list_append(assoc_cond.acct_list, account);
+
+	if (!db_query_list(errors, auth, &removed, slurmdb_accounts_remove,
+			   &acct_cond) &&
+	    (list_for_each(removed, _foreach_delete_acct,
+			   data_set_list(data_key_set(
+				   resp, "removed_associations"))) < 0))
+		resp_error(errors, ESLURM_REST_INVALID_QUERY,
+			   "unable to delete accounts", NULL);
+
+	if (!rc)
+		rc = db_query_commit(errors, auth);
+
+	FREE_NULL_LIST(removed);
+	FREE_NULL_LIST(assoc_cond.acct_list);
+	FREE_NULL_LIST(assoc_cond.user_list);
+
+	return rc;
+}
+
+extern int op_handler_account(const char *context_id,
+			      http_request_method_t method,
+			      data_t *parameters, data_t *query, int tag,
+			      data_t *resp, rest_auth_context_t *auth)
+{
+	int rc = SLURM_SUCCESS;
+	data_t *errors = populate_response_format(resp);
+	char *acct = get_str_param("account_name", errors, parameters);
+
+	if (!acct) {
+		/* no-op */;
+	} else if (method == HTTP_REQUEST_GET) {
+		slurmdb_assoc_cond_t assoc_cond = {
+			.acct_list = list_create(NULL),
+		};
+		slurmdb_account_cond_t acct_cond = {
+			.assoc_cond = &assoc_cond,
+			.with_assocs = true,
+			.with_coords = true,
+			/* with_deleted defaults to false */
+		};
+
+		list_append(assoc_cond.acct_list, acct);
+
+		/* Change search conditions based on parameters */
+		if (_parse_other_params(query, &acct_cond, errors) !=
+		    SLURM_SUCCESS)
+			rc = ESLURM_REST_INVALID_QUERY;
+		else
+			rc = _dump_accounts(resp, auth, &acct_cond);
+
+		FREE_NULL_LIST(assoc_cond.acct_list);
+
+		return rc;
+	} else if (method == HTTP_REQUEST_DELETE) {
+		return _delete_account(resp, auth, acct);
+	}
+
+	return ESLURM_REST_INVALID_QUERY;
+}
+
+/* based on sacctmgr_list_account() */
+extern int op_handler_accounts(const char *context_id,
+			       http_request_method_t method, data_t *parameters,
+			       data_t *query, int tag, data_t *resp, rest_auth_context_t *auth)
+{
+	data_t *errors = populate_response_format(resp);
+	if (method == HTTP_REQUEST_GET) {
+		slurmdb_account_cond_t acct_cond = {
+			.with_assocs = true,
+			.with_coords = true,
+			/* with_deleted defaults to false */
+		};
+
+		/* Change search conditions based on parameters */
+		_parse_other_params(query, &acct_cond, errors);
+
+		return _dump_accounts(resp, auth, &acct_cond);
+	} else if (method == HTTP_REQUEST_POST) {
+		return _update_accts(query, resp, auth, (tag != CONFIG_OP_TAG));
+	}
+
+	return ESLURM_REST_INVALID_QUERY;
+}
+
+extern void init_op_accounts(void)
+{
+	bind_operation_handler("/slurmdb/v0.0.38/accounts/",
+			       op_handler_accounts, 0);
+	bind_operation_handler("/slurmdb/v0.0.38/account/{account_name}/",
+			       op_handler_account, 0);
+}
+
+extern void destroy_op_accounts(void)
+{
+	unbind_operation_handler(op_handler_accounts);
+	unbind_operation_handler(op_handler_account);
+}
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/api.c slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/api.c
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/api.c	1970-01-01 01:00:00.000000000 +0100
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/api.c	2022-11-25 15:06:09.223023551 +0100
@@ -0,0 +1,333 @@
+/*****************************************************************************\
+ *  api.c - Slurm REST API openapi operations handlers
+ *****************************************************************************
+ *  Copyright (C) 2019-2020 SchedMD LLC.
+ *  Written by Nathan Rini <nate@schedmd.com>
+ *
+ *  This file is part of Slurm, a resource management program.
+ *  For details, see <https://slurm.schedmd.com/>.
+ *  Please also read the included file: DISCLAIMER.
+ *
+ *  Slurm is free software; you can redistribute it and/or modify it under
+ *  the terms of the GNU General Public License as published by the Free
+ *  Software Foundation; either version 2 of the License, or (at your option)
+ *  any later version.
+ *
+ *  In addition, as a special exception, the copyright holders give permission
+ *  to link the code of portions of this program with the OpenSSL library under
+ *  certain conditions as described in each individual source file, and
+ *  distribute linked combinations including the two. You must obey the GNU
+ *  General Public License in all respects for all of the code used other than
+ *  OpenSSL. If you modify file(s) with this exception, you may extend this
+ *  exception to your version of the file(s), but you are not obligated to do
+ *  so. If you do not wish to do so, delete this exception statement from your
+ *  version.  If you delete this exception statement from all source files in
+ *  the program, then also delete it here.
+ *
+ *  Slurm is distributed in the hope that it will be useful, but WITHOUT ANY
+ *  WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+ *  FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
+ *  details.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with Slurm; if not, write to the Free Software Foundation, Inc.,
+ *  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301  USA.
+\*****************************************************************************/
+
+#include "config.h"
+
+#include <math.h>
+#include <unistd.h>
+
+#include "slurm/slurm.h"
+
+#include "src/common/data.h"
+#include "src/common/log.h"
+#include "src/common/read_config.h"
+#include "src/common/ref.h"
+#include "src/common/slurm_protocol_api.h"
+#include "src/common/uid.h"
+#include "src/common/xassert.h"
+#include "src/common/xmalloc.h"
+#include "src/common/xstring.h"
+
+#include "src/slurmrestd/openapi.h"
+#include "src/slurmrestd/operations.h"
+#include "src/slurmrestd/xjson.h"
+
+#include "src/slurmrestd/plugins/openapi/dbv0.0.38/api.h"
+
+/*
+ * These variables are required by the generic plugin interface.  If they
+ * are not found in the plugin, the plugin loader will ignore it.
+ *
+ * plugin_name - a string giving a human-readable description of the
+ * plugin.  There is no maximum length, but the symbol must refer to
+ * a valid string.
+ *
+ * plugin_type - a string suggesting the type of the plugin or its
+ * applicability to a particular form of data or method of data handling.
+ * If the low-level plugin API is used, the contents of this string are
+ * unimportant and may be anything.  Slurm uses the higher-level plugin
+ * interface which requires this string to be of the form
+ *
+ *	<application>/<method>
+ *
+ * where <application> is a description of the intended application of
+ * the plugin (e.g., "select" for Slurm node selection) and <method>
+ * is a description of how this plugin satisfies that application.  Slurm will
+ * only load select plugins if the plugin_type string has a
+ * prefix of "select/".
+ *
+ * plugin_version - an unsigned 32-bit integer containing the Slurm version
+ * (major.minor.micro combined into a single number).
+ */
+const char plugin_name[] = "Slurm OpenAPI DB v0.0.38";
+const char plugin_type[] = "openapi/dbv0.0.38";
+const uint32_t plugin_id = 102;
+const uint32_t plugin_version = SLURM_VERSION_NUMBER;
+
+decl_static_data(openapi_json);
+
+extern data_t *populate_response_format(data_t *resp)
+{
+	data_t *plugin, *slurm, *slurmv, *meta;
+
+	if (data_get_type(resp) != DATA_TYPE_NULL) {
+		xassert(data_get_type(resp) == DATA_TYPE_DICT);
+		return data_key_get(resp, "errors");
+	}
+
+	data_set_dict(resp);
+
+	meta = data_set_dict(data_key_set(resp, "meta"));
+	plugin = data_set_dict(data_key_set(meta, "plugin"));
+	slurm = data_set_dict(data_key_set(meta, "Slurm"));
+	slurmv = data_set_dict(data_key_set(slurm, "version"));
+
+	data_set_string(data_key_set(slurm, "release"), SLURM_VERSION_STRING);
+	(void) data_convert_type(data_set_string(data_key_set(slurmv, "major"),
+						 SLURM_MAJOR),
+				 DATA_TYPE_INT_64);
+	(void) data_convert_type(data_set_string(data_key_set(slurmv, "micro"),
+						 SLURM_MICRO),
+				 DATA_TYPE_INT_64);
+	(void) data_convert_type(data_set_string(data_key_set(slurmv, "minor"),
+						 SLURM_MINOR),
+				 DATA_TYPE_INT_64);
+
+	data_set_string(data_key_set(plugin, "type"), plugin_type);
+	data_set_string(data_key_set(plugin, "name"), plugin_name);
+
+	return data_set_list(data_key_set(resp, "errors"));
+}
+
+extern int resp_error(data_t *errors, int error_code, const char *why,
+		      const char *source)
+{
+	data_t *e = data_set_dict(data_list_append(errors));
+
+	if (why)
+		data_set_string(data_key_set(e, "description"), why);
+
+	if (error_code) {
+		data_set_int(data_key_set(e, "error_number"), error_code);
+		data_set_string(data_key_set(e, "error"),
+				slurm_strerror(error_code));
+	}
+
+	if (source)
+		data_set_string(data_key_set(e, "source"), source);
+
+	return error_code;
+}
+
+extern int db_query_list_funcname(data_t *errors, rest_auth_context_t *auth,
+				  List *list, db_list_query_func_t func,
+				  void *cond, const char *func_name)
+{
+	List l;
+	void *db_conn;
+
+	xassert(!*list);
+	xassert(auth);
+	xassert(errors);
+
+	errno = 0;
+	if (!(db_conn = rest_auth_g_get_db_conn(auth))) {
+		return resp_error(errors, ESLURM_DB_CONNECTION,
+				  "Failed connecting to slurmdbd", func_name);
+	}
+
+	l = func(db_conn, cond);
+
+	if (errno) {
+		FREE_NULL_LIST(l);
+		return resp_error(errors, errno, NULL, func_name);
+	} else if (!l) {
+		return resp_error(errors, ESLURM_REST_INVALID_QUERY,
+				  "Unknown error with query", func_name);
+	} else if (!list_count(l)) {
+		FREE_NULL_LIST(l);
+		return resp_error(errors, ESLURM_REST_EMPTY_RESULT,
+				  "Nothing found", func_name);
+	}
+
+	*list = l;
+	return SLURM_SUCCESS;
+}
+
+extern int db_query_rc_funcname(data_t *errors,
+				rest_auth_context_t *auth, List list,
+				db_rc_query_func_t func,
+				const char *func_name)
+{
+	int rc;
+	void *db_conn;
+
+	if (!(db_conn = rest_auth_g_get_db_conn(auth))) {
+		return resp_error(errors, ESLURM_DB_CONNECTION,
+				  "Failed connecting to slurmdbd", func_name);
+	}
+
+	rc = func(db_conn, list);
+
+	if (rc)
+		return resp_error(errors, rc, NULL, func_name);
+
+	return rc;
+}
+
+extern int db_modify_rc_funcname(data_t *errors, rest_auth_context_t *auth,
+				 void *cond, void *obj,
+				 db_rc_modify_func_t func,
+				 const char *func_name)
+{
+	List changed;
+	int rc = SLURM_SUCCESS;
+	void *db_conn;
+
+	if (!(db_conn = rest_auth_g_get_db_conn(auth))) {
+		return resp_error(errors, ESLURM_DB_CONNECTION,
+				  "Failed connecting to slurmdbd", func_name);
+	}
+
+	errno = 0;
+	if (!(changed = func(db_conn, cond, obj))) {
+		if (errno)
+			rc = errno;
+		else
+			rc = SLURM_ERROR;
+
+		return resp_error(errors, rc, NULL, func_name);
+	}
+
+	FREE_NULL_LIST(changed);
+
+	return rc;
+}
+
+extern int db_query_commit(data_t *errors, rest_auth_context_t *auth)
+{
+	int rc;
+	void *db_conn;
+
+	if (!(db_conn = rest_auth_g_get_db_conn(auth))) {
+		return resp_error(errors, ESLURM_DB_CONNECTION,
+				  "Failed connecting to slurmdbd",
+				  __func__);
+	}
+	rc = slurmdb_connection_commit(db_conn, true);
+
+	if (rc)
+		return resp_error(errors, rc, NULL,
+				  "slurmdb_connection_commit");
+
+	return rc;
+}
+
+extern char *get_str_param(const char *path, data_t *errors, data_t *parameters)
+{
+	char *str = NULL;
+	data_t *dbuf;
+
+	if (!parameters) {
+		resp_error(errors, ESLURM_REST_INVALID_QUERY,
+			   "No parameters provided", "HTTP parameters");
+	} else if (!(dbuf = data_key_get(parameters, path))) {
+		resp_error(errors, ESLURM_REST_INVALID_QUERY,
+			   "Parameter not found", path);
+	} else if (data_convert_type(dbuf, DATA_TYPE_STRING) !=
+		   DATA_TYPE_STRING) {
+		resp_error(errors, ESLURM_DATA_CONV_FAILED,
+			   "Parameter incorrect format", path);
+	} else if (!(str = data_get_string(dbuf)) || !str[0]) {
+		resp_error(errors, ESLURM_REST_EMPTY_RESULT, "Parameter empty",
+			   path);
+		str = NULL;
+	}
+
+	return str;
+}
+
+extern data_t * get_query_key_list(const char *path, data_t *errors,
+				   data_t *query)
+{
+	data_t *dst = NULL;
+
+	if (!query)
+		resp_error(errors, ESLURM_REST_INVALID_QUERY,
+			   "No query provided", "HTTP query");
+	else if (!(dst = data_key_get(query, path)))
+		resp_error(errors, ESLURM_DATA_PATH_NOT_FOUND,
+			   "Query parameter not found", path);
+	else if (data_get_type(dst) != DATA_TYPE_LIST) {
+		resp_error(errors, ESLURM_DATA_PATH_NOT_FOUND,
+			   "Query parameter must be a list", path);
+		dst = NULL;
+	}
+
+	return dst;
+}
+
+const data_t *slurm_openapi_p_get_specification(void)
+{
+	data_t *spec = NULL;
+
+	static_ref_json_to_data_t(spec, openapi_json);
+
+	return spec;
+}
+
+extern void slurm_openapi_p_init(void)
+{
+	/* Check to see if we are running a supported accounting plugin */
+	if (!slurm_with_slurmdbd()) {
+		fatal("%s: slurm not configured with slurmdbd", __func__);
+	}
+
+	init_op_accounts();
+	init_op_associations();
+	init_op_config();
+	init_op_cluster();
+	init_op_diag();
+	init_op_job();
+	init_op_qos();
+	init_op_tres();
+	init_op_users();
+	init_op_wckeys();
+}
+
+extern void slurm_openapi_p_fini(void)
+{
+	destroy_op_accounts();
+	destroy_op_associations();
+	destroy_op_cluster();
+	destroy_op_config();
+	destroy_op_diag();
+	destroy_op_job();
+	destroy_op_qos();
+	destroy_op_tres();
+	destroy_op_users();
+	destroy_op_wckeys();
+}
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/api.h slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/api.h
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/api.h	1970-01-01 01:00:00.000000000 +0100
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/api.h	2022-11-25 13:53:44.201124009 +0100
@@ -0,0 +1,244 @@
+/*****************************************************************************\
+ *  api.h - Slurm REST API openapi operations handlers
+ *****************************************************************************
+ *  Copyright (C) 2019-2020 SchedMD LLC.
+ *  Written by Nathan Rini <nate@schedmd.com>
+ *
+ *  This file is part of Slurm, a resource management program.
+ *  For details, see <https://slurm.schedmd.com/>.
+ *  Please also read the included file: DISCLAIMER.
+ *
+ *  Slurm is free software; you can redistribute it and/or modify it under
+ *  the terms of the GNU General Public License as published by the Free
+ *  Software Foundation; either version 2 of the License, or (at your option)
+ *  any later version.
+ *
+ *  In addition, as a special exception, the copyright holders give permission
+ *  to link the code of portions of this program with the OpenSSL library under
+ *  certain conditions as described in each individual source file, and
+ *  distribute linked combinations including the two. You must obey the GNU
+ *  General Public License in all respects for all of the code used other than
+ *  OpenSSL. If you modify file(s) with this exception, you may extend this
+ *  exception to your version of the file(s), but you are not obligated to do
+ *  so. If you do not wish to do so, delete this exception statement from your
+ *  version.  If you delete this exception statement from all source files in
+ *  the program, then also delete it here.
+ *
+ *  Slurm is distributed in the hope that it will be useful, but WITHOUT ANY
+ *  WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+ *  FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
+ *  details.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with Slurm; if not, write to the Free Software Foundation, Inc.,
+ *  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301  USA.
+\*****************************************************************************/
+
+#ifndef SLURMRESTD_OPENAPI_DB_V0038
+#define SLURMRESTD_OPENAPI_DB_V0038
+
+#include "config.h"
+
+#include "slurm/slurm.h"
+#include "slurm/slurmdb.h"
+
+#include "src/common/data.h"
+
+#include "src/slurmrestd/operations.h"
+
+#include "src/slurmrestd/plugins/openapi/dbv0.0.38/parse.h"
+
+#define CONFIG_OP_TAG 0xfffffffe
+
+/*
+ * Fill out boilerplate for every data response
+ * RET ptr to errors dict
+ */
+extern data_t *populate_response_format(data_t *resp);
+
+/*
+ * Add a response error to errors
+ * IN errors - data list to append a new error
+ * IN why - description of error or NULL
+ * IN error_code - Error number
+ * IN source - Where the error was generated
+ * RET value of error_code
+ */
+extern int resp_error(data_t *errors, int error_code, const char *why,
+		      const char *source);
+
+/* ------------ generic typedefs for slurmdbd queries --------------- */
+
+/* Generic typedef for the DB query functions that return a list */
+typedef List (*db_list_query_func_t)(void *db_conn, void *cond);
+/*
+ * Generic typedef for the DB query functions that takes a list and returns an
+ * rc if the query was successful.
+ */
+typedef int (*db_rc_query_func_t)(void *db_conn, List list);
+/*
+ * Generic typedef for the DB modify functions that takes an object record and
+ * returns an List if the query was successful or NULL on error
+ */
+typedef List (*db_rc_modify_func_t)(void *db_conn, void **cond, void *obj);
+
+/* ------------ handlers for slurmdbd queries --------------- */
+
+/*
+ * Macro helper for Query database API for List output.
+ * Converts the function name to string.
+ */
+#define db_query_list(errors, auth, list, func, cond)                        \
+	db_query_list_funcname(errors, auth, list, \
+			       (db_list_query_func_t)func, cond, #func)
+
+/*
+ * Query database API for List output
+ * IN errors - data list to append a new error
+ * IN auth - connection authentication attr
+ * IN/OUT list - ptr to List ptr to populate with result (on success)
+ * IN func - function ptr to call
+ * IN cond - conditional to pass to func
+ * IN func_name - string of func name (for errors)
+ * RET SLURM_SUCCESS or error
+ */
+extern int db_query_list_funcname(data_t *errors, rest_auth_context_t *auth,
+				  List *list, db_list_query_func_t func,
+				  void *cond, const char *func_name);
+
+/*
+ * Macro helper for Query database API for rc output.
+ * Converts the function name to string.
+ */
+#define db_query_rc(errors, auth, list, func) \
+	db_query_rc_funcname(errors, auth, list, (db_rc_query_func_t)func, \
+                             #func)
+
+/*
+ * Query database API for List output
+ * IN errors - data list to append a new error
+ * IN auth - connection authentication attr
+ * IN list - ptr to List to pass to func
+ * IN func - function ptr to call
+ * IN func_name - string of func name (for errors)
+ * RET SLURM_SUCCESS or error
+ */
+extern int db_query_rc_funcname(data_t *errors,
+				rest_auth_context_t *auth, List list,
+				db_rc_query_func_t func,
+				const char *func_name);
+
+/*
+ * Macro helper for modify database API for List output.
+ * Converts the function name to string.
+ */
+#define db_modify_rc(errors, auth, cond, obj, func)                  \
+	db_modify_rc_funcname(errors, auth, cond, obj,               \
+			      (db_rc_modify_func_t)func, #func)
+
+/*
+ * Modify object in database API
+ * IN errors - data list to append a new error
+ * IN auth - connection authentication attr
+ * IN cond - ptr to filter conditional to pass to func
+ * IN obj - ptr to obj to pass to func
+ * IN func - function ptr to call
+ * IN func_name - string of func name (for errors)
+ * RET SLURM_SUCCESS or error
+ */
+extern int db_modify_rc_funcname(data_t *errors, rest_auth_context_t *auth,
+				 void *cond, void *obj,
+				 db_rc_modify_func_t func,
+				 const char *func_name);
+
+/*
+ * Request database API to commit connection
+ * IN errors - data list to append a new error
+ * IN auth - connection authentication attr
+ * RET SLURM_SUCCESS or error
+ */
+extern int db_query_commit(data_t *errors, rest_auth_context_t *auth);
+
+/* ------------ handlers for user requests --------------- */
+
+/*
+ * Retrieve parameter
+ * IN path - Path to parameter in query
+ * IN errors - data list to append a new error
+ * IN parameters - paramets from http request
+ * RET string or NULL on error
+ */
+extern char *get_str_param(const char *path, data_t *errors,
+			   data_t *parameters);
+
+/*
+ * Retrieve List from query list
+ * IN path - Path to parameter in query
+ * IN errors - data list to append a new error
+ * IN query - query from http request
+ * RET List ptr or NULL on error
+ */
+extern data_t *get_query_key_list(const char *path, data_t *errors,
+				   data_t *query);
+
+/* ------------ declarations for each operation --------------- */
+
+extern void init_op_associations(void);
+extern void destroy_op_associations(void);
+extern int op_handler_associations(const char *context_id,
+				   http_request_method_t method,
+				   data_t *parameters, data_t *query, int tag,
+				   data_t *resp, rest_auth_context_t *auth);
+
+extern void init_op_accounts(void);
+extern void destroy_op_accounts(void);
+extern int op_handler_accounts(const char *context_id,
+			       http_request_method_t method, data_t *parameters,
+			       data_t *query, int tag, data_t *resp,
+			       rest_auth_context_t *auth);
+
+extern void init_op_cluster(void);
+extern void destroy_op_cluster(void);
+extern int op_handler_clusters(const char *context_id,
+			       http_request_method_t method, data_t *parameters,
+			       data_t *query, int tag, data_t *resp,
+			       rest_auth_context_t *auth);
+
+extern void init_op_config(void);
+extern void destroy_op_config(void);
+
+extern void init_op_diag(void);
+extern void destroy_op_diag(void);
+
+extern void init_op_job(void);
+extern void destroy_op_job(void);
+extern int op_handler_jobs(const char *context_id, http_request_method_t method,
+			   data_t *parameters, data_t *query, int tag,
+			   data_t *resp, rest_auth_context_t *auth);
+
+extern void init_op_tres(void);
+extern void destroy_op_tres(void);
+extern int op_handler_tres(const char *context_id, http_request_method_t method,
+			   data_t *parameters, data_t *query, int tag,
+			   data_t *resp, rest_auth_context_t *auth);
+
+extern void init_op_users(void);
+extern void destroy_op_users(void);
+extern int op_handler_users(const char *context_id,
+			    http_request_method_t method, data_t *parameters,
+			    data_t *query, int tag, data_t *resp, rest_auth_context_t *auth);
+
+extern void init_op_wckeys(void);
+extern void destroy_op_wckeys(void);
+extern int op_handler_wckeys(const char *context_id,
+			     http_request_method_t method,
+			     data_t *parameters, data_t *query, int tag,
+			     data_t *resp, rest_auth_context_t *auth);
+
+extern void init_op_qos(void);
+extern void destroy_op_qos(void);
+extern int op_handler_qos(const char *context_id, http_request_method_t method,
+			  data_t *parameters, data_t *query, int tag,
+			  data_t *resp, rest_auth_context_t *auth);
+
+#endif
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/associations.c slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/associations.c
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/associations.c	1970-01-01 01:00:00.000000000 +0100
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/associations.c	2022-11-25 15:06:09.223023551 +0100
@@ -0,0 +1,403 @@
+/*****************************************************************************\
+ *  associations.c - Slurm REST API acct associations http operations handlers
+ *****************************************************************************
+ *  Copyright (C) 2020 SchedMD LLC.
+ *  Written by Nathan Rini <nate@schedmd.com>
+ *
+ *  This file is part of Slurm, a resource management program.
+ *  For details, see <https://slurm.schedmd.com/>.
+ *  Please also read the included file: DISCLAIMER.
+ *
+ *  Slurm is free software; you can redistribute it and/or modify it under
+ *  the terms of the GNU General Public License as published by the Free
+ *  Software Foundation; either version 2 of the License, or (at your option)
+ *  any later version.
+ *
+ *  In addition, as a special exception, the copyright holders give permission
+ *  to link the code of portions of this program with the OpenSSL library under
+ *  certain conditions as described in each individual source file, and
+ *  distribute linked combinations including the two. You must obey the GNU
+ *  General Public License in all respects for all of the code used other than
+ *  OpenSSL. If you modify file(s) with this exception, you may extend this
+ *  exception to your version of the file(s), but you are not obligated to do
+ *  so. If you do not wish to do so, delete this exception statement from your
+ *  version.  If you delete this exception statement from all source files in
+ *  the program, then also delete it here.
+ *
+ *  Slurm is distributed in the hope that it will be useful, but WITHOUT ANY
+ *  WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+ *  FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
+ *  details.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with Slurm; if not, write to the Free Software Foundation, Inc.,
+ *  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301  USA.
+\*****************************************************************************/
+
+#include "config.h"
+
+#include <stdint.h>
+
+#include "slurm/slurm.h"
+#include "slurm/slurmdb.h"
+
+#include "src/common/list.h"
+#include "src/common/log.h"
+#include "src/common/parse_time.h"
+#include "src/common/ref.h"
+#include "src/common/slurm_protocol_api.h"
+#include "src/common/slurmdbd_defs.h"
+#include "src/common/strlcpy.h"
+#include "src/common/uid.h"
+#include "src/common/xassert.h"
+#include "src/common/xmalloc.h"
+#include "src/common/xstring.h"
+
+#include "src/slurmrestd/operations.h"
+
+#include "src/slurmrestd/plugins/openapi/dbv0.0.38/api.h"
+
+typedef struct {
+	size_t offset;
+	char *parameter;
+} assoc_parameter_t;
+
+static const assoc_parameter_t assoc_parameters[] = {
+	{
+		offsetof(slurmdb_assoc_cond_t, partition_list),
+		"partition"
+	},
+	{
+		offsetof(slurmdb_assoc_cond_t, cluster_list),
+		"cluster"
+	},
+	{
+		offsetof(slurmdb_assoc_cond_t, acct_list),
+		"account"
+	},
+	{
+		offsetof(slurmdb_assoc_cond_t, user_list),
+		"user"
+	},
+};
+
+static int _populate_assoc_cond(data_t *errors, data_t *query,
+				slurmdb_assoc_cond_t *assoc_cond)
+{
+	if (!query)
+		return SLURM_SUCCESS;
+
+	for (int i = 0; i < ARRAY_SIZE(assoc_parameters); i++) {
+		char *value = NULL;
+		const assoc_parameter_t *ap = &assoc_parameters[i];
+		List *list = ((void *) assoc_cond) + ap->offset;
+		int rc = data_retrieve_dict_path_string(query, ap->parameter,
+							&value);
+
+		if (rc == ESLURM_DATA_PATH_NOT_FOUND) {
+			/* parameter not in query */
+			continue;
+		} else if (rc) {
+			char *err = xstrdup_printf("Invalid format for query parameter %s",
+						   ap->parameter);
+			rc = resp_error(errors, rc, err, "HTTP query");
+			xfree(err);
+			return rc;
+		}
+
+		*list = list_create(xfree_ptr);
+		(void) slurm_addto_char_list(*list, value);
+
+		xfree(value);
+	}
+
+	return SLURM_SUCCESS;
+}
+
+static int _foreach_delete_assoc(void *x, void *arg)
+{
+	char *assoc = x;
+	data_t *assocs = arg;
+
+	data_set_string(data_list_append(assocs), assoc);
+
+	return DATA_FOR_EACH_CONT;
+}
+
+static int _dump_assoc_cond(data_t *resp, void *auth, data_t *errors,
+			    slurmdb_assoc_cond_t *cond, bool only_one)
+{
+	int rc = SLURM_SUCCESS;
+	List assoc_list = NULL;
+	List tres_list = NULL;
+	List qos_list = NULL;
+	slurmdb_qos_cond_t qos_cond = {
+		.with_deleted = 1,
+	};
+	slurmdb_tres_cond_t tres_cond = {
+		.with_deleted = 1,
+	};
+
+	if (!(rc = db_query_list(errors, auth, &assoc_list,
+				 slurmdb_associations_get, cond)) &&
+	    !(rc = db_query_list(errors, auth, &tres_list, slurmdb_tres_get,
+				 &tres_cond)) &&
+	    !(rc = db_query_list(errors, auth, &qos_list, slurmdb_qos_get,
+				 &qos_cond))) {
+		ListIterator itr = list_iterator_create(assoc_list);
+		data_t *dassocs = data_set_list(
+			data_key_set(resp, "associations"));
+		slurmdb_assoc_rec_t *assoc;
+		parser_env_t penv = {
+			.g_tres_list = tres_list,
+			.g_qos_list = qos_list,
+			.g_assoc_list = assoc_list,
+		};
+
+		if (only_one && list_count(assoc_list) > 1) {
+			rc = resp_error(
+				errors, ESLURM_REST_INVALID_QUERY,
+				"Ambiguous request: More than 1 association would have been dumped.",
+				NULL);
+		}
+
+		while (!rc && (assoc = list_next(itr)))
+			rc = dump(PARSE_ASSOC, assoc,
+				  data_set_dict(data_list_append(dassocs)),
+				  &penv);
+
+		list_iterator_destroy(itr);
+	}
+
+	FREE_NULL_LIST(assoc_list);
+	FREE_NULL_LIST(tres_list);
+	FREE_NULL_LIST(qos_list);
+
+	return rc;
+}
+
+static int _delete_assoc(data_t *resp, void *auth, data_t *errors,
+			 slurmdb_assoc_cond_t *assoc_cond, bool only_one)
+{
+	int rc = SLURM_SUCCESS;
+	List removed = NULL;
+	data_t *drem = data_set_list(data_key_set(resp, "removed_associations"));
+
+	rc = db_query_list(errors, auth, &removed, slurmdb_associations_remove,
+			   assoc_cond);
+	if (rc) {
+		(void) resp_error(errors, rc, "unable to query associations",
+				NULL);
+	} else if (only_one && list_count(removed) > 1) {
+		rc = resp_error(errors, ESLURM_REST_INVALID_QUERY,
+				"ambiguous request: More than 1 association would have been deleted.",
+				NULL);
+	} else if (list_for_each(removed, _foreach_delete_assoc, drem) < 0) {
+		rc = resp_error(errors, ESLURM_REST_INVALID_QUERY,
+				"unable to delete associations", NULL);
+	} else if (!rc) {
+		rc = db_query_commit(errors, auth);
+	}
+
+	FREE_NULL_LIST(removed);
+
+	return rc;
+}
+
+#define MAGIC_FOREACH_UP_ASSOC 0xbaed2a12
+typedef struct {
+	int magic;
+	List tres_list;
+	List qos_list;
+	data_t *errors;
+	rest_auth_context_t *auth;
+} foreach_update_assoc_t;
+
+static data_for_each_cmd_t _foreach_update_assoc(data_t *data, void *arg)
+{
+	foreach_update_assoc_t *args = arg;
+	data_t *errors = args->errors;
+	slurmdb_assoc_rec_t *assoc = NULL;
+	parser_env_t penv = {
+		.g_tres_list = args->tres_list,
+		.g_qos_list = args->qos_list,
+		.auth = args->auth,
+	};
+	int rc;
+	List assoc_list = NULL;
+	slurmdb_assoc_cond_t cond = {0};
+	data_t *query_errors = data_new();
+
+	xassert(args->magic == MAGIC_FOREACH_UP_ASSOC);
+
+	if (data_get_type(data) != DATA_TYPE_DICT) {
+		resp_error(errors, ESLURM_REST_INVALID_QUERY,
+			   "Associations must be a list of dictionaries", NULL);
+		rc = DATA_FOR_EACH_FAIL;
+		goto cleanup;
+	}
+
+	assoc = xmalloc(sizeof(*assoc));
+	slurmdb_init_assoc_rec(assoc, false);
+
+	if (parse(PARSE_ASSOC, assoc, data, args->errors, &penv)) {
+		rc = DATA_FOR_EACH_FAIL;
+		goto cleanup;
+	}
+
+	cond.acct_list = list_create(NULL);
+	cond.cluster_list = list_create(NULL);
+	cond.partition_list = list_create(NULL);
+	cond.user_list = list_create(NULL);
+
+	if (assoc->acct)
+		list_append(cond.acct_list, assoc->acct);
+	else
+		list_append(cond.acct_list, "");
+
+	if (assoc->cluster)
+		list_append(cond.cluster_list, assoc->cluster);
+	else
+		list_append(cond.cluster_list, "");
+
+	if (assoc->partition)
+		list_append(cond.partition_list, assoc->partition);
+	else
+		list_append(cond.partition_list, "");
+
+	if (assoc->user)
+		list_append(cond.user_list, assoc->user);
+	else
+		list_append(cond.user_list, "");
+
+	if ((rc = db_query_list(query_errors, args->auth, &assoc_list,
+				 slurmdb_associations_get, &cond)) ||
+	    list_is_empty(assoc_list)) {
+		FREE_NULL_LIST(assoc_list);
+		assoc_list = list_create(slurmdb_destroy_assoc_rec);
+		list_append(assoc_list, assoc);
+
+		debug("%s: adding association request: acct=%s cluster=%s partition=%s user=%s",
+		      __func__, assoc->acct, assoc->cluster, assoc->partition,
+		      assoc->user);
+
+		assoc = NULL;
+		rc = db_query_rc(errors, args->auth, assoc_list,
+				 slurmdb_associations_add);
+	} else if (list_count(assoc_list) > 1) {
+		rc = resp_error(errors, ESLURM_REST_INVALID_QUERY,
+				"ambiguous modify request",
+				"slurmdb_associations_get");
+	} else {
+		debug("%s: modifying association request: acct=%s cluster=%s partition=%s user=%s",
+		      __func__, assoc->acct, assoc->cluster, assoc->partition,
+		      assoc->user);
+
+		rc = db_modify_rc(errors, args->auth, &cond, assoc,
+				  slurmdb_associations_modify);
+	}
+
+cleanup:
+
+	FREE_NULL_LIST(assoc_list);
+	FREE_NULL_LIST(cond.acct_list);
+	FREE_NULL_LIST(cond.cluster_list);
+	FREE_NULL_LIST(cond.partition_list);
+	FREE_NULL_LIST(cond.user_list);
+	FREE_NULL_DATA(query_errors);
+	slurmdb_destroy_assoc_rec(assoc);
+
+	return rc ? DATA_FOR_EACH_FAIL : DATA_FOR_EACH_CONT;
+}
+
+static int _update_assocations(data_t *query, data_t *resp,
+			       void *auth, bool commit)
+{
+	int rc = SLURM_SUCCESS;
+	data_t *errors = populate_response_format(resp);
+	slurmdb_tres_cond_t tres_cond = {
+		.with_deleted = 1,
+	};
+	slurmdb_qos_cond_t qos_cond = {
+		.with_deleted = 1,
+	};
+	foreach_update_assoc_t args = {
+		.magic = MAGIC_FOREACH_UP_ASSOC,
+		.auth = auth,
+		.errors = errors,
+	};
+	data_t *dassoc = get_query_key_list("associations", errors, query);
+
+	if (dassoc &&
+	    !(rc = db_query_list(errors, auth, &args.tres_list,
+				 slurmdb_tres_get, &tres_cond)) &&
+	    !(rc = db_query_list(errors, auth, &args.qos_list, slurmdb_qos_get,
+				 &qos_cond)) &&
+	    (data_list_for_each(dassoc, _foreach_update_assoc, &args) < 0))
+		rc = ESLURM_REST_INVALID_QUERY;
+
+	if (!rc && commit)
+		rc = db_query_commit(errors, auth);
+
+	FREE_NULL_LIST(args.tres_list);
+	FREE_NULL_LIST(args.qos_list);
+
+	return rc;
+}
+
+static int op_handler_association(const char *context_id,
+				  http_request_method_t method,
+				  data_t *parameters, data_t *query, int tag,
+				  data_t *resp, rest_auth_context_t *auth)
+{
+	int rc;
+	data_t *errors = populate_response_format(resp);
+	slurmdb_assoc_cond_t *assoc_cond = xmalloc(sizeof(*assoc_cond));
+
+	if ((rc = _populate_assoc_cond(errors, query, assoc_cond)))
+		/* no-op - already logged */;
+	if (method == HTTP_REQUEST_GET)
+		rc = _dump_assoc_cond(resp, auth, errors, assoc_cond, true);
+	else if (method == HTTP_REQUEST_DELETE)
+		rc = _delete_assoc(resp, auth, errors, assoc_cond, true);
+
+	slurmdb_destroy_assoc_cond(assoc_cond);
+	return rc;
+}
+
+extern int op_handler_associations(const char *context_id,
+				   http_request_method_t method,
+				   data_t *parameters, data_t *query, int tag,
+				   data_t *resp, rest_auth_context_t *auth)
+{
+	int rc;
+	data_t *errors = populate_response_format(resp);
+	slurmdb_assoc_cond_t *assoc_cond = xmalloc(sizeof(*assoc_cond));
+
+	if ((rc = _populate_assoc_cond(errors, query, assoc_cond)))
+		/* no-op - already logged */;
+	if (method == HTTP_REQUEST_GET)
+		rc = _dump_assoc_cond(resp, auth, errors, assoc_cond, false);
+	else if (method == HTTP_REQUEST_POST)
+		rc = _update_assocations(query, resp, auth,
+					 (tag != CONFIG_OP_TAG));
+	else if (method == HTTP_REQUEST_DELETE)
+		rc = _delete_assoc(resp, auth, errors, assoc_cond, false);
+
+	slurmdb_destroy_assoc_cond(assoc_cond);
+	return rc;
+}
+
+extern void init_op_associations(void)
+{
+	bind_operation_handler("/slurmdb/v0.0.38/associations/",
+			       op_handler_associations, 0);
+	bind_operation_handler("/slurmdb/v0.0.38/association/",
+			       op_handler_association, 0);
+}
+
+extern void destroy_op_associations(void)
+{
+	unbind_operation_handler(op_handler_associations);
+	unbind_operation_handler(op_handler_association);
+}
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/cluster.c slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/cluster.c
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/cluster.c	1970-01-01 01:00:00.000000000 +0100
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/cluster.c	2022-11-25 15:06:09.223023551 +0100
@@ -0,0 +1,290 @@
+/*****************************************************************************\
+ *  cluster.c - Slurm REST API acct cluster http operations handlers
+ *****************************************************************************
+ *  Copyright (C) 2020 SchedMD LLC.
+ *  Written by Nathan Rini <nate@schedmd.com>
+ *
+ *  This file is part of Slurm, a resource management program.
+ *  For details, see <https://slurm.schedmd.com/>.
+ *  Please also read the included file: DISCLAIMER.
+ *
+ *  Slurm is free software; you can redistribute it and/or modify it under
+ *  the terms of the GNU General Public License as published by the Free
+ *  Software Foundation; either version 2 of the License, or (at your option)
+ *  any later version.
+ *
+ *  In addition, as a special exception, the copyright holders give permission
+ *  to link the code of portions of this program with the OpenSSL library under
+ *  certain conditions as described in each individual source file, and
+ *  distribute linked combinations including the two. You must obey the GNU
+ *  General Public License in all respects for all of the code used other than
+ *  OpenSSL. If you modify file(s) with this exception, you may extend this
+ *  exception to your version of the file(s), but you are not obligated to do
+ *  so. If you do not wish to do so, delete this exception statement from your
+ *  version.  If you delete this exception statement from all source files in
+ *  the program, then also delete it here.
+ *
+ *  Slurm is distributed in the hope that it will be useful, but WITHOUT ANY
+ *  WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+ *  FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
+ *  details.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with Slurm; if not, write to the Free Software Foundation, Inc.,
+ *  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301  USA.
+\*****************************************************************************/
+
+#include "config.h"
+
+#include <stdint.h>
+
+#include "slurm/slurm.h"
+#include "slurm/slurmdb.h"
+
+#include "src/common/list.h"
+#include "src/common/log.h"
+#include "src/common/parse_time.h"
+#include "src/common/ref.h"
+#include "src/common/slurm_protocol_api.h"
+#include "src/common/slurmdbd_defs.h"
+#include "src/common/strlcpy.h"
+#include "src/common/uid.h"
+#include "src/common/xassert.h"
+#include "src/common/xmalloc.h"
+#include "src/common/xstring.h"
+
+#include "src/slurmrestd/operations.h"
+
+#include "src/slurmrestd/plugins/openapi/dbv0.0.38/api.h"
+
+#define MAGIC_FOREACH_CLUSTER 0x2aa2faf2
+typedef struct {
+	int magic;
+	data_t *clusters;
+	List tres_list;
+} foreach_cluster_t;
+
+static int _foreach_cluster(void *x, void *arg)
+{
+	slurmdb_cluster_rec_t *cluster = x;
+	foreach_cluster_t *args = arg;
+	parser_env_t penv = {
+		.g_tres_list = args->tres_list,
+	};
+
+	xassert(args->magic == MAGIC_FOREACH_CLUSTER);
+
+	if (dump(PARSE_CLUSTER_REC, cluster,
+		 data_set_dict(data_list_append(args->clusters)), &penv))
+		return -1;
+
+	return 1;
+}
+
+static int _dump_clusters(data_t *resp, data_t *errors, char *cluster,
+			  void *auth)
+{
+	int rc = SLURM_SUCCESS;
+	slurmdb_tres_cond_t tres_cond = {
+		.with_deleted = 1,
+	};
+	slurmdb_cluster_cond_t cluster_cond = {
+		.cluster_list = list_create(NULL),
+		.with_deleted = true,
+		.with_usage = true,
+		.flags = NO_VAL,
+	};
+	foreach_cluster_t args = {
+		.magic = MAGIC_FOREACH_CLUSTER,
+		.clusters = data_set_list(data_key_set(resp, "clusters")),
+	};
+	List cluster_list = NULL;
+
+	if (cluster)
+		list_append(cluster_cond.cluster_list, cluster);
+
+	if (!(rc = db_query_list(errors, auth, &args.tres_list,
+				 slurmdb_tres_get, &tres_cond)) &&
+	    !(rc = db_query_list(errors, auth, &cluster_list,
+				 slurmdb_clusters_get, &cluster_cond)) &&
+	    (list_for_each(cluster_list, _foreach_cluster, &args) < 0))
+		rc = ESLURM_DATA_CONV_FAILED;
+
+	FREE_NULL_LIST(cluster_list);
+	FREE_NULL_LIST(cluster_cond.cluster_list);
+	FREE_NULL_LIST(args.tres_list);
+
+	return rc;
+}
+
+#define MAGIC_FOREACH_DEL_CLUSTER 0xa3a2aa3a
+typedef struct {
+	int magic;
+	data_t *clusters;
+} foreach_del_cluster_t;
+
+static int _foreach_del_cluster(void *x, void *arg)
+{
+	char *cluster = x;
+	foreach_del_cluster_t *args = arg;
+
+	data_set_string(data_list_append(args->clusters), cluster);
+	return 1;
+}
+
+static int _delete_cluster(data_t *resp, data_t *errors, char *cluster,
+			   void *auth)
+{
+	int rc = SLURM_SUCCESS;
+	slurmdb_cluster_cond_t cluster_cond = {
+		.with_deleted = true,
+		.cluster_list = list_create(NULL),
+	};
+	foreach_del_cluster_t args = {
+		.magic = MAGIC_FOREACH_DEL_CLUSTER,
+		.clusters = data_set_list(
+			data_key_set(resp, "deleted_clusters")),
+	};
+	List cluster_list = NULL;
+
+	if (!cluster) {
+		rc = ESLURM_REST_EMPTY_RESULT;
+		goto cleanup;
+	}
+
+	list_append(cluster_cond.cluster_list, cluster);
+
+	if (!(rc = db_query_list(errors, auth, &cluster_list,
+				 slurmdb_clusters_remove, &cluster_cond)))
+		rc = db_query_commit(errors, auth);
+
+	if (!rc &&
+	    (list_for_each(cluster_list, _foreach_del_cluster, &args) < 0))
+		rc = ESLURM_DATA_CONV_FAILED;
+cleanup:
+	FREE_NULL_LIST(cluster_list);
+	FREE_NULL_LIST(cluster_cond.cluster_list);
+
+	return rc;
+}
+
+#define MAGIC_FOREACH_UP_CLUSTER 0xdaba3019
+typedef struct {
+	int magic;
+	List cluster_list;
+	List tres_list;
+	data_t *errors;
+	rest_auth_context_t *auth;
+} foreach_update_cluster_t;
+
+static data_for_each_cmd_t _foreach_update_cluster(data_t *data, void *arg)
+{
+	foreach_update_cluster_t *args = arg;
+	slurmdb_cluster_rec_t *cluster;
+	parser_env_t penv = {
+		.auth = args->auth,
+		.g_tres_list = args->tres_list,
+	};
+
+	xassert(args->magic == MAGIC_FOREACH_UP_CLUSTER);
+
+	if (data_get_type(data) != DATA_TYPE_DICT) {
+		resp_error(args->errors, ESLURM_REST_INVALID_QUERY,
+			   "each cluster entry must be a dictionary", NULL);
+		return DATA_FOR_EACH_FAIL;
+	}
+
+	cluster = xmalloc(sizeof(slurmdb_cluster_rec_t));
+	slurmdb_init_cluster_rec(cluster, false);
+
+	cluster->accounting_list = list_create(
+		slurmdb_destroy_cluster_accounting_rec);
+	(void)list_append(args->cluster_list, cluster);
+
+	if (parse(PARSE_CLUSTER_REC, cluster, data, args->errors, &penv))
+		return DATA_FOR_EACH_FAIL;
+
+	return DATA_FOR_EACH_CONT;
+}
+
+static int _update_clusters(data_t *query, data_t *resp, data_t *errors,
+			    void *auth, bool commit)
+{
+	int rc = SLURM_SUCCESS;
+	foreach_update_cluster_t args = {
+		.magic = MAGIC_FOREACH_UP_CLUSTER,
+		.auth = auth,
+		.errors = errors,
+		.cluster_list = list_create(slurmdb_destroy_cluster_rec),
+	};
+	slurmdb_tres_cond_t tres_cond = {
+		.with_deleted = 1,
+	};
+	data_t *dclusters = get_query_key_list("clusters", errors, query);
+
+	if (!(rc = db_query_list(errors, auth, &args.tres_list,
+				 slurmdb_tres_get, &tres_cond)) &&
+	    (data_list_for_each(dclusters, _foreach_update_cluster, &args) < 0))
+		rc = ESLURM_REST_INVALID_QUERY;
+
+	if (!(rc = db_query_rc(errors, auth, args.cluster_list,
+			       slurmdb_clusters_add)) &&
+	    commit)
+		db_query_commit(errors, auth);
+
+	FREE_NULL_LIST(args.cluster_list);
+	FREE_NULL_LIST(args.tres_list);
+
+	return rc;
+}
+
+extern int op_handler_cluster(const char *context_id,
+			      http_request_method_t method, data_t *parameters,
+			      data_t *query, int tag, data_t *resp, rest_auth_context_t *auth)
+{
+	int rc = SLURM_SUCCESS;
+	data_t *errors = populate_response_format(resp);
+	char *cluster = get_str_param("cluster_name", errors, parameters);
+
+	if (!rc && (method == HTTP_REQUEST_GET))
+		rc = _dump_clusters(resp, errors, cluster, auth);
+	else if (!rc && (method == HTTP_REQUEST_DELETE))
+		rc = _delete_cluster(resp, errors, cluster, auth);
+	else
+		rc = ESLURM_REST_INVALID_QUERY;
+
+	return rc;
+}
+
+extern int op_handler_clusters(const char *context_id,
+			       http_request_method_t method, data_t *parameters,
+			       data_t *query, int tag, data_t *resp,
+			       rest_auth_context_t *auth)
+{
+	data_t *errors = populate_response_format(resp);
+	int rc = SLURM_SUCCESS;
+
+	if (method == HTTP_REQUEST_GET)
+		rc = _dump_clusters(resp, errors, NULL, auth);
+	else if (method == HTTP_REQUEST_POST)
+		rc = _update_clusters(query, resp, errors, auth,
+				      (tag != CONFIG_OP_TAG));
+	else
+		rc = ESLURM_REST_INVALID_QUERY;
+
+	return rc;
+}
+
+extern void init_op_cluster(void)
+{
+	bind_operation_handler("/slurmdb/v0.0.38/clusters/",
+			       op_handler_clusters, 0);
+	bind_operation_handler("/slurmdb/v0.0.38/cluster/{cluster_name}",
+			       op_handler_cluster, 0);
+}
+
+extern void destroy_op_cluster(void)
+{
+	unbind_operation_handler(op_handler_clusters);
+	unbind_operation_handler(op_handler_clusters);
+}
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/config.c slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/config.c
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/config.c	1970-01-01 01:00:00.000000000 +0100
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/config.c	2022-11-25 15:06:48.504754836 +0100
@@ -0,0 +1,118 @@
+/*****************************************************************************\
+ *  config.c - Slurm REST API config http operations handlers
+ *****************************************************************************
+ *  Copyright (C) 2020 SchedMD LLC.
+ *  Written by Nathan Rini <nate@schedmd.com>
+ *
+ *  This file is part of Slurm, a resource management program.
+ *  For details, see <https://slurm.schedmd.com/>.
+ *  Please also read the included file: DISCLAIMER.
+ *
+ *  Slurm is free software; you can redistribute it and/or modify it under
+ *  the terms of the GNU General Public License as published by the Free
+ *  Software Foundation; either version 2 of the License, or (at your option)
+ *  any later version.
+ *
+ *  In addition, as a special exception, the copyright holders give permission
+ *  to link the code of portions of this program with the OpenSSL library under
+ *  certain conditions as described in each individual source file, and
+ *  distribute linked combinations including the two. You must obey the GNU
+ *  General Public License in all respects for all of the code used other than
+ *  OpenSSL. If you modify file(s) with this exception, you may extend this
+ *  exception to your version of the file(s), but you are not obligated to do
+ *  so. If you do not wish to do so, delete this exception statement from your
+ *  version.  If you delete this exception statement from all source files in
+ *  the program, then also delete it here.
+ *
+ *  Slurm is distributed in the hope that it will be useful, but WITHOUT ANY
+ *  WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+ *  FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
+ *  details.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with Slurm; if not, write to the Free Software Foundation, Inc.,
+ *  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301  USA.
+\*****************************************************************************/
+
+#include "config.h"
+
+#include <stdint.h>
+
+#include "slurm/slurm.h"
+#include "slurm/slurmdb.h"
+
+#include "src/common/list.h"
+#include "src/common/log.h"
+#include "src/common/parse_time.h"
+#include "src/common/ref.h"
+#include "src/common/slurm_protocol_api.h"
+#include "src/common/slurmdbd_defs.h"
+#include "src/common/strlcpy.h"
+#include "src/common/uid.h"
+#include "src/common/xassert.h"
+#include "src/common/xmalloc.h"
+#include "src/common/xstring.h"
+
+#include "src/slurmrestd/openapi.h"
+#include "src/slurmrestd/operations.h"
+#include "src/slurmrestd/xjson.h"
+
+#include "src/slurmrestd/plugins/openapi/dbv0.0.38/api.h"
+
+static const operation_handler_t ops[] = {
+	/* Warning: order matters */
+	op_handler_clusters,
+	op_handler_tres,
+	op_handler_accounts,
+	op_handler_users,
+	op_handler_qos,
+	op_handler_wckeys,
+	op_handler_associations,
+};
+
+static int _op_handler_config(const char *context_id,
+			      http_request_method_t method, data_t *parameters,
+			      data_t *query, int tag, data_t *resp,
+			      rest_auth_context_t *auth)
+{
+	int rc = SLURM_SUCCESS;
+	data_t *errors = populate_response_format(resp);
+
+	if ((method != HTTP_REQUEST_GET) && (method != HTTP_REQUEST_POST))
+		return resp_error(errors, ESLURM_REST_INVALID_QUERY,
+				  "invalid method requested", NULL);
+
+	for (int i = 0; (!rc) && (i < ARRAY_SIZE(ops)); i++) {
+		int rc2 = ops[i](context_id, method, parameters, query, tag,
+				 resp, auth);
+
+		/*
+		 * ignore empty results as there may simply be nothing
+		 * there to dump.
+		 **/
+		if (rc2 != ESLURM_REST_EMPTY_RESULT)
+			rc = rc2;
+	}
+
+	if (method == HTTP_REQUEST_POST) {
+		if (!rc)
+			rc = db_query_commit(errors, auth);
+		else
+			rc = resp_error(errors, rc,
+					"refusing to commit after error",
+					NULL);
+	}
+
+	return rc;
+}
+
+extern void init_op_config(void)
+{
+	bind_operation_handler("/slurmdb/v0.0.38/config", _op_handler_config,
+			       CONFIG_OP_TAG);
+}
+
+extern void destroy_op_config(void)
+{
+	unbind_operation_handler(_op_handler_config);
+}
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/diag.c slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/diag.c
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/diag.c	1970-01-01 01:00:00.000000000 +0100
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/diag.c	2022-11-25 15:06:09.223023551 +0100
@@ -0,0 +1,91 @@
+/*****************************************************************************\
+ *  diag.c - Slurm REST API accounting diag http operations handlers
+ *****************************************************************************
+ *  Copyright (C) 2020 SchedMD LLC.
+ *  Written by Nathan Rini <nate@schedmd.com>
+ *
+ *  This file is part of Slurm, a resource management program.
+ *  For details, see <https://slurm.schedmd.com/>.
+ *  Please also read the included file: DISCLAIMER.
+ *
+ *  Slurm is free software; you can redistribute it and/or modify it under
+ *  the terms of the GNU General Public License as published by the Free
+ *  Software Foundation; either version 2 of the License, or (at your option)
+ *  any later version.
+ *
+ *  In addition, as a special exception, the copyright holders give permission
+ *  to link the code of portions of this program with the OpenSSL library under
+ *  certain conditions as described in each individual source file, and
+ *  distribute linked combinations including the two. You must obey the GNU
+ *  General Public License in all respects for all of the code used other than
+ *  OpenSSL. If you modify file(s) with this exception, you may extend this
+ *  exception to your version of the file(s), but you are not obligated to do
+ *  so. If you do not wish to do so, delete this exception statement from your
+ *  version.  If you delete this exception statement from all source files in
+ *  the program, then also delete it here.
+ *
+ *  Slurm is distributed in the hope that it will be useful, but WITHOUT ANY
+ *  WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+ *  FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
+ *  details.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with Slurm; if not, write to the Free Software Foundation, Inc.,
+ *  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301  USA.
+\*****************************************************************************/
+
+#include "config.h"
+
+#include <stdint.h>
+
+#include "slurm/slurm.h"
+#include "slurm/slurmdb.h"
+
+#include "src/common/list.h"
+#include "src/common/log.h"
+#include "src/common/ref.h"
+#include "src/common/slurm_protocol_api.h"
+#include "src/common/slurmdbd_defs.h"
+#include "src/common/uid.h"
+#include "src/common/xassert.h"
+#include "src/common/xmalloc.h"
+#include "src/common/xstring.h"
+
+#include "src/slurmrestd/operations.h"
+
+#include "src/slurmrestd/plugins/openapi/dbv0.0.38/api.h"
+
+/* based on sacctmgr_list_stats() */
+static int _op_handler_diag(const char *context_id,
+			    http_request_method_t method, data_t *parameters,
+			    data_t *query, int tag, data_t *resp,
+			    rest_auth_context_t *auth)
+{
+	data_t *errors = populate_response_format(resp);
+	parser_env_t penv = { 0 };
+	slurmdb_stats_rec_t *stats_rec = NULL;
+	int rc;
+
+	debug4("%s:[%s] diag handler called", __func__, context_id);
+
+	if ((rc = slurmdb_get_stats(rest_auth_g_get_db_conn(auth), &stats_rec)))
+		resp_error(errors, rc, NULL, "slurmdb_get_stats");
+	else
+		rc = dump(PARSE_STATS_REC, stats_rec,
+			  data_set_dict(data_key_set(resp, "statistics")),
+			  &penv);
+
+	slurmdb_destroy_stats_rec(stats_rec);
+
+	return rc;
+}
+
+extern void init_op_diag(void)
+{
+	bind_operation_handler("/slurmdb/v0.0.38/diag/", _op_handler_diag, 0);
+}
+
+extern void destroy_op_diag(void)
+{
+	unbind_operation_handler(_op_handler_diag);
+}
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/jobs.c slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/jobs.c
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/jobs.c	1970-01-01 01:00:00.000000000 +0100
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/jobs.c	2022-11-25 15:06:09.223023551 +0100
@@ -0,0 +1,608 @@
+/*****************************************************************************\
+ *  job.c - Slurm REST API accounting job http operations handlers
+ *****************************************************************************
+ *  Copyright (C) 2020 SchedMD LLC.
+ *  Written by Nathan Rini <nate@schedmd.com>
+ *
+ *  This file is part of Slurm, a resource management program.
+ *  For details, see <https://slurm.schedmd.com/>.
+ *  Please also read the included file: DISCLAIMER.
+ *
+ *  Slurm is free software; you can redistribute it and/or modify it under
+ *  the terms of the GNU General Public License as published by the Free
+ *  Software Foundation; either version 2 of the License, or (at your option)
+ *  any later version.
+ *
+ *  In addition, as a special exception, the copyright holders give permission
+ *  to link the code of portions of this program with the OpenSSL library under
+ *  certain conditions as described in each individual source file, and
+ *  distribute linked combinations including the two. You must obey the GNU
+ *  General Public License in all respects for all of the code used other than
+ *  OpenSSL. If you modify file(s) with this exception, you may extend this
+ *  exception to your version of the file(s), but you are not obligated to do
+ *  so. If you do not wish to do so, delete this exception statement from your
+ *  version.  If you delete this exception statement from all source files in
+ *  the program, then also delete it here.
+ *
+ *  Slurm is distributed in the hope that it will be useful, but WITHOUT ANY
+ *  WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+ *  FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
+ *  details.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with Slurm; if not, write to the Free Software Foundation, Inc.,
+ *  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301  USA.
+\*****************************************************************************/
+
+#include "config.h"
+
+#include <ctype.h>
+#include <limits.h>
+#include <stdint.h>
+
+#include "slurm/slurm.h"
+#include "slurm/slurmdb.h"
+
+#include "src/common/list.h"
+#include "src/common/log.h"
+#include "src/common/parse_time.h"
+#include "src/common/ref.h"
+#include "src/common/slurm_protocol_api.h"
+#include "src/common/slurmdbd_defs.h"
+#include "src/common/strlcpy.h"
+#include "src/common/uid.h"
+#include "src/common/xassert.h"
+#include "src/common/xmalloc.h"
+#include "src/common/xstring.h"
+
+#include "src/slurmrestd/operations.h"
+
+#include "src/slurmrestd/plugins/openapi/dbv0.0.38/api.h"
+
+/* typedef for adding a function to add a char* to a List */
+typedef int (*add_list_t) (List char_list, char *values);
+
+#define MAGIC_FOREACH_JOB 0xf8aefef3
+typedef struct {
+	int magic;
+	data_t *jobs;
+	List tres_list;
+	List qos_list;
+	List assoc_list;
+} foreach_job_t;
+
+static int _foreach_job(void *x, void *arg)
+{
+	slurmdb_job_rec_t *job = x;
+	foreach_job_t *args = arg;
+	parser_env_t penv = {
+		.g_qos_list = args->qos_list,
+		.g_tres_list = args->tres_list,
+		.g_assoc_list = args->assoc_list,
+	};
+
+	xassert(args->magic == MAGIC_FOREACH_JOB);
+
+	if (dump(PARSE_JOB, job, data_set_dict(data_list_append(args->jobs)),
+		 &penv))
+		return -1;
+	else
+		return 1;
+}
+
+typedef struct {
+	data_t *errors;
+	slurmdb_job_cond_t *job_cond;
+} foreach_query_search_t;
+
+data_for_each_cmd_t _foreach_list_entry(data_t *data, void *arg)
+{
+	List list = arg;
+
+	if (data_convert_type(data, DATA_TYPE_STRING) != DATA_TYPE_STRING)
+		return DATA_FOR_EACH_FAIL;
+
+	if (slurm_addto_char_list(list, data_get_string(data)) < 1)
+		return DATA_FOR_EACH_FAIL;
+
+	return DATA_FOR_EACH_CONT;
+}
+
+static int _parse_csv_list(data_t *src, const char *key, List *list,
+			   data_t *errors, add_list_t add_to)
+{
+	if (!*list)
+		*list = list_create(xfree_ptr);
+
+	if (data_get_type(src) == DATA_TYPE_LIST) {
+		if (data_list_for_each(src, _foreach_list_entry, *list) < 0)
+			return resp_error(errors, ESLURM_REST_INVALID_QUERY,
+					  "error parsing CSV in form of list",
+					  key);
+
+		return SLURM_SUCCESS;
+	}
+
+	if (data_convert_type(src, DATA_TYPE_STRING) != DATA_TYPE_STRING)
+		return resp_error(errors, ESLURM_REST_INVALID_QUERY,
+				  "format must be a string", key);
+
+	if (add_to(*list, data_get_string(src)) < 1)
+		return resp_error(errors, ESLURM_REST_INVALID_QUERY,
+				  "Unable to parse CSV list", key);
+
+	return SLURM_SUCCESS;
+}
+
+/*
+ * Convert job state to numeric job state
+ * The return value is the same as slurm_addto_char_list():
+ *   the number of items added (zero on failure)
+ */
+static int _add_list_job_state(List char_list, char *values)
+{
+	int rc = 0;
+	char *last = NULL, *vdup, *value;
+
+	vdup = xstrdup(values);
+	value = strtok_r(vdup, ",", &last);
+	while (value) {
+		char *id_str;
+		uint32_t id = NO_VAL;
+
+		if (isdigit(value[0])) {
+			unsigned long id_ul;
+			errno = 0;
+			id_ul = slurm_atoul(value);
+			/*
+			 * Since zero is a valid value, we have to check if
+			 * errno is also set to know if it was an error.
+			 */
+			if ((!id_ul && errno) || (id_ul == ULONG_MAX))
+				break;
+			id = (uint32_t) id_ul;
+		} else {
+			if ((id = job_state_num(value)) == NO_VAL)
+				break;
+			else
+				id = JOB_STATE_BASE & id;
+		}
+
+		if (id >= JOB_END) {
+			break;
+		}
+
+		id_str = xstrdup_printf("%u", id);
+		rc = slurm_addto_char_list(char_list, id_str);
+		xfree(id_str);
+
+		value = strtok_r(NULL, ",", &last);
+	}
+	xfree(vdup);
+
+	return rc;
+}
+
+typedef struct {
+	char *field;
+	int offset;
+} sint_t;
+static const sint_t int_list[] = {
+	{ "cpus_max", offsetof(slurmdb_job_cond_t, cpus_max) },
+	{ "cpus_min", offsetof(slurmdb_job_cond_t, cpus_min) },
+	{ "exit_code", offsetof(slurmdb_job_cond_t, exitcode) },
+	{ "nodes_min", offsetof(slurmdb_job_cond_t, nodes_min) },
+	{ "nodes_max", offsetof(slurmdb_job_cond_t, nodes_max) },
+};
+
+typedef struct {
+	char *field;
+	uint32_t flag;
+} flag_t;
+static const flag_t flags[] = {
+	/* skipping JOBCOND_FLAG_DUP */
+	{ "skip_steps", JOBCOND_FLAG_NO_STEP },
+	/* skipping JOBCOND_FLAG_NO_TRUNC */
+	/* skipping JOBCOND_FLAG_RUNAWAY */
+	/* skipping JOBCOND_FLAG_WHOLE_HETJOB */
+	/* skipping JOBCOND_FLAG_NO_WHOLE_HETJOB */
+	{ "disable_wait_for_result", JOBCOND_FLAG_NO_WAIT },
+	/* skipping JOBCOND_FLAG_NO_DEFAULT_USAGE */
+};
+
+typedef struct {
+	char *field;
+	int offset;
+	add_list_t add_to;
+} csv_list_t;
+static const csv_list_t csv_lists[] = {
+	{
+		"account",
+		offsetof(slurmdb_job_cond_t, acct_list),
+		slurm_addto_char_list
+	},
+	{
+		"association",
+		offsetof(slurmdb_job_cond_t, associd_list),
+		slurm_addto_char_list
+	},
+	{
+		"cluster",
+		offsetof(slurmdb_job_cond_t, cluster_list),
+		slurm_addto_char_list
+	},
+	{
+		"constraints",
+		offsetof(slurmdb_job_cond_t, constraint_list),
+		slurm_addto_char_list
+	},
+	{
+		"format",
+		offsetof(slurmdb_job_cond_t, format_list),
+		slurm_addto_char_list
+	},
+	{
+		"groups",
+		offsetof(slurmdb_job_cond_t, groupid_list),
+		slurm_addto_char_list
+	},
+	{
+		"job_name",
+		offsetof(slurmdb_job_cond_t, jobname_list),
+		slurm_addto_char_list
+	},
+	{
+		"partition",
+		offsetof(slurmdb_job_cond_t, partition_list),
+		slurm_addto_char_list
+	},
+	{
+		"qos",
+		offsetof(slurmdb_job_cond_t, qos_list),
+		slurm_addto_char_list
+	},
+	{
+		"reason",
+		offsetof(slurmdb_job_cond_t, reason_list),
+		slurm_addto_char_list
+	},
+	{
+		"reservation",
+		offsetof(slurmdb_job_cond_t, resv_list),
+		slurm_addto_char_list
+	},
+	{
+		"state",
+		offsetof(slurmdb_job_cond_t, state_list),
+		_add_list_job_state
+	},
+	{
+		"wckey",
+		offsetof(slurmdb_job_cond_t, wckey_list),
+		slurm_addto_char_list
+	},
+};
+
+static data_for_each_cmd_t _foreach_step(data_t *data, void *arg)
+{
+	List list = arg;
+
+	if (data_convert_type(data, DATA_TYPE_STRING) != DATA_TYPE_STRING)
+		return DATA_FOR_EACH_FAIL;
+
+	if (slurm_addto_step_list(list, data_get_string(data)) < 1)
+		return DATA_FOR_EACH_FAIL;
+
+	return DATA_FOR_EACH_CONT;
+}
+
+static data_for_each_cmd_t _foreach_query_search(const char *key,
+						 data_t *data,
+						 void *arg)
+{
+	foreach_query_search_t *args = arg;
+	data_t *errors = args->errors;
+
+	if (!xstrcasecmp("start_time", key)) {
+		if (args->job_cond->flags & JOBCOND_FLAG_NO_DEFAULT_USAGE) {
+			resp_error(
+				errors, ESLURM_REST_INVALID_QUERY,
+				"start_time and submit_time are mutually exclusive",
+				key);
+			return DATA_FOR_EACH_FAIL;
+		}
+
+		if (data_convert_type(data, DATA_TYPE_STRING) !=
+		    DATA_TYPE_STRING) {
+			resp_error(errors, ESLURM_REST_INVALID_QUERY,
+				   "Time format must be a string", key);
+			return DATA_FOR_EACH_FAIL;
+		}
+
+		args->job_cond->usage_start = parse_time(data_get_string(data),
+							 1);
+
+		if (!args->job_cond->usage_start) {
+			resp_error(errors, ESLURM_REST_INVALID_QUERY,
+				   "Unable to parse time format", key);
+			return DATA_FOR_EACH_FAIL;
+		}
+
+		return DATA_FOR_EACH_CONT;
+	}
+
+	if (!xstrcasecmp("end_time", key)) {
+		if (data_convert_type(data, DATA_TYPE_STRING) !=
+		    DATA_TYPE_STRING) {
+			resp_error(errors, ESLURM_REST_INVALID_QUERY,
+				   "Time format must be a string", key);
+			return DATA_FOR_EACH_FAIL;
+		}
+
+		args->job_cond->usage_end = parse_time(data_get_string(data),
+						       1);
+
+		if (!args->job_cond->usage_end) {
+			resp_error(errors, ESLURM_REST_INVALID_QUERY,
+				   "Unable to parse time format", key);
+			return DATA_FOR_EACH_FAIL;
+		}
+
+		return DATA_FOR_EACH_CONT;
+	}
+
+	if (!xstrcasecmp("submit_time", key)) {
+		if (args->job_cond->usage_start) {
+			resp_error(
+				errors, ESLURM_REST_INVALID_QUERY,
+				"start_time and submit_time are mutually exclusive",
+				key);
+			return DATA_FOR_EACH_FAIL;
+		}
+
+		if (data_convert_type(data, DATA_TYPE_STRING) !=
+		    DATA_TYPE_STRING) {
+			resp_error(errors, ESLURM_REST_INVALID_QUERY,
+				   "Time format must be a string", key);
+			return DATA_FOR_EACH_FAIL;
+		}
+
+		args->job_cond->usage_start = parse_time(data_get_string(data),
+							 1);
+
+		if (!args->job_cond->usage_start) {
+			resp_error(errors, ESLURM_REST_INVALID_QUERY,
+				   "Unable to parse time format", key);
+			return DATA_FOR_EACH_FAIL;
+		}
+
+		args->job_cond->flags |= JOBCOND_FLAG_NO_DEFAULT_USAGE;
+
+		return DATA_FOR_EACH_CONT;
+	}
+
+	if (!xstrcasecmp("node", key)) {
+		if (data_convert_type(data, DATA_TYPE_STRING) !=
+		    DATA_TYPE_STRING) {
+			resp_error(errors, ESLURM_REST_INVALID_QUERY,
+				   "format must be a string", key);
+			return DATA_FOR_EACH_FAIL;
+		}
+
+		args->job_cond->used_nodes = xstrdup(
+			data_get_string_const(data));
+
+		return DATA_FOR_EACH_CONT;
+	}
+
+	if (!xstrcasecmp("step", key)) {
+		if (!args->job_cond->step_list)
+			args->job_cond->step_list = list_create(
+				slurm_destroy_selected_step);
+
+		if (data_get_type(data) == DATA_TYPE_LIST) {
+			if (data_list_for_each(data, _foreach_step,
+					       args->job_cond->step_list) < 0) {
+				(void) resp_error(
+					errors, ESLURM_REST_INVALID_QUERY,
+					"error parsing steps in form of list",
+					key);
+				return DATA_FOR_EACH_FAIL;
+			}
+
+			return DATA_FOR_EACH_CONT;
+		}
+
+		if (data_convert_type(data, DATA_TYPE_STRING) !=
+		    DATA_TYPE_STRING) {
+			resp_error(errors, ESLURM_REST_INVALID_QUERY,
+				   "format must be a string", key);
+			return DATA_FOR_EACH_FAIL;
+		}
+
+		slurm_addto_step_list(args->job_cond->step_list,
+				      data_get_string(data));
+
+		if (!list_count(args->job_cond->step_list)) {
+			resp_error(errors, ESLURM_REST_INVALID_QUERY,
+				   "Unable to parse job/step format", key);
+			return DATA_FOR_EACH_FAIL;
+		}
+
+		return DATA_FOR_EACH_CONT;
+	}
+
+	for (int i = 0; i < ARRAY_SIZE(csv_lists); i++) {
+		if (!xstrcasecmp(csv_lists[i].field, key)) {
+			List *list = (((void *) args->job_cond) +
+				      csv_lists[i].offset);
+
+			if (_parse_csv_list(data, key, list, errors,
+					    csv_lists[i].add_to))
+				return DATA_FOR_EACH_FAIL;
+
+			return DATA_FOR_EACH_CONT;
+		}
+	}
+
+	for (int i = 0; i < ARRAY_SIZE(flags); i++) {
+		if (!xstrcasecmp(flags[i].field, key)) {
+			if (data_convert_type(data, DATA_TYPE_BOOL) !=
+			    DATA_TYPE_BOOL) {
+				resp_error(errors, ESLURM_REST_INVALID_QUERY,
+					   "must be an Boolean", key);
+				return DATA_FOR_EACH_FAIL;
+			}
+
+			if (data_get_bool(data))
+				args->job_cond->flags |= flags[i].flag;
+			else
+				args->job_cond->flags &= ~flags[i].flag;
+
+			return DATA_FOR_EACH_CONT;
+		}
+	}
+
+	for (int i = 0; i < ARRAY_SIZE(int_list); i++) {
+		if (!xstrcasecmp(int_list[i].field, key)) {
+			int32_t *t = (((void *) args->job_cond) +
+				      int_list[i].offset);
+
+			if (data_convert_type(data, DATA_TYPE_INT_64) !=
+			    DATA_TYPE_INT_64) {
+				resp_error(errors, ESLURM_REST_INVALID_QUERY,
+					   "must be an integer", key);
+				return DATA_FOR_EACH_FAIL;
+			}
+
+			*t = data_get_int(data);
+
+			return DATA_FOR_EACH_CONT;
+		}
+	}
+
+	resp_error(errors, ESLURM_REST_INVALID_QUERY, "Unknown Query field",
+		   NULL);
+	return DATA_FOR_EACH_FAIL;
+}
+
+static int _dump_jobs(const char *context_id, http_request_method_t method,
+		      data_t *parameters, data_t *query, int tag, data_t *resp,
+		      void *auth, data_t *errors, slurmdb_job_cond_t *job_cond)
+{
+	int rc = SLURM_SUCCESS;
+	slurmdb_assoc_cond_t assoc_cond = {
+		.with_deleted = 1,
+		.without_parent_info = 1,
+		.without_parent_limits = 1,
+	};
+	slurmdb_qos_cond_t qos_cond = {
+		.with_deleted = 1,
+	};
+	slurmdb_tres_cond_t tres_cond = {
+		.with_deleted = 1,
+	};
+	foreach_job_t args = {
+		.magic = MAGIC_FOREACH_JOB,
+		.jobs = data_set_list(data_key_set(resp, "jobs")),
+	};
+	List jobs = NULL;
+
+	/* set cluster by default if not specified */
+	if (job_cond &&
+	    (!job_cond->cluster_list ||
+	     list_is_empty(job_cond->cluster_list))) {
+		FREE_NULL_LIST(job_cond->cluster_list);
+		job_cond->cluster_list = list_create(xfree_ptr);
+		list_append(job_cond->cluster_list,
+			    xstrdup(slurm_conf.cluster_name));
+	}
+
+	if (!db_query_list(errors, auth, &jobs, slurmdb_jobs_get, job_cond) &&
+	    !db_query_list(errors, auth, &args.assoc_list,
+			   slurmdb_associations_get, &assoc_cond) &&
+	    !db_query_list(errors, auth, &args.qos_list, slurmdb_qos_get,
+			   &qos_cond) &&
+	    !db_query_list(errors, auth, &args.tres_list, slurmdb_tres_get,
+			   &tres_cond) &&
+	    (list_for_each(jobs, _foreach_job, &args) < 0))
+		rc = ESLURM_DATA_CONV_FAILED;
+
+	FREE_NULL_LIST(args.tres_list);
+	FREE_NULL_LIST(args.qos_list);
+	FREE_NULL_LIST(args.assoc_list);
+	FREE_NULL_LIST(jobs);
+
+	return rc;
+}
+
+/* based on get_data() in sacct/options.c */
+extern int op_handler_jobs(const char *context_id, http_request_method_t method,
+			   data_t *parameters, data_t *query, int tag,
+			   data_t *resp, rest_auth_context_t *auth)
+{
+	data_t *errors = populate_response_format(resp);
+
+	if (query && data_get_dict_length(query)) {
+		slurmdb_job_cond_t job_cond = {
+			/*
+			 * default to grabbing all information
+			 * based on _init_params()
+			 */
+			.flags = (JOBCOND_FLAG_DUP | JOBCOND_FLAG_NO_TRUNC),
+			.db_flags = SLURMDB_JOB_FLAG_NOTSET,
+		};
+		foreach_query_search_t args = {
+			.errors = errors,
+			.job_cond = &job_cond,
+		};
+
+		if (data_dict_for_each(query, _foreach_query_search, &args) < 0)
+			return SLURM_ERROR;
+
+		return _dump_jobs(context_id, method, parameters, query, tag,
+				  resp, auth, errors, &job_cond);
+	} else
+		return _dump_jobs(context_id, method, parameters, query, tag,
+				  resp, auth, errors, NULL);
+}
+
+/* based on get_data() in sacct/options.c */
+static int _op_handler_job(const char *context_id, http_request_method_t method,
+			   data_t *parameters, data_t *query, int tag,
+			   data_t *resp, rest_auth_context_t *auth)
+{
+	int rc = SLURM_SUCCESS;
+	data_t *errors = populate_response_format(resp);
+	char *jobid;
+	slurmdb_job_cond_t job_cond = {
+		.flags = (JOBCOND_FLAG_DUP | JOBCOND_FLAG_NO_TRUNC),
+		.db_flags = SLURMDB_JOB_FLAG_NOTSET,
+	};
+
+	if ((jobid = get_str_param("job_id", errors, parameters))) {
+		job_cond.step_list = list_create(slurm_destroy_selected_step);
+		slurm_addto_step_list(job_cond.step_list, jobid);
+	} else
+		rc = ESLURM_REST_INVALID_QUERY;
+
+	if (!rc)
+		rc = _dump_jobs(context_id, method, parameters, query, tag,
+				resp, auth, errors, &job_cond);
+
+	FREE_NULL_LIST(job_cond.step_list);
+	return rc;
+}
+
+extern void init_op_job(void)
+{
+	bind_operation_handler("/slurmdb/v0.0.38/jobs/", op_handler_jobs, 0);
+	bind_operation_handler("/slurmdb/v0.0.38/job/{job_id}", _op_handler_job,
+			       0);
+}
+
+extern void destroy_op_job(void)
+{
+	unbind_operation_handler(_op_handler_job);
+	unbind_operation_handler(op_handler_jobs);
+}
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/LICENSE-openapi.json slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/LICENSE-openapi.json
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/LICENSE-openapi.json	1970-01-01 01:00:00.000000000 +0100
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/LICENSE-openapi.json	2022-11-10 21:49:08.000000000 +0100
@@ -0,0 +1,19 @@
+The OpenAPI specification for Slurm's RESTful interface, as described in the
+openapi.json file in this directory, is made available under the following
+license terms:
+
+Under Slurm's license described in LICENSE.OpenSSL in the top level of the
+Slurm source, and generally referred to as "GPLv2+ with OpenSSL linking
+exception".
+
+Or, under the terms of The Apache License, Version 2.0, and abbreviated as
+"Apache 2.0" within the OpenAPI specification itself. A copy of this license is
+available at https://www.apache.org/licenses/LICENSE-2.0.html.
+
+As an additional requirement for changes to this specification: Contributors
+wishing to submit changes to these files must sign SchedMD's Contributor
+License Agreement ("CLA"). (https://slurm.schedmd.com/contributor.html).
+By signing the CLA, contributors are specifically licensing any changes,
+modifications, revisions, or edits to the OpenAPI specification for Slurms
+RESTful interface back to SchedMD to the maximum extent permitted by the
+relevant law and the CLA.
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/Makefile.am slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/Makefile.am
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/Makefile.am	1970-01-01 01:00:00.000000000 +0100
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/Makefile.am	2022-11-10 21:49:08.000000000 +0100
@@ -0,0 +1,47 @@
+# Makefile for openapi/dbv0.0.38 plugin
+
+AUTOMAKE_OPTIONS = foreign
+CLEANFILES = *.bino
+
+EXTRA_DIST = LICENSE-openapi.json
+
+REF = openapi.json
+
+PLUGIN_FLAGS = -module -avoid-version --export-dynamic
+
+AM_CPPFLAGS = -I$(top_srcdir) -I$(top_srcdir)/src/common $(JSON_CPPFLAGS)
+
+BIN_REF = $(REF:.json=.bino)
+
+%.bino: %.json
+	$(AM_V_GEN)pushd $(abs_srcdir); $(LD) -r -o "$(abs_builddir)/$*.bino" -z noexecstack --format=binary "$(notdir $<)"; popd
+	$(AM_V_at)@OBJCOPY@ --rename-section .data=.rodata,alloc,load,readonly,data,contents "$*.bino"
+
+openapi_ref.lo: $(BIN_REF)
+	$(AM_V_at)echo "# $@ - a libtool object file" >"$@"
+	$(AM_V_at)echo "# Generated by $(shell @LIBTOOL@ --version | head -n 1)" >>"$@"
+	$(AM_V_at)echo "#" >>"$@"
+	$(AM_V_at)echo "# Please DO NOT delete this file!" >>"$@"
+	$(AM_V_at)echo "# It is necessary for linking the library." >>"$@"
+	$(AM_V_at)echo >>"$@"
+	$(AM_V_at)echo "# Name of the PIC object." >>"$@"
+	$(AM_V_at)echo "pic_object='$(BIN_REF)'" >>"$@"
+	$(AM_V_at)echo >>"$@"
+	$(AM_V_at)echo "# Name of the non-PIC object" >>"$@"
+	$(AM_V_at)echo "non_pic_object=''" >>"$@"
+	$(AM_V_at)echo >>"$@"
+
+libopenapi_ref_la_SOURCES =
+libopenapi_ref_la_DEPENDENCIES = openapi_ref.lo
+
+pkglib_LTLIBRARIES = openapi_dbv0_0_38.la
+noinst_LTLIBRARIES = libopenapi_ref.la
+
+openapi_dbv0_0_38_la_SOURCES = \
+	accounts.c associations.c api.h api.c cluster.c \
+	config.c diag.c jobs.c qos.c parse.c parse.h \
+	tres.c users.c wckeys.c
+
+openapi_dbv0_0_38_la_DEPENDENCIES = $(LIB_SLURM_BUILD)
+openapi_dbv0_0_38_la_LDFLAGS = $(PLUGIN_FLAGS)
+openapi_dbv0_0_38_la_LIBADD = $(libslurmfull_la_LIBADD) openapi_ref.lo
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/openapi.json slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/openapi.json
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/openapi.json	1970-01-01 01:00:00.000000000 +0100
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/openapi.json	2022-11-10 21:49:08.000000000 +0100
@@ -0,0 +1,4040 @@
+{
+  "openapi": "3.0.3",
+  "info": {
+    "description": "API to access and control Slurm DB.",
+    "license": {
+      "name": "Apache 2.0",
+      "url": "https://www.apache.org/licenses/LICENSE-2.0.html"
+    },
+    "termsOfService": "https://github.com/SchedMD/slurm/blob/master/DISCLAIMER",
+    "contact": {
+      "name": "SchedMD LLC",
+      "url": "https://www.schedmd.com/",
+      "email": "sales@schedmd.com"
+    },
+    "title": "Slurm Rest API",
+    "version": "dbv0.0.38"
+  },
+  "tags": [
+    {
+      "name": "slurm",
+      "description": "methods that query slurmdbd"
+    },
+    {
+      "name": "openapi",
+      "description": "methods that query for OpenAPI specifications"
+    }
+  ],
+  "servers": [
+    {
+      "url": "/slurmdb/v0.0.38/"
+    }
+  ],
+  "security": [
+    {
+      "user": []
+    },
+    {
+      "token": []
+    }
+  ],
+  "paths": {
+    "/job/{job_id}": {
+      "get": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "get_job",
+        "summary": "Get job info",
+        "description": "This endpoint may return multiple job entries since job_id is not a unique key - only the tuple (cluster, job_id, start_time) is unique. If the requested job_id is a component of a heterogeneous job all components are returned.",
+        "parameters": [
+          {
+            "name": "job_id",
+            "in": "path",
+            "description": "Slurm JobID",
+            "required": true,
+            "style": "simple",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          }
+        ],
+        "responses": {
+          "200": {
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_job_info"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_job_info"
+                }
+              }
+            },
+            "description": "Job description"
+          },
+          "default": {
+            "description": "Unable to find job",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              }
+            }
+          }
+        }
+      }
+    },
+    "/config": {
+      "get": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "get_config",
+        "summary": "Dump all configuration information",
+        "responses": {
+          "200": {
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_config_info"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_config_info"
+                }
+              }
+            },
+            "description": "slurmdbd configuration"
+          },
+          "default": {
+            "description": "Unable to dump config",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              }
+            }
+          }
+        }
+      },
+      "post": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "set_config",
+        "summary": "Load all configuration information",
+        "responses": {
+          "200": {
+            "description": "Load config",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_config_response"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_config_response"
+                }
+              }
+            }
+          },
+          "default": {
+            "description": "Unable to set config",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              }
+            }
+          }
+        }
+      }
+    },
+    "/tres/": {
+      "post": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "update_tres",
+        "responses": {
+          "200": {
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_response_tres"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_response_tres"
+                }
+              }
+            },
+            "description": "List of TRES"
+          },
+          "default": {
+            "description": "Unable to update TRES",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              }
+            }
+          }
+        },
+        "summary": "Set TRES info"
+      },
+      "get": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "get_tres",
+        "responses": {
+          "200": {
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_tres_info"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_tres_info"
+                }
+              }
+            },
+            "description": "List of TRES"
+          },
+          "default": {
+            "description": "Unable to retrieve TRES"
+          }
+        },
+        "summary": "Get TRES info"
+      }
+    },
+    "/qos/{qos_name}": {
+      "delete": {
+        "tags": [
+          "slurm"
+        ],
+        "summary": "Delete QOS",
+        "operationId": "delete_qos",
+        "responses": {
+          "200": {
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_response_qos_delete"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_response_qos_delete"
+                }
+              }
+            },
+            "description": "Delete qos"
+          },
+          "default": {
+            "description": "Unable to delete QOS",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              }
+            }
+          }
+        },
+        "parameters": [
+          {
+            "name": "qos_name",
+            "in": "path",
+            "description": "Slurm QOS Name",
+            "required": true,
+            "style": "simple",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          }
+        ]
+      },
+      "get": {
+        "tags": [
+          "slurm"
+        ],
+        "summary": "Get QOS info",
+        "operationId": "get_single_qos",
+        "responses": {
+          "200": {
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_qos_info"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_qos_info"
+                }
+              }
+            },
+            "description": "QOS information"
+          },
+          "default": {
+            "description": "QOS not found",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              }
+            }
+          }
+        },
+        "parameters": [
+          {
+            "name": "qos_name",
+            "in": "path",
+            "description": "Slurm QOS Name",
+            "required": true,
+            "style": "simple",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "name": "with_deleted",
+            "in": "query",
+            "description": "Include deleted QOSs. False by default.",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "boolean"
+            }
+          }
+        ]
+      }
+    },
+    "/qos/": {
+      "get": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "get_qos",
+        "parameters": [
+          {
+            "name": "with_deleted",
+            "in": "query",
+            "description": "Include deleted QOSs. False by default.",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "boolean"
+            }
+          }
+        ],
+        "responses": {
+          "200": {
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_qos_info"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_qos_info"
+                }
+              }
+            },
+            "description": "List of QOS'"
+          },
+          "default": {
+            "description": "QOS not found",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              }
+            }
+          }
+        },
+        "summary": "Get QOS list"
+      },
+      "post": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "update_qos",
+        "responses": {
+          "200": {
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_response_qos"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_response_qos"
+                }
+              }
+            },
+            "description": "QOS update response"
+          },
+          "default": {
+            "description": "Unable to update QOSs",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              }
+            }
+          }
+        },
+        "requestBody": {
+          "description": "Add or update QOSs",
+          "content": {
+            "application/json": {
+              "schema": {
+                "$ref": "#/components/schemas/dbv0.0.38_update_qos"
+              }
+            },
+            "application/x-yaml": {
+              "schema": {
+                "$ref": "#/components/schemas/dbv0.0.38_update_qos"
+              }
+            }
+          },
+          "required": true
+        },
+        "summary": "Set QOS info"
+      }
+    },
+    "/associations/": {
+      "post": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "update_associations",
+        "responses": {
+          "200": {
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_response_associations"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_response_associations"
+                }
+              }
+            },
+            "description": "status of associations update"
+          },
+          "default": {
+            "description": "Unable to update associations",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              }
+            }
+          }
+        },
+        "summary": "Set associations info"
+      },
+      "get": {
+        "tags": [
+	  "slurm"
+	],
+	"operationId": "get_associations",
+	"parameters": [
+	  {
+	    "name": "cluster",
+	    "in": "query",
+	    "description": "Cluster name",
+	    "required": false,
+	    "style": "form",
+	    "explode": false,
+	    "schema": {
+	      "type": "string"
+	    }
+	  },
+	  {
+	    "name": "account",
+	    "in": "query",
+	    "description": "Account name",
+	    "required": false,
+	    "style": "form",
+	    "explode": false,
+	    "schema": {
+	      "type": "string"
+	    }
+	  },
+	  {
+	    "name": "user",
+	    "in": "query",
+	    "description": "User name",
+	    "required": false,
+	    "style": "form",
+	    "explode": false,
+	    "schema": {
+	      "type": "string"
+	    }
+	  },
+	  {
+	    "name": "partition",
+	    "in": "query",
+	    "description": "Partition Name",
+	    "required": false,
+	    "style": "form",
+	    "explode": false,
+	    "schema": {
+	      "type": "string"
+	    }
+	  }
+	],
+	"responses": {
+	  "200": {
+	    "content": {
+	      "application/json": {
+		"schema": {
+		  "$ref": "#/components/schemas/dbv0.0.38_associations_info"
+		}
+	      },
+	      "application/x-yaml": {
+		"schema": {
+		  "$ref": "#/components/schemas/dbv0.0.38_associations_info"
+		}
+	      }
+	    },
+	    "description": "List of associations"
+	  },
+	  "default": {
+	    "description": "Association not found",
+	    "content": {
+	      "application/json": {
+		"schema": {
+		  "$ref": "#/components/schemas/dbv0.0.38_errors"
+		}
+	      },
+	      "application/x-yaml": {
+		"schema": {
+		  "$ref": "#/components/schemas/dbv0.0.38_errors"
+		}
+	      }
+	    }
+	  }
+	},
+	"summary": "Get association list"
+      },
+      "delete": {
+	"tags": [
+	  "slurm"
+	],
+	"operationId": "delete_associations",
+	"parameters": [
+	  {
+	    "name": "cluster",
+	    "in": "query",
+	    "description": "Cluster name",
+	    "required": false,
+	    "style": "form",
+	    "explode": false,
+	    "schema": {
+	      "type": "string"
+	    }
+	  },
+	  {
+	    "name": "account",
+	    "in": "query",
+	    "description": "Account name",
+	    "required": false,
+	    "style": "form",
+	    "explode": false,
+	    "schema": {
+	      "type": "string"
+	    }
+	  },
+	  {
+	    "name": "user",
+	    "in": "query",
+	    "description": "User name",
+	    "required": false,
+	    "style": "form",
+	    "explode": false,
+	    "schema": {
+	      "type": "string"
+	    }
+	  },
+	  {
+	    "name": "partition",
+	    "in": "query",
+	    "description": "Partition Name",
+	    "required": false,
+	    "style": "form",
+	    "explode": false,
+	    "schema": {
+	      "type": "string"
+	    }
+	  }
+	],
+	"responses": {
+	  "200": {
+	    "content": {
+	      "application/json": {
+		"schema": {
+		  "$ref": "#/components/schemas/dbv0.0.38_response_associations_delete"
+		}
+	      },
+	      "application/x-yaml": {
+		"schema": {
+		  "$ref": "#/components/schemas/dbv0.0.38_response_associations_delete"
+		}
+	      }
+	    },
+	    "description": "Delete associations"
+	  },
+	  "default": {
+	    "description": "Associations not found or unable to delete association",
+	    "content": {
+	      "application/json": {
+		"schema": {
+		  "$ref": "#/components/schemas/dbv0.0.38_errors"
+		}
+	      },
+	      "application/x-yaml": {
+		"schema": {
+		  "$ref": "#/components/schemas/dbv0.0.38_errors"
+		}
+	      }
+	    }
+	  }
+	},
+	"summary": "Delete associations"
+      }
+    },
+    "/association/": {
+      "get": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "get_association",
+        "parameters": [
+          {
+            "name": "cluster",
+            "in": "query",
+            "description": "Cluster name",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "name": "account",
+            "in": "query",
+            "description": "Account name",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "name": "user",
+            "in": "query",
+            "description": "User name",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "name": "partition",
+            "in": "query",
+            "description": "Partition Name",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          }
+        ],
+        "responses": {
+          "200": {
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_associations_info"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_associations_info"
+                }
+              }
+            },
+            "description": "List of associations"
+          },
+          "default": {
+            "description": "Association not found",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              }
+            }
+          }
+        },
+        "summary": "Get association info"
+      },
+      "delete": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "delete_association",
+        "parameters": [
+          {
+            "name": "cluster",
+            "in": "query",
+            "description": "Cluster name",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "name": "account",
+            "in": "query",
+            "description": "Account name",
+	    "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "name": "user",
+            "in": "query",
+            "description": "User name",
+	    "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "name": "partition",
+            "in": "query",
+            "description": "Partition Name",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          }
+        ],
+        "responses": {
+          "200": {
+            "content": {
+              "application/json": {
+                "schema": {
+		  "$ref": "#/components/schemas/dbv0.0.38_response_associations_delete"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+		  "$ref": "#/components/schemas/dbv0.0.38_response_associations_delete"
+                }
+              }
+            },
+            "description": "Delete associations"
+          },
+          "default": {
+            "description": "Association not found or unable to delete association",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              }
+            }
+          }
+        },
+        "summary": "Delete association"
+      }
+    },
+    "/user/{user_name}": {
+      "delete": {
+        "tags": [
+          "slurm"
+        ],
+        "summary": "Delete user",
+        "operationId": "delete_user",
+        "responses": {
+          "200": {
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_response_user_delete"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_response_user_delete"
+                }
+              }
+            },
+            "description": "Delete user"
+          },
+          "default": {
+            "description": "User not found or unable to delete user",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              }
+            }
+          }
+        },
+        "parameters": [
+          {
+            "name": "user_name",
+            "in": "path",
+            "description": "Slurm User Name",
+            "required": true,
+            "style": "simple",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          }
+        ]
+      },
+      "get": {
+        "tags": [
+          "slurm"
+        ],
+        "summary": "Get user info",
+        "operationId": "get_user",
+        "responses": {
+          "200": {
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_user_info"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_user_info"
+                }
+              }
+            },
+            "description": "List of users"
+          },
+          "default": {
+            "description": "User not found",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              }
+            }
+          }
+        },
+        "parameters": [
+          {
+            "name": "user_name",
+            "in": "path",
+            "description": "Slurm User Name",
+            "required": true,
+            "style": "simple",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "name": "with_deleted",
+            "in": "query",
+            "description": "Include deleted users. False by default.",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "boolean"
+            }
+          }
+        ]
+      }
+    },
+    "/users/": {
+      "post": {
+        "tags": [
+          "slurm"
+        ],
+        "summary": "Update user",
+        "operationId": "update_users",
+        "requestBody": {
+          "description": "add or update user",
+          "content": {
+            "application/json": {
+              "schema": {
+                "$ref": "#/components/schemas/dbv0.0.38_update_users"
+              }
+            },
+            "application/x-yaml": {
+              "schema": {
+                "$ref": "#/components/schemas/dbv0.0.38_update_users"
+              }
+            }
+          },
+          "required": true
+        },
+        "responses": {
+          "200": {
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_response_user_update"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_response_user_update"
+                }
+              }
+            },
+            "description": "Update users"
+          },
+          "default": {
+            "description": "User not found or not able to update user",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              }
+            }
+          }
+        }
+      },
+      "get": {
+        "tags": [
+          "slurm"
+        ],
+        "summary": "Get user list",
+        "operationId": "get_users",
+        "responses": {
+          "200": {
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_user_info"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_user_info"
+                }
+              }
+            },
+            "description": "List of users"
+          },
+          "default": {
+            "description": "User not found",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              }
+            }
+          }
+        },
+        "parameters": [
+          {
+            "name": "with_deleted",
+            "in": "query",
+            "description": "Include deleted users. False by default.",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "boolean"
+            }
+          }
+        ]
+      }
+    },
+    "/cluster/{cluster_name}": {
+      "delete": {
+        "tags": [
+          "slurm"
+        ],
+        "summary": "Delete cluster",
+        "operationId": "delete_cluster",
+        "responses": {
+          "200": {
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_response_cluster_delete"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_response_cluster_delete"
+                }
+              }
+            },
+            "description": "Delete cluster"
+          },
+          "default": {
+            "description": "Cluster not found or unable to delete cluster",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              }
+            }
+          }
+        },
+        "parameters": [
+          {
+            "name": "cluster_name",
+            "in": "path",
+            "description": "Slurm cluster name",
+            "required": true,
+            "style": "simple",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          }
+        ]
+      },
+      "get": {
+        "tags": [
+          "slurm"
+        ],
+        "summary": "Get cluster info",
+        "operationId": "get_cluster",
+        "responses": {
+          "200": {
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_cluster_info"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_cluster_info"
+                }
+              }
+            },
+            "description": "Cluster information"
+          },
+          "default": {
+            "description": "Cluster not found",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              }
+            }
+          }
+        },
+        "parameters": [
+          {
+            "name": "cluster_name",
+            "in": "path",
+            "description": "Slurm cluster name",
+            "required": true,
+            "style": "simple",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          }
+        ]
+      }
+    },
+    "/clusters/": {
+      "get": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "get_clusters",
+        "responses": {
+          "200": {
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_cluster_info"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_cluster_info"
+                }
+              }
+            },
+            "description": "List of clusters"
+          },
+          "default": {
+            "description": "Cluster not found",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              }
+            }
+          }
+        },
+        "summary": "Get cluster list"
+      },
+      "post": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "add_clusters",
+        "responses": {
+          "200": {
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_response_cluster_add"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_response_cluster_add"
+                }
+              }
+            },
+            "description": "List of clusters"
+          },
+          "default": {
+            "description": "Unable to add cluster",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              }
+            }
+          }
+        },
+        "summary": "Add clusters",
+        "requestBody": {
+          "description": "Add or update clusters",
+          "content": {
+            "application/json": {
+              "schema": {
+                "$ref": "#/components/schemas/dbv0.0.38_clusters_properties"
+              }
+            },
+            "application/x-yaml": {
+              "schema": {
+                "$ref": "#/components/schemas/dbv0.0.38_clusters_properties"
+              }
+            }
+          },
+          "required": true
+        }
+      }
+    },
+    "/wckey/{wckey}": {
+      "delete": {
+        "tags": [
+          "slurm"
+        ],
+        "summary": "Delete wckey",
+        "operationId": "delete_wckey",
+        "responses": {
+          "200": {
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_response_wckey_delete"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_response_wckey_delete"
+                }
+              }
+            },
+            "description": "Delete wckey"
+          },
+          "default": {
+            "description": "wckey not found or unable to delete wckey",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              }
+            }
+          }
+        },
+        "parameters": [
+          {
+            "name": "wckey",
+            "in": "path",
+            "description": "Slurm wckey name",
+            "required": true,
+            "style": "simple",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          }
+        ]
+      },
+      "get": {
+        "tags": [
+          "slurm"
+        ],
+        "summary": "Get wckey info",
+        "operationId": "get_wckey",
+        "responses": {
+          "200": {
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_wckey_info"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_wckey_info"
+                }
+              }
+            },
+            "description": "List of wckey"
+          },
+          "default": {
+            "description": "wckey not found",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              }
+            }
+          }
+        },
+        "parameters": [
+          {
+            "name": "wckey",
+            "in": "path",
+            "description": "Slurm wckey name",
+            "required": true,
+            "style": "simple",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          }
+        ]
+      }
+    },
+    "/wckeys/": {
+      "get": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "get_wckeys",
+        "responses": {
+          "200": {
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_wckey_info"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_wckey_info"
+                }
+              }
+            },
+            "description": "List of wckeys"
+          },
+          "default": {
+            "description": "wckey not found",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              }
+            }
+          }
+        },
+        "summary": "Get wckey list"
+      },
+      "post": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "add_wckeys",
+        "responses": {
+          "200": {
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_response_wckey_add"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_response_wckey_add"
+                }
+              }
+            },
+            "description": "List of wckeys"
+          },
+          "default": {
+            "description": "Unable to add wckey",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              }
+            }
+          }
+        },
+        "summary": "Add wckeys"
+      }
+    },
+    "/account/{account_name}": {
+      "delete": {
+        "tags": [
+          "slurm"
+        ],
+        "summary": "Delete account",
+        "operationId": "delete_account",
+        "responses": {
+          "200": {
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_response_account_delete"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_response_account_delete"
+                }
+              }
+            },
+            "description": "Delete account"
+          },
+          "default": {
+            "description": "Unable to delete account",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              }
+            }
+          }
+        },
+        "parameters": [
+          {
+            "name": "account_name",
+            "in": "path",
+            "description": "Slurm Account Name",
+            "required": true,
+            "style": "simple",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          }
+        ]
+      },
+      "get": {
+        "tags": [
+          "slurm"
+        ],
+        "summary": "Get account info",
+        "operationId": "get_account",
+        "responses": {
+          "200": {
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_account_info"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_account_info"
+                }
+              }
+            },
+            "description": "List of accounts"
+          },
+          "default": {
+            "description": "Account not found",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              }
+            }
+          }
+        },
+        "parameters": [
+          {
+            "name": "account_name",
+            "in": "path",
+            "description": "Slurm Account Name",
+            "required": true,
+            "style": "simple",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "name": "with_deleted",
+            "in": "query",
+            "description": "Include deleted accounts. False by default.",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "boolean"
+            }
+          }
+        ]
+      }
+    },
+    "/accounts/": {
+      "get": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "get_accounts",
+        "parameters": [
+          {
+            "name": "with_deleted",
+            "in": "query",
+            "description": "Include deleted accounts. False by default.",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "boolean"
+            }
+          }
+        ],
+        "responses": {
+          "200": {
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_account_info"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_account_info"
+                }
+              }
+            },
+            "description": "List of accounts"
+          },
+          "default": {
+            "description": "Account not found",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              }
+            }
+          }
+        },
+        "summary": "Get account list"
+      },
+      "post": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "update_account",
+        "responses": {
+          "200": {
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_account_response"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_account_response"
+                }
+              }
+            },
+            "description": "Add/update list of accounts"
+          },
+          "default": {
+            "description": "Unable to add or update accounts",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              }
+            }
+          }
+        },
+        "summary": "Update accounts",
+        "requestBody": {
+          "description": "update/create accounts",
+          "content": {
+            "application/json": {
+              "schema": {
+                "$ref": "#/components/schemas/dbv0.0.38_update_account"
+              }
+            },
+            "application/x-yaml": {
+              "schema": {
+                "$ref": "#/components/schemas/dbv0.0.38_update_account"
+              }
+            }
+          },
+          "required": true
+        }
+      }
+    },
+    "/jobs/": {
+      "get": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "get_jobs",
+        "parameters": [
+          {
+            "in": "query",
+            "name": "submit_time",
+            "description": "Filter by submission time\n Accepted formats:\n HH:MM[:SS] [AM|PM]\r\nMMDD[YY] or MM/DD[/YY] or MM.DD[.YY]\r\nMM/DD[/YY]-HH:MM[:SS]\r\nYYYY-MM-DD[THH:MM[:SS]]",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "in": "query",
+            "name": "start_time",
+            "description": "Filter by start time\n Accepted formats:\n HH:MM[:SS] [AM|PM]\r\nMMDD[YY] or MM/DD[/YY] or MM.DD[.YY]\r\nMM/DD[/YY]-HH:MM[:SS]\r\nYYYY-MM-DD[THH:MM[:SS]]",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "in": "query",
+            "name": "end_time",
+            "description": "Filter by end time\n Accepted formats:\n HH:MM[:SS] [AM|PM]\r\nMMDD[YY] or MM/DD[/YY] or MM.DD[.YY]\r\nMM/DD[/YY]-HH:MM[:SS]\r\nYYYY-MM-DD[THH:MM[:SS]]",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "in": "query",
+            "name": "account",
+            "description": "Comma delimited list of accounts to match",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "in": "query",
+            "name": "association",
+            "description": "Comma delimited list of associations to match",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "in": "query",
+            "name": "cluster",
+            "description": "Comma delimited list of cluster to match",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "in": "query",
+            "name": "constraints",
+            "description": "Comma delimited list of constraints to match",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "in": "query",
+            "name": "cpus_max",
+            "description": "Number of CPUs high range",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "in": "query",
+            "name": "cpus_min",
+            "description": "Number of CPUs low range",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "in": "query",
+            "name": "skip_steps",
+            "description": "Report job step information",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "boolean"
+            }
+          },
+          {
+            "in": "query",
+            "name": "disable_wait_for_result",
+            "description": "Disable waiting for result from slurmdbd",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "boolean"
+            }
+          },
+          {
+            "in": "query",
+            "name": "exit_code",
+            "description": "Exit code of job",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "in": "query",
+            "name": "format",
+            "description": "Comma delimited list of formats to match",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "in": "query",
+            "name": "group",
+            "description": "Comma delimited list of groups to match",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "in": "query",
+            "name": "job_name",
+            "description": "Comma delimited list of job names to match",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "in": "query",
+            "name": "nodes_max",
+            "description": "Number of nodes high range",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "in": "query",
+            "name": "nodes_min",
+            "description": "Number of nodes low range",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "in": "query",
+            "name": "partition",
+            "description": "Comma delimited list of partitions to match",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "in": "query",
+            "name": "qos",
+            "description": "Comma delimited list of QOS to match",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "in": "query",
+            "name": "reason",
+            "description": "Comma delimited list of job reasons to match",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "in": "query",
+            "name": "reservation",
+            "description": "Comma delimited list of reservations to match",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "in": "query",
+            "name": "state",
+            "description": "Comma delimited list of states to match",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "in": "query",
+            "name": "step",
+            "description": "Comma delimited list of job steps to match",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "in": "query",
+            "name": "node",
+            "description": "Comma delimited list of used nodes to match",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "in": "query",
+            "name": "wckey",
+            "description": "Comma delimited list of wckeys to match",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          }
+        ],
+        "responses": {
+          "200": {
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_job_info"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_job_info"
+                }
+              }
+            },
+            "description": "List of jobs"
+          },
+          "default": {
+            "description": "Unable to query jobs",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              }
+            }
+          }
+        },
+        "summary": "Get job list"
+      }
+    },
+    "/diag/": {
+      "get": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "diag",
+        "responses": {
+          "200": {
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_diag"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_diag"
+                }
+              }
+            },
+            "description": "Dictionary of statistics"
+          },
+          "default": {
+            "description": "Unable to query diagnostics",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/dbv0.0.38_errors"
+                }
+              }
+            }
+          }
+        },
+        "summary": "Get slurmdb diagnostics"
+      }
+    },
+    "/openapi": {
+      "get": {
+        "tags": [
+          "openapi"
+        ],
+        "responses": {
+          "200": {
+            "description": "OpenAPI Specification"
+          }
+        },
+        "summary": "Retrieve OpenAPI Specification"
+      },
+      "servers": [
+        {
+          "url": "/"
+        }
+      ]
+    },
+    "/openapi.json": {
+      "get": {
+        "tags": [
+          "openapi"
+        ],
+        "responses": {
+          "200": {
+            "description": "OpenAPI Specification"
+          }
+        },
+        "summary": "Retrieve OpenAPI Specification"
+      },
+      "servers": [
+        {
+          "url": "/"
+        }
+      ]
+    },
+    "/openapi.yaml": {
+      "get": {
+        "tags": [
+          "openapi"
+        ],
+        "responses": {
+          "200": {
+            "description": "OpenAPI Specification"
+          }
+        },
+        "summary": "Retrieve OpenAPI Specification"
+      },
+      "servers": [
+        {
+          "url": "/"
+        }
+      ]
+    },
+    "/openapi/v3": {
+      "get": {
+        "tags": [
+          "openapi"
+        ],
+        "responses": {
+          "200": {
+            "description": "OpenAPI Specification"
+          }
+        },
+        "summary": "Retrieve OpenAPI Specification"
+      },
+      "servers": [
+        {
+          "url": "/"
+        }
+      ]
+    }
+  },
+  "components": {
+    "securitySchemes": {
+      "user": {
+        "type": "apiKey",
+        "description": "User name",
+        "name": "X-SLURM-USER-NAME",
+        "in": "header"
+      },
+      "token": {
+        "type": "apiKey",
+        "description": "User access token",
+        "name": "X-SLURM-USER-TOKEN",
+        "in": "header"
+      }
+    },
+    "schemas": {
+      "dbv0.0.38_diag": {
+        "properties": {
+          "meta": {
+            "$ref": "#/components/schemas/v0.0.38_meta"
+          },
+          "errors": {
+            "type": "array",
+            "description": "Slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_error"
+            }
+          },
+          "statistics": {
+            "type": "object",
+            "properties": {
+              "time_start": {
+                "type": "integer",
+                "description": "Unix timestamp of start time"
+              },
+              "rollups": {
+                "type": "array",
+                "items": {
+                  "type": "object",
+                  "description": "Rollup statistics",
+                  "properties": {
+                    "type": {
+                      "type": "string",
+                      "description": "Type of rollup"
+                    },
+                    "last_run": {
+                      "type": "integer",
+                      "description": "Timestamp of last rollup"
+                    },
+                    "last_cycle": {
+                      "type": "integer",
+                      "description": "Timestamp of last cycle"
+                    },
+                    "max_cycle": {
+                      "type": "integer",
+                      "description": "Max time of all cycles"
+                    },
+                    "total_time": {
+                      "type": "integer",
+                      "description": "Total time (s) spent doing rollup"
+                    },
+                    "mean_cycles": {
+                      "type": "integer",
+                      "description": "Average time (s) of cycle"
+                    }
+                  }
+                }
+              },
+              "RPCs": {
+                "type": "array",
+                "items": {
+                  "type": "object",
+                  "description": "Statistics by RPC type",
+                  "properties": {
+                    "rpc": {
+                      "type": "string",
+                      "description": "RPC type"
+                    },
+                    "count": {
+                      "type": "integer",
+                      "description": "Number of RPCs"
+                    },
+                    "time": {
+                      "type": "object",
+                      "description": "Time values",
+                      "properties": {
+                        "average": {
+                          "type": "integer",
+                          "description": "Average time spent processing this RPC type"
+                        },
+                        "total": {
+                          "type": "integer",
+                          "description": "Total time spent processing this RPC type"
+                        }
+                      }
+                    }
+                  }
+                }
+              },
+              "users": {
+                "type": "array",
+                "items": {
+                  "type": "object",
+                  "description": "Statistics by user RPCs",
+                  "properties": {
+                    "user": {
+                      "type": "string",
+                      "description": "User name"
+                    },
+                    "count": {
+                      "type": "integer",
+                      "description": "Number of RPCs"
+                    },
+                    "time": {
+                      "type": "object",
+                      "description": "Time values",
+                      "properties": {
+                        "average": {
+                          "type": "integer",
+                          "description": "Average time spent processing each user RPC"
+                        },
+                        "total": {
+                          "type": "integer",
+                          "description": "Total time spent processing each user RPC"
+                        }
+                      }
+                    }
+                  }
+                }
+              }
+            }
+          }
+        }
+      },
+      "dbv0.0.38_account": {
+        "type": "object",
+        "description": "Account description",
+        "properties": {
+          "associations": {
+            "type": "array",
+            "description": "List of assigned associations",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_association_short_info"
+            }
+          },
+          "coordinators": {
+            "type": "array",
+            "description": "List of assigned coordinators",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_coordinator_info"
+            }
+          },
+          "description": {
+            "type": "string",
+            "description": "Description of account"
+          },
+          "name": {
+            "type": "string",
+            "description": "Name of account"
+          },
+          "organization": {
+            "type": "string",
+            "description": "Assigned organization of account"
+          },
+          "flags": {
+            "type": "array",
+            "description": "List of properties of account",
+            "items": {
+              "type": "string",
+              "description": "String of property name"
+            }
+          }
+        }
+      },
+      "dbv0.0.38_account_info": {
+        "type": "object",
+        "properties": {
+          "meta": {
+            "$ref": "#/components/schemas/v0.0.38_meta"
+          },
+          "errors": {
+            "type": "array",
+            "description": "Slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_error"
+            }
+          },
+          "accounts": {
+            "type": "array",
+            "description": "List of accounts",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_account"
+            }
+          }
+        }
+      },
+      "dbv0.0.38_coordinator_info": {
+        "type": "object",
+        "properties": {
+          "name": {
+            "type": "string",
+            "description": "Name of user"
+          },
+          "direct": {
+            "type": "integer",
+            "description": "If user is coordinator of this account directly or coordinator status was inheirted from a higher account in the tree"
+          }
+        }
+      },
+      "dbv0.0.38_association_short_info": {
+        "type": "object",
+        "properties": {
+          "account": {
+            "type": "string",
+            "description": "Account name"
+          },
+          "cluster": {
+            "type": "string",
+            "description": "Cluster name"
+          },
+          "partition": {
+            "type": "string",
+            "description": "Partition name (optional)"
+          },
+          "user": {
+            "type": "string",
+            "description": "User name"
+          }
+        }
+      },
+      "dbv0.0.38_response_account_delete": {
+        "type": "object",
+        "properties": {
+          "meta": {
+            "$ref": "#/components/schemas/v0.0.38_meta"
+          },
+          "errors": {
+            "type": "array",
+            "description": "Slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_error"
+            }
+          }
+        }
+      },
+      "dbv0.0.38_response_wckey_add": {
+        "type": "object",
+        "properties": {
+          "meta": {
+            "$ref": "#/components/schemas/v0.0.38_meta"
+          },
+          "errors": {
+            "type": "array",
+            "description": "Slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_error"
+            }
+          }
+        }
+      },
+      "dbv0.0.38_wckey_info": {
+        "type": "object",
+        "properties": {
+          "meta": {
+            "$ref": "#/components/schemas/v0.0.38_meta"
+          },
+          "errors": {
+            "type": "array",
+            "description": "Slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_error"
+            }
+          },
+          "wckeys": {
+            "type": "array",
+            "description": "List of wckeys",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_wckey"
+            }
+          }
+        }
+      },
+      "dbv0.0.38_wckey": {
+        "type": "object",
+        "properties": {
+          "accounts": {
+            "type": "array",
+            "description": "List of assigned accounts",
+            "items": {
+              "type": "string",
+              "description": "Account name"
+            }
+          },
+          "cluster": {
+            "type": "string",
+            "description": "Cluster name"
+          },
+          "id": {
+            "type": "integer",
+            "description": "wckey database unique id"
+          },
+          "name": {
+            "type": "string",
+            "description": "wckey name"
+          },
+          "user": {
+            "type": "string",
+            "description": "wckey user"
+          },
+          "flags": {
+            "type": "array",
+            "description": "List of properties of wckey",
+            "items": {
+              "type": "string",
+              "description": "String of property name"
+            }
+          }
+        }
+      },
+      "dbv0.0.38_response_wckey_delete": {
+        "type": "object",
+        "properties": {
+          "meta": {
+            "$ref": "#/components/schemas/v0.0.38_meta"
+          },
+          "errors": {
+            "type": "array",
+            "description": "Slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_error"
+            }
+          }
+        }
+      },
+      "dbv0.0.38_response_cluster_add": {
+        "type": "object",
+        "properties": {
+          "meta": {
+            "$ref": "#/components/schemas/v0.0.38_meta"
+          },
+          "errors": {
+            "type": "array",
+            "description": "Slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_error"
+            }
+          }
+        }
+      },
+      "dbv0.0.38_cluster_info": {
+        "type": "object",
+        "properties": {
+          "controller": {
+            "type": "object",
+            "description": "Information about controller",
+            "properties": {
+              "host": {
+                "type": "string",
+                "description": "Hostname"
+              },
+              "port": {
+                "type": "integer",
+                "description": "Port number"
+              }
+            }
+          },
+          "flags": {
+            "type": "array",
+            "description": "List of properties of cluster",
+            "items": {
+              "type": "string",
+              "description": "String of property name"
+            }
+          },
+          "name": {
+            "type": "string",
+            "description": "Cluster name"
+          },
+          "nodes": {
+            "type": "string",
+            "description": "Assigned nodes"
+          },
+          "select_plugin": {
+            "type": "string",
+            "description": "Configured select plugin"
+          },
+          "associations": {
+            "type": "object",
+            "description": "Information about associations",
+            "properties": {
+              "root": {
+                "$ref": "#/components/schemas/dbv0.0.38_association_short_info"
+              }
+            }
+          },
+          "rpc_version": {
+            "type": "integer",
+            "description": "Number rpc version"
+          },
+          "tres": {
+            "type": "array",
+            "description": "List of TRES in cluster",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_response_tres"
+            }
+          }
+        }
+      },
+      "dbv0.0.38_response_cluster_delete": {
+        "type": "object",
+        "properties": {
+          "meta": {
+            "$ref": "#/components/schemas/v0.0.38_meta"
+          },
+          "errors": {
+            "type": "array",
+            "description": "Slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_error"
+            }
+          }
+        }
+      },
+      "dbv0.0.38_response_user_update": {
+        "type": "object",
+        "properties": {
+          "meta": {
+            "$ref": "#/components/schemas/v0.0.38_meta"
+          },
+          "errors": {
+            "type": "array",
+            "description": "Slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_error"
+            }
+          }
+        }
+      },
+      "dbv0.0.38_user": {
+        "type": "object",
+        "description": "User description",
+        "properties": {
+          "administrator_level": {
+            "type": "string",
+            "description": "Description of administrator level"
+          },
+          "associations": {
+            "type": "array",
+            "description": "Assigned associations",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_association_short_info"
+            }
+          },
+          "coordinators": {
+            "type": "array",
+            "description": "List of assigned coordinators",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_coordinator_info"
+            }
+          },
+          "default": {
+            "type": "object",
+            "description": "Default settings",
+            "properties": {
+              "account": {
+                "type": "string",
+                "description": "Default account name"
+              },
+              "wckey": {
+                "type": "string",
+                "description": "Default wckey"
+              }
+            }
+          },
+          "flags": {
+            "type": "array",
+            "description": "List of properties of user",
+            "items": {
+              "type": "string",
+              "description": "String of property name"
+            }
+          },
+          "name": {
+            "type": "string",
+            "description": "User name"
+          }
+        }
+      },
+      "dbv0.0.38_user_info": {
+        "type": "object",
+        "properties": {
+          "meta": {
+            "$ref": "#/components/schemas/v0.0.38_meta"
+          },
+          "errors": {
+            "type": "array",
+            "description": "Slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_error"
+            }
+          },
+          "users": {
+            "type": "array",
+            "description": "Array of users",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_user"
+            }
+          }
+        }
+      },
+      "dbv0.0.38_response_user_delete": {
+        "type": "object",
+        "properties": {
+          "meta": {
+            "$ref": "#/components/schemas/v0.0.38_meta"
+          },
+          "errors": {
+            "type": "array",
+            "description": "Slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_error"
+            }
+          }
+        }
+      },
+      "dbv0.0.38_association": {
+        "type": "object",
+        "description": "Association description",
+        "properties": {
+          "account": {
+            "type": "string",
+            "description": "Assigned account"
+          },
+          "cluster": {
+            "type": "string",
+            "description": "Assigned cluster"
+          },
+          "default": {
+            "type": "object",
+            "description": "Default settings",
+            "properties": {
+              "qos": {
+                "type": "string",
+                "description": "Default QOS"
+              }
+            }
+          },
+          "flags": {
+            "type": "array",
+            "description": "List of properties of association",
+            "items": {
+              "type": "string",
+              "description": "String of property name"
+            }
+          },
+          "max": {
+            "type": "object",
+            "description": "Max settings",
+            "properties": {
+              "jobs": {
+                "type": "object",
+                "description": "Max jobs settings",
+                "properties": {
+                  "per": {
+                    "type": "object",
+                    "description": "Max jobs per settings",
+                    "properties": {
+                      "wall_clock": {
+                        "type": "integer",
+                        "description": "Max wallclock per job"
+                      }
+                    }
+                  }
+                }
+              },
+              "per": {
+                "type": "object",
+                "description": "Max per settings",
+                "properties": {
+                  "account": {
+                    "type": "object",
+                    "description": "Max per accounting settings",
+                    "properties": {
+                      "wall_clock": {
+                        "type": "integer",
+                        "description": "Max wallclock per account"
+                      }
+                    }
+                  }
+                }
+              },
+              "tres": {
+                "type": "object",
+                "description": "Max TRES settings",
+                "properties": {
+                  "per": {
+                    "type": "object",
+                    "description": "Max TRES per settings",
+                    "properties": {
+                      "job": {
+                        "$ref": "#/components/schemas/dbv0.0.38_tres_list"
+                      },
+                      "node": {
+                        "$ref": "#/components/schemas/dbv0.0.38_tres_list"
+                      }
+                    }
+                  },
+                  "total": {
+                    "$ref": "#/components/schemas/dbv0.0.38_tres_list"
+                  },
+                  "minutes": {
+                    "type": "object",
+                    "description": "Max TRES minutes settings",
+                    "properties": {
+                      "per": {
+                        "type": "object",
+                        "description": "Max TRES minutes per settings",
+                        "properties": {
+                          "job": {
+                            "$ref": "#/components/schemas/dbv0.0.38_tres_list"
+                          }
+                        }
+                      },
+                      "total": {
+                        "$ref": "#/components/schemas/dbv0.0.38_tres_list"
+                      }
+                    }
+                  }
+                }
+              }
+            }
+          },
+          "min": {
+            "type": "object",
+            "description": "Min settings",
+            "properties": {
+              "priority_threshold": {
+                "type": "integer",
+                "description": "Min priority threshold"
+              }
+            }
+          },
+          "parent_account": {
+            "type": "string",
+            "description": "Parent account name"
+          },
+          "partition": {
+            "type": "string",
+            "description": "Assigned partition"
+          },
+          "priority": {
+            "type": "integer",
+            "description": "Assigned priority"
+          },
+          "qos": {
+            "type": "array",
+            "description": "Assigned QOS",
+            "items": {
+              "type": "string",
+              "description": "Assigned single QOS name"
+            }
+          },
+          "shares_raw": {
+            "type": "integer",
+            "description": "Raw fairshare shares"
+          },
+          "usage": {
+            "type": "object",
+            "description": "Association usage",
+            "properties": {
+              "accrue_job_count": {
+                "type": "integer",
+                "description": "Jobs accuring priority"
+              },
+              "group_used_wallclock": {
+                "type": "number",
+                "description": "Group used wallclock time (s)"
+              },
+              "fairshare_factor": {
+                "type": "number",
+                "description": "Fairshare factor"
+              },
+              "fairshare_shares": {
+                "type": "integer",
+                "description": "Fairshare shares"
+              },
+              "normalized_priority": {
+                "type": "integer",
+                "description": "Currently active jobs"
+              },
+              "normalized_shares": {
+                "type": "number",
+                "description": "Normalized shares"
+              },
+              "effective_normalized_usage": {
+                "type": "number",
+                "description": "Effective normalized usage"
+              },
+              "raw_usage": {
+                "type": "integer",
+                "description": "Raw usage"
+              },
+              "job_count": {
+                "type": "integer",
+                "description": "Total jobs submitted"
+              },
+              "fairshare_level": {
+                "type": "number",
+                "description": "Fairshare level"
+              }
+            }
+          },
+          "user": {
+            "type": "string",
+            "description": "Assigned user"
+          }
+        }
+      },
+      "dbv0.0.38_associations_info": {
+        "type": "object",
+        "properties": {
+          "meta": {
+            "$ref": "#/components/schemas/v0.0.38_meta"
+          },
+          "errors": {
+            "type": "array",
+            "description": "Slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_error"
+            }
+          },
+          "associations": {
+            "type": "array",
+            "description": "Array of associations",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_association"
+            }
+          }
+        }
+      },
+      "dbv0.0.38_qos": {
+        "type": "object",
+        "description": "QOS description",
+        "properties": {
+          "description": {
+            "type": "string",
+            "description": "QOS description"
+          },
+          "flags": {
+            "type": "array",
+            "description": "List of properties of QOS",
+            "items": {
+              "type": "string",
+              "description": "String of property name"
+            }
+          },
+          "id": {
+            "type": "string",
+            "description": "Database id"
+          },
+          "limits": {
+            "type": "object",
+            "description": "Assigned limits",
+            "properties": {
+              "factor": {
+                "type": "number",
+                "description": "factor to apply to TRES count for associations using this QOS"
+              },
+              "max": {
+                "type": "object",
+                "description": "Limits on max settings",
+                "properties": {
+                  "wall_clock": {
+                    "type": "object",
+                    "description": "Limit on wallclock settings",
+                    "properties": {
+                      "per": {
+                        "type": "object",
+                        "description": "Limit on wallclock per settings",
+                        "properties": {
+                          "qos": {
+                            "type": "integer",
+                            "description": "Max wallclock per QOS"
+                          },
+                          "job": {
+                            "type": "integer",
+                            "description": "Max wallclock per job"
+                          }
+                        }
+                      }
+                    }
+                  },
+                  "jobs": {
+                    "type": "object",
+                    "description": "Limits on jobs settings",
+                    "properties": {
+                      "active_jobs": {
+                        "type": "object",
+                        "description": "Limits on active jobs settings",
+                        "properties": {
+                          "per": {
+                            "type": "object",
+                            "description": "Limits on active jobs per settings",
+                            "properties": {
+                              "account": {
+                                "type": "integer",
+                                "description": "Max jobs per account"
+                              },
+                              "user": {
+                                "type": "integer",
+                                "description": "Max jobs per user"
+                              }
+                            }
+                          }
+                        }
+                      }
+                    }
+                  },
+                  "accruing": {
+                    "type": "object",
+                    "description": "Limits on accruing priority",
+                    "properties": {
+                      "per": {
+                        "type": "object",
+                        "description": "Max accuring priority per setting",
+                        "properties": {
+                          "account": {
+                            "type": "integer",
+                            "description": "Max accuring priority per account"
+                          },
+                          "user": {
+                            "type": "integer",
+                            "description": "Max accuring priority per user"
+                          }
+                        }
+                      }
+                    }
+                  },
+                  "tres": {
+                    "type": "object",
+                    "description": "Limits on TRES",
+                    "properties": {
+                      "minutes": {
+                        "type": "object",
+                        "description": "Max TRES minutes settings",
+                        "properties": {
+                          "per": {
+                            "type": "object",
+                            "description": "Max TRES minutes per settings",
+                            "properties": {
+                              "job": {
+                                "$ref": "#/components/schemas/dbv0.0.38_tres_list"
+                              },
+                              "account": {
+                                "$ref": "#/components/schemas/dbv0.0.38_tres_list"
+                              },
+                              "user": {
+                                "$ref": "#/components/schemas/dbv0.0.38_tres_list"
+                              },
+                              "qos": {
+                                "$ref": "#/components/schemas/dbv0.0.38_tres_list"
+                              }
+                            }
+                          }
+                        }
+                      },
+                      "per": {
+                        "type": "object",
+                        "description": "Max TRES per settings",
+                        "properties": {
+                          "account": {
+                            "$ref": "#/components/schemas/dbv0.0.38_tres_list"
+                          },
+                          "job": {
+                            "$ref": "#/components/schemas/dbv0.0.38_tres_list"
+                          },
+                          "node": {
+                            "$ref": "#/components/schemas/dbv0.0.38_tres_list"
+                          },
+                          "user": {
+                            "$ref": "#/components/schemas/dbv0.0.38_tres_list"
+                          }
+                        }
+                      }
+                    }
+                  }
+                }
+              },
+              "min": {
+                "type": "object",
+                "description": "Min limit settings",
+                "properties": {
+                  "priority_threshold": {
+                    "type": "integer",
+                    "description": "Min priority threshold"
+                  },
+                  "tres": {
+                    "type": "object",
+                    "description": "Min tres settings",
+                    "properties": {
+                      "per": {
+                        "type": "object",
+                        "description": "Min tres per settings",
+                        "properties": {
+                          "job": {
+                            "$ref": "#/components/schemas/dbv0.0.38_tres_list"
+                          }
+                        }
+                      }
+                    }
+                  }
+                }
+              }
+            }
+          },
+          "preempt": {
+            "type": "object",
+            "description": "Preemption settings",
+            "properties": {
+              "list": {
+                "type": "array",
+                "description": "List of preemptable QOS",
+                "items": {
+                  "type": "string",
+                  "description": "Preemptable QOS"
+                }
+              },
+              "mode": {
+                "type": "array",
+                "description": "List of preemption modes",
+                "items": {
+                  "type": "string",
+                  "description": "Preemption mode"
+                }
+              },
+              "exempt_time": {
+                "type": "integer",
+                "description": "Grace period (s) before jobs can preempted"
+              }
+            }
+          },
+          "priority": {
+            "type": "integer",
+            "description": "QOS priority"
+          },
+          "usage_factor": {
+            "type": "number",
+            "description": "Usage factor"
+          },
+          "usage_threshold": {
+            "type": "number",
+            "description": "Usage threshold"
+          },
+          "name": {
+            "description": "Assigned name of QOS",
+            "type": "string"
+          }
+        }
+      },
+      "dbv0.0.38_qos_info": {
+        "type": "object",
+        "properties": {
+          "meta": {
+            "$ref": "#/components/schemas/v0.0.38_meta"
+          },
+          "errors": {
+            "type": "array",
+            "description": "Slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_error"
+            }
+          },
+          "qos": {
+            "type": "array",
+            "description": "Array of QOS",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_qos"
+            }
+          }
+        }
+      },
+      "dbv0.0.38_response_qos": {
+        "type": "object",
+        "properties": {
+          "meta": {
+            "$ref": "#/components/schemas/v0.0.38_meta"
+          },
+          "errors": {
+            "type": "array",
+            "description": "Slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_error"
+            }
+          }
+        }
+      },
+      "dbv0.0.38_response_qos_delete": {
+        "type": "object",
+        "properties": {
+          "meta": {
+            "$ref": "#/components/schemas/v0.0.38_meta"
+          },
+          "errors": {
+            "type": "array",
+            "description": "Slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_error"
+            }
+          }
+        }
+      },
+      "dbv0.0.38_response_associations": {
+        "type": "object",
+        "properties": {
+          "meta": {
+            "$ref": "#/components/schemas/v0.0.38_meta"
+          },
+          "errors": {
+            "type": "array",
+            "description": "Slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_error"
+            }
+          }
+        }
+      },
+      "dbv0.0.38_response_tres": {
+        "type": "object",
+        "properties": {
+          "meta": {
+            "$ref": "#/components/schemas/v0.0.38_meta"
+          },
+          "errors": {
+            "type": "array",
+            "description": "Slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_error"
+            }
+          }
+        }
+      },
+      "dbv0.0.38_tres_info": {
+        "type": "object",
+        "properties": {
+          "meta": {
+            "$ref": "#/components/schemas/v0.0.38_meta"
+          },
+          "errors": {
+            "type": "array",
+            "description": "Slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_error"
+            }
+          },
+          "tres": {
+            "$ref": "#/components/schemas/dbv0.0.38_tres_list"
+          }
+        }
+      },
+      "dbv0.0.38_tres_list": {
+        "type": "array",
+        "description": "TRES list of attributes",
+        "items": {
+          "type": "object",
+          "properties": {
+            "type": {
+              "type": "string",
+              "description": "TRES type"
+            },
+            "name": {
+              "type": "string",
+              "description": "TRES name (optional)"
+            },
+            "id": {
+              "type": "integer",
+              "description": "database id"
+            },
+            "count": {
+              "type": "integer",
+              "description": "count of TRES"
+            }
+          }
+        }
+      },
+      "dbv0.0.38_job_info": {
+        "type": "object",
+        "properties": {
+          "meta": {
+            "$ref": "#/components/schemas/v0.0.38_meta"
+          },
+          "errors": {
+            "type": "array",
+            "description": "Slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_error"
+            }
+          },
+          "jobs": {
+            "type": "array",
+            "description": "Array of jobs",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_job"
+            }
+          }
+        }
+      },
+      "dbv0.0.38_job": {
+        "type": "object",
+        "description": "Single job description",
+        "properties": {
+          "account": {
+            "type": "string",
+            "description": "Account charged by job"
+          },
+          "comment": {
+            "type": "object",
+            "description": "Job comments by type",
+            "properties": {
+              "administrator": {
+                "type": "string",
+                "description": "Administrator set comment"
+              },
+              "job": {
+                "type": "string",
+                "description": "Job comment"
+              },
+              "system": {
+                "type": "string",
+                "description": "System set comment"
+              }
+            }
+          },
+          "allocation_nodes": {
+            "type": "string",
+            "description": "Nodes allocated to job"
+          },
+          "array": {
+            "type": "object",
+            "description": "Array properties (optional)",
+            "properties": {
+              "job_id": {
+                "type": "integer",
+                "description": "Job id of array"
+              },
+              "limits": {
+                "type": "object",
+                "description": "Limits on array settings",
+                "properties": {
+                  "max": {
+                    "type": "object",
+                    "description": "Limits on array settings",
+                    "properties": {
+                      "running": {
+                        "type": "object",
+                        "description": "Limits on array settings",
+                        "properties": {
+                          "tasks": {
+                            "type": "integer",
+                            "description": "Max running tasks in array at any one time"
+                          }
+                        }
+                      }
+                    }
+                  }
+                }
+              },
+              "task": {
+                "type": "string",
+                "description": "Array task"
+              },
+              "task_id": {
+                "type": "integer",
+                "description": "Array task id"
+              }
+            }
+          },
+          "time": {
+            "type": "object",
+            "description": "Time properties",
+            "properties": {
+              "elapsed": {
+                "type": "integer",
+                "description": "Total time elapsed"
+              },
+              "eligible": {
+                "type": "integer",
+                "description": "Total time eligible to run"
+              },
+              "end": {
+                "type": "integer",
+                "description": "Timestamp of when job ended"
+              },
+              "start": {
+                "type": "integer",
+                "description": "Timestamp of when job started"
+              },
+              "submission": {
+                "type": "integer",
+                "description": "Timestamp of when job submitted"
+              },
+              "suspended": {
+                "type": "integer",
+                "description": "Timestamp of when job last suspended"
+              },
+              "system": {
+                "type": "object",
+                "description": "System time values",
+                "properties": {
+                  "seconds": {
+                    "type": "integer",
+                    "description": "Total number of CPU-seconds used by the system on behalf of the process (in kernel mode), in seconds"
+                  },
+                  "microseconds": {
+                    "type": "integer",
+                    "description": "Total number of CPU-seconds used by the system on behalf of the process (in kernel mode), in microseconds"
+                  }
+                }
+              },
+              "total": {
+                "type": "object",
+                "description": "System time values",
+                "properties": {
+                  "seconds": {
+                    "type": "integer",
+                    "description": "Total number of CPU-seconds used by the job, in seconds"
+                  },
+                  "microseconds": {
+                    "type": "integer",
+                    "description": "Total number of CPU-seconds used by the job, in microseconds"
+                  }
+                }
+              },
+              "user": {
+                "type": "object",
+                "description": "User land time values",
+                "properties": {
+                  "seconds": {
+                    "type": "integer",
+                    "description": "Total number of CPU-seconds used by the job in user land, in seconds"
+                  },
+                  "microseconds": {
+                    "type": "integer",
+                    "description": "Total number of CPU-seconds used by the job in user land, in microseconds"
+                  }
+                }
+              },
+              "limit": {
+                "type": "integer",
+                "description": "Job wall clock time limit"
+              }
+            }
+          },
+          "association": {
+            "$ref": "#/components/schemas/dbv0.0.38_association_short_info"
+          },
+          "cluster": {
+            "type": "string",
+            "description": "Assigned cluster"
+          },
+          "constraints": {
+            "type": "string",
+            "description": "Constraints on job"
+          },
+          "derived_exit_code": {
+            "$ref": "#/components/schemas/dbv0.0.38_job_exit_code"
+          },
+          "exit_code": {
+            "$ref": "#/components/schemas/dbv0.0.38_job_exit_code"
+          },
+          "flags": {
+            "type": "array",
+            "description": "List of properties of job",
+            "items": {
+              "type": "string",
+              "description": "String of property name"
+            }
+          },
+          "group": {
+            "type": "string",
+            "description": "User's group to run job"
+          },
+          "het": {
+            "type": "object",
+            "description": "Heterogeneous Job details (optional)",
+            "properties": {
+              "job_id": {
+                "type": "integer",
+                "description": "Parent HetJob id"
+              },
+              "job_offset": {
+                "type": "integer",
+                "description": "Offset of this job to parent"
+              }
+            }
+          },
+          "job_id": {
+            "type": "integer",
+            "description": "Job id"
+          },
+          "name": {
+            "type": "string",
+            "description": "Assigned job name"
+          },
+          "mcs": {
+            "type": "object",
+            "description": "Multi-Category Security",
+            "properties": {
+              "label": {
+                "type": "string",
+                "description": "Assigned MCS label"
+              }
+            }
+          },
+          "nodes": {
+            "type": "string",
+            "description": "List of nodes allocated for job"
+          },
+          "partition": {
+            "type": "string",
+            "description": "Assigned job's partition"
+          },
+          "priority": {
+            "type": "integer",
+            "description": "Priority"
+          },
+          "qos": {
+            "type": "string",
+            "description": "Assigned qos name"
+          },
+          "required": {
+            "type": "object",
+            "description": "Job run requirements",
+            "properties": {
+              "CPUs": {
+                "type": "integer",
+                "description": "Required number of CPUs"
+              },
+              "memory": {
+                "type": "integer",
+                "description": "Required amount of memory (MiB)"
+              }
+            }
+          },
+          "kill_request_user": {
+            "type": "string",
+            "description": "User who requested job killed"
+          },
+          "reservation": {
+            "type": "object",
+            "description": "Reservation usage details",
+            "properties": {
+              "id": {
+                "type": "integer",
+                "description": "Database id of reservation"
+              },
+              "name": {
+                "type": "integer",
+                "description": "Name of reservation"
+              }
+            }
+          },
+          "state": {
+            "type": "object",
+            "description": "State properties of job",
+            "properties": {
+              "current": {
+                "type": "string",
+                "description": "Current state of job"
+              },
+              "reason": {
+                "type": "string",
+                "description": "Last reason job didn't run"
+              }
+            }
+          },
+          "steps": {
+            "type": "array",
+            "description": "Job step description",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_job_step"
+            }
+          },
+          "tres": {
+            "type": "object",
+            "description": "TRES settings",
+            "properties": {
+              "allocated": {
+                "$ref": "#/components/schemas/dbv0.0.38_tres_list"
+              },
+              "requested": {
+                "$ref": "#/components/schemas/dbv0.0.38_tres_list"
+              }
+            }
+          },
+          "user": {
+            "type": "string",
+            "description": "Job user"
+          },
+          "wckey": {
+            "type": "object",
+            "description": "Job assigned wckey details",
+            "properties": {
+              "wckey": {
+                "type": "string",
+                "description": "Job assigned wckey"
+              },
+              "flags": {
+                "type": "array",
+                "description": "wckey flags",
+                "items": {
+                  "type": "string",
+                  "description": "Flag string"
+                }
+              }
+            }
+          },
+          "working_directory": {
+            "type": "string",
+            "description": "Directory where job was initially started"
+          },
+          "container": {
+            "type": "string",
+            "description": "absolute path to OCI container bundle"
+          }
+        }
+      },
+      "dbv0.0.38_job_exit_code": {
+        "type": "object",
+        "properties": {
+          "status": {
+            "type": "string",
+            "description": "Job exit status"
+          },
+          "return_code": {
+            "type": "integer",
+            "description": "Return code from parent process"
+          },
+          "signal": {
+            "type": "object",
+            "description": "Signal details (if signaled)",
+            "properties": {
+              "signal_id": {
+                "type": "integer",
+                "description": "Signal number process received"
+              },
+              "name": {
+                "type": "string",
+                "description": "Name of signal received"
+              }
+            }
+          }
+        }
+      },
+      "dbv0.0.38_job_step": {
+        "type": "object",
+        "properties": {
+          "time": {
+            "type": "object",
+            "description": "Time properties",
+            "properties": {
+              "elapsed": {
+                "type": "integer",
+                "description": "Total time elapsed"
+              },
+              "end": {
+                "type": "integer",
+                "description": "Timestamp of when job ended"
+              },
+              "start": {
+                "type": "integer",
+                "description": "Timestamp of when job started"
+              },
+              "suspended": {
+                "type": "integer",
+                "description": "Timestamp of when job last suspended"
+              },
+              "system": {
+                "type": "object",
+                "description": "System time values",
+                "properties": {
+                  "seconds": {
+                    "type": "integer",
+                    "description": "Total number of CPU-seconds used by the system on behalf of the process (in kernel mode), in seconds"
+                  },
+                  "microseconds": {
+                    "type": "integer",
+                    "description": "Total number of CPU-seconds used by the system on behalf of the process (in kernel mode), in microseconds"
+                  }
+                }
+              },
+              "total": {
+                "type": "object",
+                "description": "System time values",
+                "properties": {
+                  "seconds": {
+                    "type": "integer",
+                    "description": "Total number of CPU-seconds used by the job, in seconds"
+                  },
+                  "microseconds": {
+                    "type": "integer",
+                    "description": "Total number of CPU-seconds used by the job, in microseconds"
+                  }
+                }
+              },
+              "user": {
+                "type": "object",
+                "description": "User land time values",
+                "properties": {
+                  "seconds": {
+                    "type": "integer",
+                    "description": "Total number of CPU-seconds used by the job in user land, in seconds"
+                  },
+                  "microseconds": {
+                    "type": "integer",
+                    "description": "Total number of CPU-seconds used by the job in user land, in microseconds"
+                  }
+                }
+              }
+            }
+          },
+          "exit_code": {
+            "$ref": "#/components/schemas/dbv0.0.38_job_exit_code"
+          },
+          "nodes": {
+            "type": "object",
+            "description": "Node details",
+            "properties": {
+              "count": {
+                "type": "integer",
+                "description": "Total number of nodes in step"
+              },
+              "range": {
+                "type": "string",
+                "description": "Nodes in step"
+              }
+            }
+          },
+          "tasks": {
+            "type": "object",
+            "description": "Task properties",
+            "properties": {
+              "count": {
+                "type": "integer",
+                "description": "Number of tasks in step"
+              }
+            }
+          },
+          "pid": {
+            "type": "string",
+            "description": "First process PID"
+          },
+          "CPU": {
+            "type": "object",
+            "description": "CPU properties",
+            "properties": {
+              "requested_frequency": {
+                "type": "object",
+                "description": "CPU frequency requested",
+                "properties": {
+                  "min": {
+                    "type": "integer",
+                    "description": "Min CPU frequency"
+                  },
+                  "max": {
+                    "type": "integer",
+                    "description": "Max CPU frequency"
+                  }
+                }
+              },
+              "governor": {
+                "type": "array",
+                "description": "CPU governor",
+                "items": {
+                  "type": "string",
+                  "description": "CPU governor type"
+                }
+              }
+            }
+          },
+          "kill_request_user": {
+            "type": "string",
+            "description": "User who requested job killed"
+          },
+          "state": {
+            "type": "string",
+            "description": "State of job step"
+          },
+          "statistics": {
+            "type": "object",
+            "description": "Statistics of job step",
+            "properties": {
+              "CPU": {
+                "type": "object",
+                "description": "Statistics of CPU",
+                "properties": {
+                  "actual_frequency": {
+                    "type": "integer",
+                    "description": "Actual frequency of CPU during step"
+                  }
+                }
+              },
+              "energy": {
+                "type": "object",
+                "description": "Statistics of energy",
+                "properties": {
+                  "consumed": {
+                    "type": "integer",
+                    "description": "Energy consumed during step"
+                  }
+                }
+              }
+            }
+          },
+          "step": {
+            "type": "object",
+            "description": "Step details",
+            "properties": {
+              "job_id": {
+                "type": "integer",
+                "description": "Parent job id"
+              },
+              "het": {
+                "type": "object",
+                "description": "Heterogeneous job details",
+                "properties": {
+                  "component": {
+                    "type": "integer",
+                    "description": "Parent HetJob component id"
+                  }
+                }
+              },
+              "id": {
+                "type": "string",
+                "description": "Step id"
+              },
+              "name": {
+                "type": "string",
+                "description": "Step name"
+              }
+            }
+          },
+          "task": {
+            "description": "Task distribution properties",
+            "type": "string"
+          },
+          "tres": {
+            "type": "object",
+            "description": "TRES usage",
+            "properties": {
+              "requested": {
+                "type": "object",
+                "description": "TRES requested for job",
+                "properties": {
+                  "average": {
+                    "$ref": "#/components/schemas/dbv0.0.38_tres_list"
+                  },
+                  "max": {
+                    "$ref": "#/components/schemas/dbv0.0.38_tres_list"
+                  },
+                  "min": {
+                    "$ref": "#/components/schemas/dbv0.0.38_tres_list"
+                  },
+                  "total": {
+                    "$ref": "#/components/schemas/dbv0.0.38_tres_list"
+                  }
+                }
+              },
+              "consumed": {
+                "type": "object",
+                "description": "TRES requested for job",
+                "properties": {
+                  "average": {
+                    "$ref": "#/components/schemas/dbv0.0.38_tres_list"
+                  },
+                  "max": {
+                    "$ref": "#/components/schemas/dbv0.0.38_tres_list"
+                  },
+                  "min": {
+                    "$ref": "#/components/schemas/dbv0.0.38_tres_list"
+                  },
+                  "total": {
+                    "$ref": "#/components/schemas/dbv0.0.38_tres_list"
+                  }
+                }
+              },
+              "allocated": {
+                "$ref": "#/components/schemas/dbv0.0.38_tres_list"
+              }
+            }
+          }
+        }
+      },
+      "dbv0.0.38_config_info": {
+        "type": "object",
+        "properties": {
+          "meta": {
+            "$ref": "#/components/schemas/v0.0.38_meta"
+          },
+          "errors": {
+            "type": "array",
+            "description": "Slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_error"
+            }
+          },
+          "tres": {
+            "type": "array",
+            "description": "Array of TRES",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_tres_list"
+            }
+          },
+          "accounts": {
+            "type": "array",
+            "description": "Array of accounts",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_account"
+            }
+          },
+          "associations": {
+            "type": "array",
+            "description": "Array of associations",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_association"
+            }
+          },
+          "users": {
+            "type": "array",
+            "description": "Array of users",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_user"
+            }
+          },
+          "qos": {
+            "type": "array",
+            "description": "Array of qos",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_qos"
+            }
+          },
+          "wckeys": {
+            "type": "array",
+            "description": "Array of wckeys",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_wckey"
+            }
+          }
+        }
+      },
+      "dbv0.0.38_account_response": {
+        "type": "object",
+        "properties": {
+          "meta": {
+            "$ref": "#/components/schemas/v0.0.38_meta"
+          },
+          "errors": {
+            "type": "array",
+            "description": "Slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_error"
+            }
+          }
+        }
+      },
+      "dbv0.0.38_config_response": {
+        "type": "object",
+        "properties": {
+          "meta": {
+            "$ref": "#/components/schemas/v0.0.38_meta"
+          },
+          "errors": {
+            "type": "array",
+            "description": "Slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_error"
+            }
+          }
+        }
+      },
+      "dbv0.0.38_errors": {
+        "type": "array",
+        "description": "Slurm errors",
+        "items": {
+          "$ref": "#/components/schemas/dbv0.0.38_error"
+        }
+      },
+      "dbv0.0.38_error": {
+        "properties": {
+          "error_number": {
+            "description": "Slurm internal error number",
+            "type": "integer"
+          },
+          "error": {
+            "description": "Error message",
+            "type": "string"
+          },
+          "source": {
+            "description": "Where error occured in the source",
+            "type": "string"
+          },
+          "description": {
+            "description": "Explaination of cause of error",
+            "type": "string"
+          }
+        },
+        "type": "object"
+      },
+      "dbv0.0.38_meta": {
+        "type": "object",
+        "properties": {
+          "plugin": {
+            "type": "object",
+            "properties": {
+              "type": {
+                "type": "string",
+                "description": ""
+              },
+              "name": {
+                "type": "string",
+                "description": ""
+              }
+            }
+          },
+          "Slurm": {
+            "type": "object",
+            "description": "Slurm information",
+            "properties": {
+              "version": {
+                "type": "object",
+                "properties": {
+                  "major": {
+                    "type": "string",
+                    "description": ""
+                  },
+                  "micro": {
+                    "type": "string",
+                    "description": ""
+                  },
+                  "minor": {
+                    "type": "string",
+                    "description": ""
+                  }
+                }
+              },
+              "release": {
+                "type": "string",
+                "description": "version specifier"
+              }
+            }
+          }
+        }
+      },
+      "dbv0.0.38_update_users": {
+        "properties": {
+          "users": {
+            "type": "array",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_user"
+            }
+          }
+        }
+      },
+      "dbv0.0.38_update_qos": {
+        "properties": {
+          "qos": {
+            "type": "array",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_qos"
+            }
+          }
+        }
+      },
+      "dbv0.0.38_update_account": {
+        "properties": {
+          "accounts": {
+            "type": "array",
+            "items": {
+              "$ref": "#/components/schemas/dbv0.0.38_account"
+            }
+          }
+        }
+      },
+      "dbv0.0.38_clusters_properties": {
+        "type": "object",
+        "properties": {
+          "clusters": {
+            "$ref": "#/components/schemas/dbv0.0.38_cluster_info"
+          }
+        }
+      },
+      "dbv0.0.38_response_associations_delete": {
+	"type": "object",
+	"properties": {
+	  "meta": {
+	    "$ref": "#/components/schemas/v0.0.38_meta"
+	  },
+	  "errors": {
+	    "type": "array",
+	    "description": "Slurm errors",
+	    "items": {
+	      "$ref": "#/components/schemas/dbv0.0.38_error"
+	    }
+	  },
+	  "removed_associations": {
+	    "type": "array",
+	    "description": "the associations",
+	    "items": {
+	      "type": "string"
+	    }
+	  }
+	}
+      }
+    }
+  }
+}
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/parse.c slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/parse.c
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/parse.c	1970-01-01 01:00:00.000000000 +0100
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/parse.c	2023-01-17 15:45:21.845030083 +0100
@@ -0,0 +1,3061 @@
+/*****************************************************************************\
+ *  parse.c - Slurm REST API parsing handlers
+ *****************************************************************************
+ *  Copyright (C) 2019-2020 SchedMD LLC.
+ *  Written by Nathan Rini <nate@schedmd.com>
+ *
+ *  This file is part of Slurm, a resource management program.
+ *  For details, see <https://slurm.schedmd.com/>.
+ *  Please also read the included file: DISCLAIMER.
+ *
+ *  Slurm is free software; you can redistribute it and/or modify it under
+ *  the terms of the GNU General Public License as published by the Free
+ *  Software Foundation; either version 2 of the License, or (at your option)
+ *  any later version.
+ *
+ *  In addition, as a special exception, the copyright holders give permission
+ *  to link the code of portions of this program with the OpenSSL library under
+ *  certain conditions as described in each individual source file, and
+ *  distribute linked combinations including the two. You must obey the GNU
+ *  General Public License in all respects for all of the code used other than
+ *  OpenSSL. If you modify file(s) with this exception, you may extend this
+ *  exception to your version of the file(s), but you are not obligated to do
+ *  so. If you do not wish to do so, delete this exception statement from your
+ *  version.  If you delete this exception statement from all source files in
+ *  the program, then also delete it here.
+ *
+ *  Slurm is distributed in the hope that it will be useful, but WITHOUT ANY
+ *  WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+ *  FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
+ *  details.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with Slurm; if not, write to the Free Software Foundation, Inc.,
+ *  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301  USA.
+\*****************************************************************************/
+
+#include "config.h"
+
+#include <math.h>
+#include <unistd.h>
+
+#include "slurm/slurm.h"
+
+#include "src/common/data.h"
+#include "src/common/log.h"
+#include "src/common/read_config.h"
+#include "src/common/node_select.h"
+#include "src/common/slurm_protocol_api.h"
+#include "src/common/slurmdbd_defs.h"
+#include "src/common/uid.h"
+#include "src/common/xassert.h"
+#include "src/common/xmalloc.h"
+#include "src/common/xstring.h"
+
+#include "src/slurmrestd/plugins/openapi/dbv0.0.38/api.h"
+
+/*
+ * WARNING: parser uses a ton of macros to avoid massive amounts of copy and
+ * pasta of nearly identicaly code. Make sure to check the macros first before
+ * adding code.
+ *
+ * The database structures have many cross calls and it was found just handling
+ * them all in one location was much cleaner than having to copy and paste the
+ * same code many times along with all the possible errors that will introduce.
+ *
+ * This code is basically equivent to the pack/unpack code used for the Slurm
+ * internal protocol but is designed to be used against data_t for slurmrestd.
+ * The code is not version checked as the plugins are locked to a specific
+ * version instead.
+ *
+ * Each struct to be parsed/dumped is defined with an array of parser_t entries.
+ * Macros are used to fill out the contents of each entry since they are highly
+ * repetitive. Each struct should directly mirror the order and structure of the
+ * original structure for simplicity. Through the use of offsetof() macro, we
+ * are able to directly record the location of each struct member and then work
+ * them directly while using a generic structure for the parser itself. At the
+ * top of every parser/dumper, we put the pointer math to keep everything simple
+ * in the function when using the offsets. The field can be a nested dictionary
+ * and any "/" will automatically be expanded into dictionary entries to make
+ * writting these definitions easy.
+ *
+ * Every struct type and primitive type is defined in parser_type_t along with a
+ * description of what is being transcribed. With most types, there is a dumper
+ * and parser function that needs to be defined in funcs[]. It is fully expected
+ * that these functions are *symmetric* to make it easy to dump and load the
+ * structs.
+ *
+ * Flags and quasi bool fields have special handlers to avoid needing to write
+ * function to handle piles of flags. Even if a struct uses an individual bool
+ * field for a flag, we try to place all of them in a flags array anyway.
+ *
+ * Stand alone structs that will be parsed/dumped need to be added to parsers to
+ * allow external calls to easily dump/parse the structs.
+ *
+ * When structs are not symmetric, we use more complex types to make it look
+ * symmetric to the users and handle the dirty work here instead of expecting
+ * the user to do it.
+ *
+ * How to use:
+ * 	1. Add all types to parser_type_t
+ * 	2. Add parser and dumper function following prototypes to funcs[].
+ * 	3. Add full structures to parsers (if needed).
+ * 	4. Call dumper/parser where needed, if only a field is being modified,
+ * 	this step can be skipped.
+ *
+ */
+
+typedef struct {
+	enum {
+		PARSER_ENUM_FLAG_INVALID = 0,
+		PARSER_ENUM_FLAG_BIT, /* set a single bit */
+		PARSER_ENUM_FLAG_BOOL, /* set a bool using offset */
+	} type;
+	uint64_t flag;
+	size_t size;
+	const char *string;
+	size_t field_offset;
+} parser_enum_t;
+
+/* QOS preemption list uses one field to list and one field to set */
+typedef struct {
+	size_t field_offset_preempt_bitstr;
+	size_t field_offset_preempt_list;
+} parser_qos_preempt_t;
+
+typedef struct {
+	parser_type_t type;
+	bool required;
+	size_t field_offset;
+	char *key;
+} parser_t;
+
+/* templates for read and write functions */
+typedef int (*parse_rfunc_t)(const parser_t *const parse, void *dst,
+			     data_t *src, data_t *errors,
+			     const parser_env_t *penv);
+typedef int (*parse_wfunc_t)(const parser_t *const parse, void *src,
+			     data_t *dst, const parser_env_t *penv);
+
+typedef struct {
+	parser_type_t type;
+	const parser_t *const parse;
+	const size_t parse_member_count;
+} parsers_t;
+
+#define add_parser(stype, mtype, req, field, path)                            \
+	{                                                                     \
+		.field_offset = offsetof(stype, field),                       \
+		.key = path,                                                  \
+		.required = req,                                              \
+		.type = PARSE_##mtype,                                        \
+	}
+
+/*
+ * flags are a special exception that just passed the flags_array as the
+ * field_offset since the array actually has all of the flag offsets in the
+ * struct
+ */
+#define add_parser_flags(flags_array, req, path)                              \
+	{                                                                     \
+		.field_offset = (uintptr_t)flags_array,                       \
+		.key = path,                                                  \
+		.required = req,                                              \
+		.type = PARSE_FLAGS,                                          \
+	}
+#define add_parser_enum_flag(stype, field, flagv, stringv)                    \
+	{                                                                     \
+		.flag = flagv,                                                \
+		.field_offset = offsetof(stype, field),                       \
+		.size = sizeof(((stype *) 0)->field),                         \
+		.string = stringv,                                            \
+		.type = PARSER_ENUM_FLAG_BIT,                                 \
+	}
+/* will never set to FALSE, only will set to TRUE if matched  */
+#define add_parse_enum_bool(stype, field, stringv)                            \
+	{                                                                     \
+		.field_offset = offsetof(stype, field),                       \
+		.size = sizeof(((stype *) 0)->field),                         \
+		.string = stringv,                                            \
+		.type = PARSER_ENUM_FLAG_BOOL,                                \
+	}
+
+static int _parser_run(void *obj, const parser_t *const parse,
+		       const size_t parse_member_count, data_t *data,
+		       data_t *errors, const parser_env_t *penv);
+static int _parser_dump(void *obj, const parser_t *const parse,
+			const size_t parse_member_count, data_t *data,
+			const parser_env_t *penv);
+
+#define _add_parse(mtype, field, path) \
+	add_parser(slurmdb_assoc_rec_t, mtype, false, field, path)
+#define _add_parse_req(mtype, field, path) \
+	add_parser(slurmdb_assoc_rec_t, mtype, true, field, path)
+
+static const parser_enum_t parser_assoc_flags[] = {
+	add_parser_enum_flag(slurmdb_assoc_rec_t, flags, ASSOC_FLAG_DELETED,
+			     "DELETED"),
+	add_parse_enum_bool(slurmdb_assoc_rec_t, is_def, "DEFAULT"),
+	{0}
+};
+
+static const parser_t parse_assoc_short[] = {
+	/* Identifiers required for any given association */
+	_add_parse_req(STRING, acct, "account"),
+	_add_parse(STRING, cluster, "cluster"),
+	_add_parse(STRING, partition, "partition"),
+	_add_parse_req(STRING, user, "user"),
+};
+
+/* should mirror the structure of slurmdb_assoc_rec_t */
+static const parser_t parse_assoc[] = {
+	/* skipping accounting_list */
+	_add_parse_req(STRING, acct, "account"),
+	/* skipping assoc_next */
+	/* skipping assoc_next_id */
+	/* skipping bf_usage (not packed) */
+	_add_parse(STRING, cluster, "cluster"),
+	_add_parse(QOS_ID, def_qos_id, "default/qos"),
+	add_parser_flags(parser_assoc_flags, false, "flags"),
+	/* skip lft */
+	_add_parse(UINT32, grp_jobs, "max/jobs/per/count"),
+	_add_parse(UINT32, grp_jobs_accrue, "max/jobs/per/accruing"),
+	_add_parse(UINT32, grp_submit_jobs, "max/jobs/per/submitted"),
+	_add_parse(TRES_LIST, grp_tres, "max/tres/total"),
+	/* skipping gres_tres_ctld  (not packed) */
+	_add_parse(TRES_LIST, max_tres_mins_pj, "max/tres/minutes/per/job"),
+	_add_parse(TRES_LIST, grp_tres_mins, "max/tres/group/minutes"),
+	/* skipping grp_tres_mins_ctld (not packed) */
+	_add_parse(TRES_LIST, grp_tres_run_mins, "max/tres/group/active"),
+	/* skipping grp_tres_run_mins_ctld (not packed) */
+	/* skipping max_tres_mins_ctld */
+	/* skipping id */
+	_add_parse(UINT16, is_def, "is_default"),
+	/* skipping lft */
+	_add_parse(UINT32, max_jobs, "max/jobs/active"),
+	_add_parse(UINT32, max_jobs_accrue, "max/jobs/accruing"),
+	_add_parse(UINT32, max_submit_jobs, "max/jobs/total"),
+	/* skipping max_tres_mins_ctld (not packed) */
+	_add_parse(TRES_LIST, max_tres_run_mins, "max/tres/minutes/total"),
+	/* skipping grp_tres_run_mins_ctld (not packed) */
+	_add_parse(UINT32, grp_wall, "max/per/account/wall_clock"),
+	/* skipping max_tres_mins_ctld (not packed) */
+	_add_parse(TRES_LIST, max_tres_pj, "max/tres/per/job"),
+	/* skipping max_tres_ctld (not packed) */
+	_add_parse(TRES_LIST, max_tres_pn, "max/tres/per/node"),
+	/* skipping max_tres_pn_ctld */
+	_add_parse(UINT32, max_wall_pj, "max/jobs/per/wall_clock"),
+	_add_parse(UINT32, min_prio_thresh, "min/priority_threshold"),
+	_add_parse(STRING, parent_acct, "parent_account"),
+	/* skip parent_id */
+	_add_parse(STRING, partition, "partition"),
+	_add_parse(UINT32, priority, "priority"),
+	_add_parse(QOS_ID_LIST, qos_list, "qos"),
+	/* skip rgt */
+	_add_parse(UINT32, shares_raw, "shares_raw"),
+	/* slurmdbd should never set uid - it should always be zero */
+	_add_parse(ASSOC_USAGE, usage, "usage"),
+	_add_parse_req(STRING, user, "user"),
+	/* skipping user_rec (not packed) */
+};
+#undef _add_parse
+#undef _add_parse_req
+
+#define _add_parse(mtype, field, path) \
+	add_parser(slurmdb_user_rec_t, mtype, false, field, path)
+#define _add_parse_req(mtype, field, path) \
+	add_parser(slurmdb_user_rec_t, mtype, true, field, path)
+
+#define _add_flag(flagn, flagv) \
+	add_parser_enum_flag(slurmdb_user_rec_t, flags, flagn, flagv)
+static const parser_enum_t parser_user_flags[] = {
+	_add_flag(SLURMDB_USER_FLAG_DELETED, "DELETED"),
+};
+#undef _add_flag
+
+/* should mirror the structure of slurmdb_user_rec */
+static const parser_t parse_user[] = {
+	_add_parse(ADMIN_LVL, admin_level, "administrator_level"),
+	_add_parse(ASSOC_SHORT_LIST, assoc_list, "associations"),
+	_add_parse(COORD_LIST, coord_accts, "coordinators"),
+	_add_parse(STRING, default_acct, "default/account"),
+	_add_parse(STRING, default_wckey, "default/wckey"),
+	add_parser_flags(parser_user_flags, false, "flags"),
+	_add_parse_req(STRING, name, "name"),
+	/* skipping old_name */
+	/* skipping uid (should always be 0) */
+};
+#undef _add_parse
+#undef _add_parse_req
+
+#define _add_flag(flagn, flagv) \
+	add_parser_enum_flag(slurmdb_job_rec_t, flags, flagn, flagv)
+static const parser_enum_t parser_job_flags[] = {
+	_add_flag(SLURMDB_JOB_CLEAR_SCHED, "CLEAR_SCHEDULING"),
+	_add_flag(SLURMDB_JOB_FLAG_NOTSET, "NOT_SET"),
+	_add_flag(SLURMDB_JOB_FLAG_SUBMIT, "STARTED_ON_SUBMIT"),
+	_add_flag(SLURMDB_JOB_FLAG_SCHED, "STARTED_ON_SCHEDULE"),
+	_add_flag(SLURMDB_JOB_FLAG_BACKFILL, "STARTED_ON_BACKFILL"),
+	{0}
+};
+#undef _add_flag
+
+#define _add_parse(mtype, field, path) \
+	add_parser(slurmdb_job_rec_t, mtype, false, field, path)
+/* should mirror the structure of slurmdb_job_rec_t  */
+static const parser_t parse_job[] = {
+	_add_parse(STRING, account, "account"),
+	_add_parse(STRING, admin_comment, "comment/administrator"),
+	_add_parse(UINT32, alloc_nodes, "allocation_nodes"),
+	_add_parse(UINT32, array_job_id, "array/job_id"),
+	_add_parse(UINT32, array_max_tasks, "array/limits/max/running/tasks"),
+	_add_parse(STRING, array_task_str, "array/task"),
+	_add_parse(UINT32, array_task_id, "array/task_id"),
+	_add_parse(ASSOC_ID, associd, "association"),
+	/* skip blockid (deprecated bluegene) */
+	_add_parse(STRING, cluster, "cluster"),
+	_add_parse(STRING, constraints, "constraints"),
+	//_add_parse(STRING, container, "container"),
+	/* skip db_index */
+	_add_parse(JOB_EXIT_CODE, derived_ec, "derived_exit_code"),
+	_add_parse(STRING, derived_es, "comment/job"),
+	_add_parse(UINT32, elapsed, "time/elapsed"),
+
+	_add_parse(UINT32, eligible, "time/eligible"),
+	_add_parse(UINT32, end, "time/end"),
+	_add_parse(JOB_EXIT_CODE, exitcode, "exit_code"),
+	add_parser_flags(parser_job_flags, false, "flags"),
+	/* skipping first_step_ptr (already added in steps) */
+	_add_parse(GROUP_ID, gid, "group"),
+	_add_parse(UINT32, het_job_id, "het/job_id"),
+	_add_parse(UINT32, het_job_offset, "het/job_offset"),
+	_add_parse(UINT32, jobid, "job_id"),
+	_add_parse(STRING, jobname, "name"),
+	/* skip lft */
+	_add_parse(STRING, mcs_label, "mcs/label"),
+	_add_parse(STRING, nodes, "nodes"),
+	_add_parse(STRING, partition, "partition"),
+	_add_parse(UINT32, priority, "priority"),
+	_add_parse(QOS_ID, qosid, "qos"),
+	_add_parse(UINT32, req_cpus, "required/CPUs"),
+	_add_parse(UINT32, req_mem, "required/memory"),
+	_add_parse(USER_ID, requid, "kill_request_user"),
+	_add_parse(UINT32, resvid, "reservation/id"),
+	_add_parse(UINT32, resv_name, "reservation/name"),
+	/* skipping show_full */
+	_add_parse(UINT32, start, "time/start"),
+	_add_parse(JOB_STATE, state, "state/current"),
+	_add_parse(JOB_REASON, state_reason_prev, "state/reason"),
+	_add_parse(UINT32, submit, "time/submission"),
+	_add_parse(JOB_STEPS, steps, "steps"),
+	_add_parse(UINT32, suspended, "time/suspended"),
+	_add_parse(STRING, system_comment, "comment/system"),
+	_add_parse(UINT32, sys_cpu_sec, "time/system/seconds"),
+	_add_parse(UINT32, sys_cpu_usec, "time/system/microseconds"),
+	_add_parse(UINT32, timelimit, "time/limit"),
+	_add_parse(UINT32, tot_cpu_sec, "time/total/seconds"),
+	_add_parse(UINT32, tot_cpu_usec, "time/total/microseconds"),
+	/* skipping track steps */
+	_add_parse(TRES_LIST, tres_alloc_str, "tres/allocated"),
+	_add_parse(TRES_LIST, tres_req_str, "tres/requested"),
+	/* skipping uid (dup with user below) */
+	/* skipping alloc_gres (dup with TRES) */
+	/* skipping uid */
+	_add_parse(STRING, user, "user"),
+	_add_parse(UINT32, user_cpu_sec, "time/user/seconds"),
+	_add_parse(UINT32, user_cpu_usec, "time/user/microseconds"),
+	_add_parse(WCKEY_TAG, wckey, "wckey"),
+	/* skipping wckeyid (redundant) */
+	_add_parse(STRING, work_dir, "working_directory"),
+};
+#undef _add_parse
+
+static const parser_enum_t parser_acct_flags[] = {
+	add_parser_enum_flag(slurmdb_account_rec_t, flags,
+			     SLURMDB_ACCT_FLAG_DELETED, "DELETED"),
+};
+
+#define _add_parse(mtype, field, path) \
+	add_parser(slurmdb_account_rec_t, mtype, false, field, path)
+/* should mirror the structure of slurmdb_account_rec_t */
+static const parser_t parse_acct[] = {
+	_add_parse(ASSOC_SHORT_LIST, assoc_list, "associations"),
+	_add_parse(COORD_LIST, coordinators, "coordinators"),
+	_add_parse(STRING, description, "description"),
+	_add_parse(STRING, name, "name"),
+	_add_parse(STRING, organization, "organization"),
+	add_parser_flags(parser_acct_flags, false, "flags"),
+};
+#undef _add_parse
+
+#define _add_parse(mtype, field, path) \
+	add_parser(slurmdb_coord_rec_t, mtype, false, field, path)
+#define _add_parse_req(mtype, field, path) \
+	add_parser(slurmdb_coord_rec_t, mtype, true, field, path)
+/* should mirror the structure of slurmdb_coord_rec_t  */
+static const parser_t parse_coord[] = {
+	_add_parse_req(STRING, name, "name"),
+	_add_parse(UINT16, direct, "direct"),
+};
+#undef _add_parse
+#undef _add_parse_req
+
+static const parser_enum_t parser_wckey_flags[] = {
+	add_parser_enum_flag(slurmdb_wckey_rec_t, flags,
+			     SLURMDB_WCKEY_FLAG_DELETED, "DELETED"),
+	add_parse_enum_bool(slurmdb_wckey_rec_t, is_def, "DEFAULT"),
+	{0}
+};
+
+#define _add_parse(mtype, field, path) \
+	add_parser(slurmdb_wckey_rec_t, mtype, false, field, path)
+#define _add_parse_req(mtype, field, path) \
+	add_parser(slurmdb_wckey_rec_t, mtype, true, field, path)
+/* should mirror the structure of slurmdb_wckey_rec_t */
+static const parser_t parse_wckey[] = {
+	_add_parse(ACCOUNT_LIST, accounting_list, "accounts"),
+	_add_parse_req(STRING, cluster, "cluster"),
+	_add_parse_req(UINT32, id, "id"),
+	_add_parse_req(STRING, name, "name"),
+	_add_parse_req(STRING, user, "user"),
+	/* skipping uid */
+	add_parser_flags(parser_wckey_flags, false, "flags"),
+};
+#undef _add_parse
+#undef _add_parse_req
+
+#define _add_parse(mtype, field, path) \
+	add_parser(slurmdb_tres_rec_t, mtype, false, field, path)
+#define _add_parse_req(mtype, field, path) \
+	add_parser(slurmdb_tres_rec_t, mtype, true, field, path)
+/* should mirror the structure of slurmdb_tres_rec_t  */
+static const parser_t parse_tres[] = {
+	/* skip alloc_secs (sreport func) */
+	/* skip rec_count (not packed) */
+	_add_parse_req(STRING, type, "type"),
+	_add_parse(STRING, name, "name"),
+	_add_parse(UINT32, id, "id"),
+	_add_parse(INT64, count, "count"),
+};
+#undef _add_parse
+#undef _add_parse_req
+
+#define _add_flag(flagn, flagstr) \
+	add_parser_enum_flag(slurmdb_qos_rec_t, flags, flagn, flagstr)
+static const parser_enum_t parser_qos_flags[] = {
+	/* skipping QOS_FLAG_BASE */
+	/* skipping QOS_FLAG_NOTSET */
+	/* skipping QOS_FLAG_ADD */
+	/* skipping QOS_FLAG_REMOVE */
+	_add_flag(QOS_FLAG_PART_MIN_NODE, "PARTITION_MINIMUM_NODE"),
+	_add_flag(QOS_FLAG_PART_MAX_NODE, "PARTITION_MAXIMUM_NODE"),
+	_add_flag(QOS_FLAG_PART_TIME_LIMIT, "PARTITION_TIME_LIMIT"),
+	_add_flag(QOS_FLAG_ENFORCE_USAGE_THRES, "ENFORCE_USAGE_THRESHOLD"),
+	_add_flag(QOS_FLAG_NO_RESERVE, "NO_RESERVE"),
+	_add_flag(QOS_FLAG_REQ_RESV, "REQUIRED_RESERVATION"),
+	_add_flag(QOS_FLAG_DENY_LIMIT, "DENY_LIMIT"),
+	_add_flag(QOS_FLAG_OVER_PART_QOS, "OVERRIDE_PARTITION_QOS"),
+	_add_flag(QOS_FLAG_NO_DECAY, "NO_DECAY"),
+	_add_flag(QOS_FLAG_USAGE_FACTOR_SAFE, "USAGE_FACTOR_SAFE"),
+	{0}
+};
+#undef _add_flag
+
+#define _add_flag(flagn, flagstr) \
+	add_parser_enum_flag(slurmdb_qos_rec_t, preempt_mode, flagn, flagstr)
+static const parser_enum_t parser_qos_preempt_flags[] = {
+	_add_flag(PREEMPT_MODE_SUSPEND, "SUSPEND"),
+	_add_flag(PREEMPT_MODE_REQUEUE, "REQUEUE"),
+	_add_flag(PREEMPT_MODE_CANCEL, "CANCEL"),
+	_add_flag(PREEMPT_MODE_GANG, "GANG"),
+	/* skip PREEMPT_MODE_OFF (it is implied by empty list) */
+	{0}
+};
+#undef _add_flag
+
+#define _add_parse(mtype, field, path) \
+	add_parser(slurmdb_qos_rec_t, mtype, false, field, path)
+#define _add_parse_req(mtype, field, path) \
+	add_parser(slurmdb_qos_rec_t, mtype, true, field, path)
+/* should mirror the structure of slurmdb_qos_rec_t */
+static const parser_t parse_qos[] = {
+	/* skipping accounting_list */
+	_add_parse(STRING, description, "description"),
+	add_parser_flags(parser_qos_flags, false, "flags"),
+	_add_parse(UINT32, id, "id"),
+	_add_parse(UINT32, grace_time, "limits/grace_time"),
+	_add_parse(UINT32, grp_jobs_accrue, "limits/max/active_jobs/accruing"),
+	_add_parse(UINT32, grp_jobs, "limits/max/active_jobs/count"),
+	_add_parse(TRES_LIST, grp_tres, "limits/max/tres/total"),
+	/* skipping grp_tres_ctld (not packed) */
+	_add_parse(TRES_LIST, grp_tres_run_mins,
+		   "limits/max/tres/minutes/per/qos"),
+	/* skipping grp_tres_run_mins_ctld (not packed) */
+	_add_parse(STRING, name, "name"),
+	_add_parse(UINT32, grp_wall, "limits/max/wall_clock/per/qos"),
+	//_add_parse(FLOAT64, limit_factor, "limits/factor"),
+	_add_parse(UINT32, max_jobs_pa,
+		   "limits/max/jobs/active_jobs/per/account"),
+	_add_parse(UINT32, max_jobs_pu, "limits/max/jobs/active_jobs/per/user"),
+	_add_parse(UINT32, max_jobs_accrue_pa,
+		   "limits/max/accruing/per/account"),
+	_add_parse(UINT32, max_jobs_accrue_pu, "limits/max/accruing/per/user"),
+	_add_parse(UINT32, max_submit_jobs_pa, "limits/max/jobs/per/account"),
+	_add_parse(UINT32, max_submit_jobs_pu, "limits/max/jobs/per/user"),
+	_add_parse(TRES_LIST, max_tres_mins_pj,
+		   "limits/max/tres/minutes/per/job"),
+	/* skipping max_tres_mins_pj_ctld (not packed) */
+	_add_parse(TRES_LIST, max_tres_pa, "limits/max/tres/per/account"),
+	/* skipping max_tres_pa_ctld (not packed) */
+	_add_parse(TRES_LIST, max_tres_pj, "limits/max/tres/per/job"),
+	/* skipping max_tres_pj_ctld (not packed) */
+	_add_parse(TRES_LIST, max_tres_pn, "limits/max/tres/per/node"),
+	/* skipping max_tres_pn_ctld (not packed) */
+	_add_parse(TRES_LIST, max_tres_pu, "limits/max/tres/per/user"),
+	/* skipping max_tres_pu_ctld (not packed) */
+	_add_parse(TRES_LIST, max_tres_run_mins_pa,
+		   "limits/max/tres/minutes/per/account"),
+	/* skipping max_tres_run_mins_pa_ctld (not packed) */
+	_add_parse(TRES_LIST, max_tres_run_mins_pu,
+		   "limits/max/tres/minutes/per/user"),
+	/* skipping max_tres_run_mins_pu_ctld (not packed) */
+	_add_parse(UINT32, max_wall_pj, "limits/max/wall_clock/per/job"),
+	_add_parse(UINT32, min_prio_thresh, "limits/min/priority_threshold"),
+	_add_parse(TRES_LIST, min_tres_pj, "limits/min/tres/per/job"),
+	/* skipping min_tres_pj_ctld (not packed) */
+	_add_parse(QOS_PREEMPT_LIST, preempt_list, "preempt/list"),
+	add_parser_flags(parser_qos_preempt_flags, false, "preempt/mode"),
+	_add_parse(UINT32, preempt_exempt_time, "preempt/exempt_time"),
+	_add_parse(UINT32, priority, "priority"),
+	/* skip usage (not packed) */
+	_add_parse(FLOAT64, usage_factor, "usage_factor"),
+	_add_parse(FLOAT64, usage_thres, "usage_threshold"),
+	/* skip blocked_until (not packed) */
+};
+#undef _add_parse
+#undef _add_parse_req
+
+#define _add_flag(flagn, flagv) \
+	add_parser_enum_flag(slurmdb_step_rec_t, req_cpufreq_gov, flagn, flagv)
+static const parser_enum_t parse_job_step_cpu_freq_flags[] = {
+	_add_flag(CPU_FREQ_CONSERVATIVE, "Conservative"),
+	_add_flag(CPU_FREQ_PERFORMANCE, "Performance"),
+	_add_flag(CPU_FREQ_POWERSAVE, "PowerSave"),
+	_add_flag(CPU_FREQ_ONDEMAND, "OnDemand"),
+	_add_flag(CPU_FREQ_USERSPACE, "UserSpace"),
+	{0}
+};
+#undef _add_flag
+
+#define _add_parse(mtype, field, path) \
+	add_parser(slurmdb_step_rec_t, mtype, false, field, path)
+/* should mirror the structure of slurmdb_step_rec_t   */
+static const parser_t parse_job_step[] = {
+	_add_parse(UINT32, elapsed, "time/elapsed"),
+	_add_parse(UINT32, end, "time/end"),
+	_add_parse(JOB_EXIT_CODE, exitcode, "exit_code"),
+	/* skipping job_ptr (redundant here) */
+	_add_parse(UINT32, nnodes, "nodes/count"),
+	_add_parse(STRING, nodes, "nodes/range"),
+	_add_parse(UINT32, ntasks, "tasks/count"),
+	_add_parse(STRING, pid_str, "pid"),
+	_add_parse(UINT32, req_cpufreq_min, "CPU/requested_frequency/min"),
+	_add_parse(UINT32, req_cpufreq_max, "CPU/requested_frequency/max"),
+	add_parser_flags(parse_job_step_cpu_freq_flags, false, "CPU/governor"),
+	_add_parse(USER_ID, requid, "kill_request_user"),
+	_add_parse(UINT32, start, "time/start"),
+	_add_parse(JOB_STATE, state, "state"),
+	_add_parse(UINT32, stats.act_cpufreq,
+		   "statistics/CPU/actual_frequency"),
+	_add_parse(UINT32, stats.consumed_energy, "statistics/energy/consumed"),
+	_add_parse(UINT32, step_id.job_id, "step/job_id"),
+	_add_parse(UINT32, step_id.step_het_comp, "step/het/component"),
+	_add_parse(STEP_ID, step_id.step_id, "step/id"),
+	_add_parse(STRING, stepname, "step/name"),
+	_add_parse(UINT32, suspended, "time/suspended"),
+	_add_parse(UINT32, sys_cpu_sec, "time/system/seconds"),
+	_add_parse(UINT32, sys_cpu_usec, "time/system/microseconds"),
+	_add_parse(TASK_DISTRIBUTION, task_dist, "task/distribution"),
+	_add_parse(UINT32, tot_cpu_sec, "time/total/seconds"),
+	_add_parse(UINT32, tot_cpu_usec, "time/total/microseconds"),
+	_add_parse(UINT32, user_cpu_sec, "time/user/seconds"),
+	_add_parse(UINT32, user_cpu_usec, "time/user/microseconds"),
+
+	/*
+	 * Handled in _dump_job_steps()->_foreach_step():
+	 * 	stats.tres_usage_in_max
+	 * 	stats.tres_usage_in_max_nodeid
+	 * 	stats.tres_usage_in_max_taskid
+	 * 	stats.tres_usage_out_max
+	 * 	stats.tres_usage_out_max_nodeid
+	 * 	stats.tres_usage_out_max_taskid
+	 * 	stats.tres_usage_out_min
+	 * 	stats.tres_usage_out_min_nodeid
+	 * 	stats.tres_usage_out_min_taskid
+	 */
+	_add_parse(TRES_LIST, stats.tres_usage_in_ave,
+		   "tres/requested/average"),
+	_add_parse(TRES_LIST, stats.tres_usage_in_tot, "tres/requested/total"),
+	_add_parse(TRES_LIST, stats.tres_usage_out_ave,
+		   "tres/consumed/average"),
+	_add_parse(TRES_LIST, stats.tres_usage_out_tot, "tres/consumed/total"),
+	_add_parse(TRES_LIST, tres_alloc_str, "tres/allocated"),
+};
+#undef _add_parse
+
+#define _add_parse(mtype, field, path) \
+	add_parser(slurmdb_stats_rec_t, mtype, false, field, path)
+/* should mirror the structure of slurmdb_stats_rec_t */
+static const parser_t parse_stats_rec[] = {
+	_add_parse(UINT32, time_start, "time_start"),
+	_add_parse(STATS_REC_ARRAY, dbd_rollup_stats, "rollups"),
+	_add_parse(STATS_RPC_LIST, rpc_list, "RPCs"),
+	_add_parse(STATS_USER_LIST, user_list, "users"),
+};
+#undef _add_parse
+
+#define _add_parse(mtype, field, path) \
+	add_parser(slurmdb_rpc_obj_t, mtype, false, field, path)
+/* should mirror the structure of slurmdb_rpc_obj_t */
+static const parser_t parse_stats_user_rpcs[] = {
+	_add_parse(USER_ID, id, "user"),
+	_add_parse(UINT32, cnt, "count"),
+	_add_parse(UINT64, time_ave, "time/average"),
+	_add_parse(UINT64, time, "time/total"),
+};
+#undef _add_parse
+
+#define _add_parse(mtype, field, path) \
+	add_parser(slurmdb_rpc_obj_t, mtype, false, field, path)
+/* should mirror the structure of slurmdb_rpc_obj_t */
+static const parser_t parse_stats_rpcs[] = {
+	_add_parse(RPC_ID, id, "rpc"),
+	_add_parse(UINT32, cnt, "count"),
+	_add_parse(UINT64, time_ave, "time/average"),
+	_add_parse(UINT64, time, "time/total"),
+};
+#undef _add_parse
+
+#define _add_flag(flagn, flagstr) \
+	add_parser_enum_flag(slurmdb_cluster_rec_t, flags, flagn, flagstr)
+static const parser_enum_t parse_cluster_rec_flags[] = {
+	_add_flag(CLUSTER_FLAG_MULTSD, "MULTIPLE_SLURMD"),
+	_add_flag(CLUSTER_FLAG_FE, "FRONT_END"),
+	_add_flag(CLUSTER_FLAG_CRAY, "CRAY_NATIVE"),
+	_add_flag(CLUSTER_FLAG_FED, "FEDERATION"),
+	_add_flag(CLUSTER_FLAG_EXT, "EXTERNAL"),
+	{0}
+};
+#undef _add_flag
+
+#define _add_parse(mtype, field, path) \
+	add_parser(slurmdb_cluster_rec_t, mtype, false, field, path)
+/* should mirror the structure of slurmdb_cluster_rec_t */
+static const parser_t parse_cluster_rec[] = {
+	/* skip accounting (sreport func only) */
+	/* skip classification (to be deprecated) */
+	/* skip comm_fail_time (not packed) */
+	/* skip control_addr (not packed) */
+	_add_parse(STRING, control_host, "controller/host"),
+	_add_parse(UINT32, control_port, "controller/port"),
+	/* skip dim_size (BG deprecated) */
+	/* skip fed[eration] support */
+	add_parser_flags(parse_cluster_rec_flags, false, "flags"),
+	/* skip lock (not packed) */
+	_add_parse(STRING, name, "name"),
+	_add_parse(STRING, nodes, "nodes"),
+	_add_parse(SELECT_PLUGIN_ID, plugin_id_select, "select_plugin"),
+	_add_parse(ASSOC_SHORT, root_assoc, "associations/root"),
+	_add_parse(UINT16, rpc_version, "rpc_version"),
+	/* skip send_rpc (not packed) */
+	_add_parse(TRES_LIST, tres_str, "tres"),
+};
+#undef _add_parse
+
+#define _add_parse(mtype, field, path) \
+	add_parser(slurmdb_cluster_accounting_rec_t, mtype, false, field, path)
+/* should mirror the structure of slurmdb_cluster_accounting_rec_t */
+static const parser_t parse_cluster_accounting_rec[] = {
+	_add_parse(UINT64, alloc_secs, "time/allocated"),
+	_add_parse(UINT64, down_secs, "time/down"),
+	_add_parse(UINT64, idle_secs, "time/idle"),
+	_add_parse(UINT64, over_secs, "time/overcommitted"),
+	_add_parse(UINT64, pdown_secs, "time/planned_down"),
+	_add_parse(UINT64, period_start, "time/start"),
+	_add_parse(UINT64, period_start, "time/reserved"),
+	_add_parse(STRING, tres_rec.name, "tres/name"),
+	_add_parse(STRING, tres_rec.type, "tres/type"),
+	_add_parse(UINT32, tres_rec.id, "tres/id"),
+	_add_parse(UINT64, tres_rec.count, "tres/count"),
+};
+#undef _add_parse
+
+static int _parse_to_string(const parser_t *const parse, void *obj, data_t *str,
+			    data_t *errors, const parser_env_t *penv)
+{
+	int rc = SLURM_SUCCESS;
+	char **dst = (((void *)obj) + parse->field_offset);
+
+	if (data_get_type(str) == DATA_TYPE_NULL) {
+		xfree(*dst);
+	} else if (data_convert_type(str, DATA_TYPE_STRING) ==
+		   DATA_TYPE_STRING) {
+		xfree(*dst);
+		*dst = xstrdup(data_get_string(str));
+	} else {
+		rc = ESLURM_DATA_CONV_FAILED;
+	}
+
+	debug5("%s: string %s rc[%d]=%s", __func__, *dst, rc,
+	       slurm_strerror(rc));
+
+	return rc;
+}
+
+static int _dump_to_string(const parser_t *const parse, void *obj, data_t *data,
+			   const parser_env_t *penv)
+{
+	int rc = SLURM_SUCCESS;
+	char **src = (((void *)obj) + parse->field_offset);
+
+	if (*src)
+		data_set_string(data, *src);
+	else
+		data_set_null(data);
+
+	return rc;
+}
+
+static int _parse_to_float128(const parser_t *const parse, void *obj,
+			      data_t *str, data_t *errors,
+			      const parser_env_t *penv)
+{
+	long double *dst = (((void *)obj) + parse->field_offset);
+	int rc = SLURM_SUCCESS;
+
+	xassert(sizeof(long double) * 8 == 128);
+
+	if (data_get_type(str) == DATA_TYPE_NULL)
+		*dst = (double)NO_VAL;
+	else if (data_convert_type(str, DATA_TYPE_FLOAT) == DATA_TYPE_FLOAT)
+		*dst = data_get_float(str);
+	else
+		rc = ESLURM_DATA_CONV_FAILED;
+
+	log_flag(DATA, "%s: string %Lf rc[%d]=%s", __func__, *dst, rc,
+		 slurm_strerror(rc));
+
+	return rc;
+}
+
+static int _dump_to_float128(const parser_t *const parse, void *obj,
+			     data_t *dst, const parser_env_t *penv)
+{
+	long double *src = (((void *)obj) + parse->field_offset);
+
+	xassert(data_get_type(dst) == DATA_TYPE_NULL);
+
+	/* see bug#9674 */
+	if (((uint32_t)*src == INFINITE) || ((uint32_t)*src == NO_VAL))
+		data_set_null(dst);
+	else
+		(void)data_set_float(dst, *src);
+
+	return SLURM_SUCCESS;
+}
+
+static int _parse_to_float64(const parser_t *const parse, void *obj,
+			     data_t *str, data_t *errors,
+			     const parser_env_t *penv)
+{
+	double *dst = (((void *)obj) + parse->field_offset);
+	int rc = SLURM_SUCCESS;
+
+	xassert(sizeof(double) * 8 == 64);
+
+	if (data_get_type(str) == DATA_TYPE_NULL)
+		*dst = (double)NO_VAL;
+	else if (data_convert_type(str, DATA_TYPE_FLOAT) == DATA_TYPE_FLOAT)
+		*dst = data_get_float(str);
+	else
+		rc = ESLURM_DATA_CONV_FAILED;
+
+	log_flag(DATA, "%s: string %f rc[%d]=%s", __func__, *dst, rc,
+		 slurm_strerror(rc));
+
+	return rc;
+}
+
+static int _dump_to_float64(const parser_t *const parse, void *obj, data_t *dst,
+			    const parser_env_t *penv)
+{
+	double *src = (((void *)obj) + parse->field_offset);
+
+	xassert(data_get_type(dst) == DATA_TYPE_NULL);
+
+	/* see bug#9674 */
+	if (((uint32_t)*src == INFINITE) || ((uint32_t)*src == NO_VAL))
+		(void)data_set_null(dst);
+	else
+		(void)data_set_float(dst, *src);
+
+	return SLURM_SUCCESS;
+}
+
+static int _parse_to_int64(const parser_t *const parse, void *obj, data_t *str,
+			   data_t *errors, const parser_env_t *penv)
+{
+	int64_t *dst = (((void *)obj) + parse->field_offset);
+	int rc = SLURM_SUCCESS;
+
+	if (data_get_type(str) == DATA_TYPE_NULL)
+		*dst = (double)NO_VAL;
+	else if (data_convert_type(str, DATA_TYPE_FLOAT) == DATA_TYPE_FLOAT)
+		*dst = data_get_float(str);
+	else
+		rc = ESLURM_DATA_CONV_FAILED;
+
+	log_flag(DATA, "%s: string %"PRId64" rc[%d]=%s", __func__, *dst, rc,
+		 slurm_strerror(rc));
+
+	return rc;
+}
+
+static int _dump_to_int64(const parser_t *const parse, void *obj, data_t *dst,
+			  const parser_env_t *penv)
+{
+	int64_t *src = (((void *)obj) + parse->field_offset);
+
+	xassert(data_get_type(dst) == DATA_TYPE_NULL);
+
+	/* Never set values of INF or NO_VAL */
+	if ((*src == NO_VAL64) || (*src == INFINITE64))
+		(void)data_set_null(dst);
+	else
+		(void)data_set_int(dst, *src);
+
+	return SLURM_SUCCESS;
+}
+
+static int _parse_to_uint16(const parser_t *const parse, void *obj, data_t *str,
+			    data_t *errors, const parser_env_t *penv)
+{
+	uint16_t *dst = (((void *)obj) + parse->field_offset);
+	int rc = SLURM_SUCCESS;
+
+	if (data_get_type(str) == DATA_TYPE_NULL)
+		*dst = 0;
+	else if (data_convert_type(str, DATA_TYPE_INT_64) == DATA_TYPE_INT_64)
+		*dst = data_get_int(str);
+	else
+		rc = ESLURM_DATA_CONV_FAILED;
+
+	log_flag(DATA, "%s: string %hu rc[%d]=%s", __func__, *dst, rc,
+		 slurm_strerror(rc));
+
+	return rc;
+}
+
+static int _dump_to_uint16(const parser_t *const parse, void *obj, data_t *dst,
+			   const parser_env_t *penv)
+{
+	uint16_t *src = (((void *)obj) + parse->field_offset);
+
+	xassert(data_get_type(dst) == DATA_TYPE_NULL);
+
+	/* Never set values of INF or NO_VAL */
+	if ((*src == NO_VAL16) || (*src == INFINITE16))
+		data_set_null(dst);
+	else
+		(void)data_set_int(dst, *src);
+
+	return SLURM_SUCCESS;
+}
+
+static int _parse_to_uint64(const parser_t *const parse, void *obj, data_t *str,
+			    data_t *errors, const parser_env_t *penv)
+{
+	uint64_t *dst = (((void *)obj) + parse->field_offset);
+	int rc = SLURM_SUCCESS;
+
+	if (data_get_type(str) == DATA_TYPE_NULL)
+		*dst = 0;
+	else if (data_convert_type(str, DATA_TYPE_INT_64) == DATA_TYPE_INT_64)
+		*dst = data_get_int(str);
+	else
+		rc = ESLURM_DATA_CONV_FAILED;
+
+	log_flag(DATA, "%s: string %"PRIu64" rc[%d]=%s", __func__, *dst, rc,
+		 slurm_strerror(rc));
+
+	return rc;
+}
+
+static int _dump_to_uint64(const parser_t *const parse, void *obj, data_t *dst,
+			   const parser_env_t *penv)
+{
+	uint64_t *src = (((void *)obj) + parse->field_offset);
+
+	xassert(data_get_type(dst) == DATA_TYPE_NULL);
+
+	/* Never set values of INF or NO_VAL */
+	if ((*src == NO_VAL64) || (*src == INFINITE64))
+		data_set_null(dst);
+	else
+		(void)data_set_int(dst, *src);
+
+	return SLURM_SUCCESS;
+}
+
+static int _parse_to_uint32(const parser_t *const parse, void *obj, data_t *str,
+			    data_t *errors, const parser_env_t *penv)
+{
+	uint32_t *dst = (((void *)obj) + parse->field_offset);
+	int rc = SLURM_SUCCESS;
+
+	if (data_get_type(str) == DATA_TYPE_NULL) {
+		*dst = 0;
+	} else if (data_convert_type(str, DATA_TYPE_INT_64) ==
+		   DATA_TYPE_INT_64) {
+		/* catch -1 and set to NO_VAL instead of rolling */
+		if (0xFFFFFFFF00000000 & data_get_int(str))
+			*dst = NO_VAL;
+		else
+			*dst = data_get_int(str);
+	} else
+		rc = ESLURM_DATA_CONV_FAILED;
+
+	log_flag(DATA, "%s: string %u rc[%d]=%s", __func__, *dst, rc,
+		 slurm_strerror(rc));
+
+	return rc;
+}
+
+static int _dump_to_uint32(const parser_t *const parse, void *obj, data_t *dst,
+			   const parser_env_t *penv)
+{
+	uint32_t *src = (((void *)obj) + parse->field_offset);
+
+	xassert(data_get_type(dst) == DATA_TYPE_NULL);
+
+	if ((*src == NO_VAL) || (*src == INFINITE))
+		data_set_null(dst);
+	else
+		(void)data_set_int(dst, *src);
+
+	return SLURM_SUCCESS;
+}
+
+#define MAGIC_FOREACH_PARSE_FLAGS 0xba2d2a13
+typedef struct {
+	int magic;
+	uint32_t *flags;
+	data_t *errors;
+	const parser_t *const parse;
+	void *obj;
+} for_each_parse_flag_t;
+
+static data_for_each_cmd_t _for_each_parse_flag(data_t *data, void *arg)
+{
+	for_each_parse_flag_t *args = arg;
+	bool found_a_match = false;
+
+	xassert(args->magic == MAGIC_FOREACH_PARSE_FLAGS);
+
+	if (data_convert_type(data, DATA_TYPE_STRING) != DATA_TYPE_STRING)
+		return DATA_FOR_EACH_FAIL;
+
+	for (const parser_enum_t *f = (NULL + args->parse->field_offset);
+	     f->type; f++) {
+		bool match = !xstrcasecmp(data_get_string(data), f->string);
+
+		if (match)
+			found_a_match = true;
+
+		if (f->type == PARSER_ENUM_FLAG_BIT) {
+			const size_t b = f->size;
+			if (!match)
+				continue;
+
+			/* C allows complier to choose a size for the enum */
+			if (b == sizeof(uint64_t)) {
+				uint64_t *flags = (((void *)args->obj) +
+						   f->field_offset);
+				*flags |= f->flag;
+			} else if (b == sizeof(uint32_t)) {
+				uint32_t *flags = (((void *)args->obj) +
+						   f->field_offset);
+				*flags |= f->flag;
+			} else if (b == sizeof(uint16_t)) {
+				uint16_t *flags = (((void *)args->obj) +
+						   f->field_offset);
+				*flags |= f->flag;
+			} else if (b == sizeof(uint8_t)) {
+				uint8_t *flags = (((void *)args->obj) +
+						  f->field_offset);
+				*flags |= f->flag;
+			} else
+				fatal("%s: unexpected enum size: %zu", __func__,
+				      b);
+		} else if (f->type == PARSER_ENUM_FLAG_BOOL) {
+			const size_t b = f->size;
+
+			if (!match)
+				continue;
+
+			/*
+			 * flag applies to a bool in the obj structure and not
+			 * in obj->flags.
+			 * Set true while being aware of the size of the
+			 * original variable since it may move around the 1.
+			 */
+
+			if (b == sizeof(uint64_t)) {
+				uint64_t *ptr = (((void *)args->obj) +
+						 f->field_offset);
+				*ptr = true;
+			} else if (b == sizeof(uint32_t)) {
+				uint32_t *ptr = (((void *)args->obj) +
+						 f->field_offset);
+				*ptr = true;
+			} else if (b == sizeof(uint16_t)) {
+				uint16_t *ptr = (((void *)args->obj) +
+						 f->field_offset);
+				*ptr = true;
+			} else if (b == sizeof(uint8_t)) {
+				uint8_t *ptr = (((void *)args->obj) +
+						f->field_offset);
+				*ptr = true;
+			} else
+				fatal("%s: unexpected bool size: %zu", __func__,
+				      b);
+		} else
+			fatal("%s: unexpect type", __func__);
+	}
+
+	if (!found_a_match) {
+		resp_error(args->errors, ESLURM_REST_FAIL_PARSING,
+			   "Unknown flag", args->parse->key);
+		return DATA_FOR_EACH_FAIL;
+	}
+
+	return DATA_FOR_EACH_CONT;
+}
+
+static int _parse_flags(const parser_t *const parse, void *obj, data_t *src,
+			data_t *errors, const parser_env_t *penv)
+{
+	for_each_parse_flag_t args = {
+		.errors = errors,
+		.magic = MAGIC_FOREACH_PARSE_FLAGS,
+		.obj = obj,
+		.parse = parse,
+	};
+
+	if (data_get_type(src) != DATA_TYPE_LIST)
+		return ESLURM_REST_FAIL_PARSING;
+
+	if (data_list_for_each(src, _for_each_parse_flag, &args) < 0)
+		return ESLURM_REST_FAIL_PARSING;
+
+	return SLURM_SUCCESS;
+	;
+}
+
+static int _dump_flags(const parser_t *const parse, void *obj, data_t *data,
+		       const parser_env_t *penv)
+{
+	xassert(data_get_type(data) == DATA_TYPE_NULL);
+	data_set_list(data);
+
+	for (const parser_enum_t *f = (NULL + parse->field_offset);
+	     f->type; f++) {
+		bool found = false;
+
+		if (f->type == PARSER_ENUM_FLAG_BIT) {
+			const size_t b = f->size;
+
+			/* C allows complier to choose a size for the enum */
+			if (b == sizeof(uint64_t)) {
+				uint64_t *flags = (((void *)obj) +
+						   f->field_offset);
+				if (*flags & f->flag)
+					found = true;
+			} else if (b == sizeof(uint32_t)) {
+				uint32_t *flags = (((void *)obj) +
+						   f->field_offset);
+				if (*flags & f->flag)
+					found = true;
+			} else if (b == sizeof(uint16_t)) {
+				uint16_t *flags = (((void *)obj) +
+						   f->field_offset);
+				if (*flags & f->flag)
+					found = true;
+			} else if (b == sizeof(uint8_t)) {
+				uint8_t *flags = (((void *)obj) +
+						  f->field_offset);
+				if (*flags & f->flag)
+					found = true;
+			} else
+				fatal("%s: unexpected enum size: %zu", __func__,
+				      b);
+
+		} else if (f->type == PARSER_ENUM_FLAG_BOOL) {
+			const size_t b = f->size;
+
+			if (b == sizeof(uint64_t)) {
+				uint64_t *ptr = (((void *)obj) +
+						 f->field_offset);
+				if (*ptr)
+					found = true;
+			} else if (b == sizeof(uint32_t)) {
+				uint32_t *ptr = (((void *)obj) +
+						 f->field_offset);
+				if (*ptr)
+					found = true;
+			} else if (b == sizeof(uint16_t)) {
+				uint16_t *ptr = (((void *)obj) +
+						 f->field_offset);
+				if (*ptr)
+					found = true;
+			} else if (b == sizeof(uint8_t)) {
+				uint8_t *ptr = (((void *)obj) +
+						f->field_offset);
+				if (*ptr)
+					found = true;
+			} else
+				fatal("%s: unexpected bool size: %zu", __func__,
+				      b);
+
+		} else
+			fatal("%s: unknown flag type", __func__);
+
+		if (found)
+			data_set_string(data_list_append(data), f->string);
+	}
+
+	return SLURM_SUCCESS;
+	;
+}
+
+#define MAGIC_FOREACH_PARSE_QOS 0xabaa2c18
+typedef struct {
+	int magic;
+	List qos_list;
+	data_t *errors;
+} for_each_parse_qos_t;
+
+static data_for_each_cmd_t _parse_qos_common(data_t *data,
+					     data_t **name)
+{
+	xassert(name);
+
+	switch (data_get_type(data)) {
+	case DATA_TYPE_STRING:
+		*name = data;
+		break;
+	case DATA_TYPE_DICT:
+		/*
+		 * Note: we ignore everything but name for loading QOS into an
+		 * qos_list as that is the only field accepted
+		 */
+		if (!(*name = data_key_get(data, "name")) ||
+		    data_convert_type(*name, DATA_TYPE_STRING) !=
+			    DATA_TYPE_STRING)
+			return DATA_FOR_EACH_FAIL;
+		break;
+	default:
+		return DATA_FOR_EACH_FAIL;
+	}
+
+	return DATA_FOR_EACH_CONT;
+}
+
+static data_for_each_cmd_t _for_each_parse_qos(data_t *data, void *arg)
+{
+	data_for_each_cmd_t rc;
+	for_each_parse_qos_t *args = arg;
+	data_t *name;
+
+	xassert(args->magic == MAGIC_FOREACH_PARSE_QOS);
+
+	rc = _parse_qos_common(data, &name);
+	if (rc != DATA_FOR_EACH_CONT)
+		return rc;
+
+	(void)list_append(args->qos_list, xstrdup(data_get_string(name)));
+
+	return DATA_FOR_EACH_CONT;
+}
+
+static int _parse_qos_str_id(const parser_t *const parse, void *obj,
+			     data_t *src, data_t *errors,
+			     const parser_env_t *penv)
+{
+	char *qos_name = NULL;
+	uint32_t *qos_id = (((void *)obj) + parse->field_offset);
+	slurmdb_qos_rec_t *qos;
+
+	if (data_get_type(src) == DATA_TYPE_NULL) {
+		*qos_id = 0;
+		return SLURM_SUCCESS;
+	} else if (data_convert_type(src, DATA_TYPE_STRING) ==
+		   DATA_TYPE_STRING) {
+		qos_name = data_get_string(src);
+	} else
+		return ESLURM_DATA_CONV_FAILED;
+
+	if (!qos_name || !qos_name[0])
+		return ESLURM_DATA_CONV_FAILED;
+
+	/* find qos by name from global list */
+	xassert(penv->g_qos_list);
+	if (!penv->g_qos_list)
+		return ESLURM_REST_EMPTY_RESULT;
+
+	qos = list_find_first(penv->g_qos_list,
+			      slurmdb_find_qos_in_list_by_name, qos_name);
+	if (!qos)
+		return ESLURM_REST_EMPTY_RESULT;
+
+	*qos_id = qos->id;
+
+	return SLURM_SUCCESS;
+}
+
+static int _dump_qos_str_id(const parser_t *const parse, void *obj, data_t *dst,
+			    const parser_env_t *penv)
+{
+	uint32_t *qos_id = (((void *)obj) + parse->field_offset);
+	slurmdb_qos_rec_t *qos;
+
+	if (*qos_id == 0) {
+		data_set_null(dst);
+		return SLURM_SUCCESS;
+	}
+
+	/* find qos by id from global list */
+	xassert(penv->g_qos_list);
+	if (!penv->g_qos_list)
+		return ESLURM_REST_EMPTY_RESULT;
+
+	qos = list_find_first(penv->g_qos_list, slurmdb_find_qos_in_list,
+			      qos_id);
+	if (!qos)
+		/* QOS has an ID but it is not found??? */
+		return ESLURM_REST_EMPTY_RESULT;
+
+	(void)data_set_string(dst, qos->name);
+
+	return SLURM_SUCCESS;
+}
+
+static int _parse_qos_str_list(const parser_t *const parse, void *obj,
+			       data_t *src, data_t *errors,
+			       const parser_env_t *penv)
+{
+	List *qos_list = (((void *)obj) + parse->field_offset);
+	for_each_parse_qos_t args = {
+		.magic = MAGIC_FOREACH_PARSE_QOS,
+		.errors = errors,
+	};
+
+	if (!*qos_list)
+		*qos_list = list_create(xfree_ptr);
+
+	args.qos_list = *qos_list;
+
+	if (data_list_for_each(src, _for_each_parse_qos, &args) < 0)
+		return ESLURM_REST_FAIL_PARSING;
+
+	return SLURM_SUCCESS;
+}
+
+#define MAGIC_FOREACH_DUMP_QOS_STR_LIST 0xaaae2af2
+typedef struct {
+	int magic;
+	data_t *qos;
+} foreach_dump_qos_str_list_t;
+
+static int _foreach_dump_qos_str_list(void *x, void *arg)
+{
+	char *qos = x;
+	foreach_dump_qos_str_list_t *args = arg;
+
+	xassert(args->magic == MAGIC_FOREACH_DUMP_QOS_STR_LIST);
+
+	data_set_string(data_list_append(args->qos), qos);
+	return 0;
+}
+
+static int _dump_qos_str_list(const parser_t *const parse, void *obj,
+			      data_t *dst, const parser_env_t *penv)
+{
+        //List *qos_list = (((void *)obj) + parse->field_offset);
+	foreach_dump_qos_str_list_t args = {
+		.magic = MAGIC_FOREACH_DUMP_QOS_STR_LIST,
+		.qos = dst,
+	};
+	/* Convert list of QOS id strings into actual name strings */
+	List qos_list_names = NULL; //get_qos_name_list(penv->g_qos_list, *qos_list);
+	if (!qos_list_names)
+		return SLURM_SUCCESS;
+
+	list_sort(qos_list_names, slurm_sort_char_list_asc);
+
+	xassert(data_get_type(dst) == DATA_TYPE_NULL);
+	data_set_list(dst);
+
+	if (list_for_each(qos_list_names, _foreach_dump_qos_str_list,
+			  &args) < 0) {
+		FREE_NULL_LIST(qos_list_names);
+		return ESLURM_DATA_CONV_FAILED;
+	}
+
+	FREE_NULL_LIST(qos_list_names);
+	return SLURM_SUCCESS;
+}
+
+#define MAGIC_FOREACH_PARSE_QOS_ID 0xabaa2c19
+typedef struct {
+	int magic;
+	List qos_list;
+	List g_qos_list;
+	data_t *errors;
+} for_each_parse_qos_id_t;
+
+static data_for_each_cmd_t _for_each_parse_qos_id(data_t *data, void *arg)
+{
+	data_for_each_cmd_t rc;
+	for_each_parse_qos_id_t *args = arg;
+	data_t *name;
+
+	xassert(args->magic == MAGIC_FOREACH_PARSE_QOS_ID);
+
+	rc = _parse_qos_common(data, &name);
+	if (rc != DATA_FOR_EACH_CONT)
+		return rc;
+
+	if (slurmdb_addto_qos_char_list(args->qos_list, args->g_qos_list,
+					data_get_string(name), 0) > 0) {
+		return DATA_FOR_EACH_CONT;
+	} else {
+		resp_error(args->errors, ESLURM_REST_FAIL_PARSING,
+			   "QOS name to ID conversion failed",
+			   data_get_string(name));
+
+		return DATA_FOR_EACH_FAIL;
+	}
+}
+
+#define EMPTY_QOS_ID_ENTRY "\'\'"
+static int _parse_qos_id_list(const parser_t *const parse, void *obj,
+			      data_t *src, data_t *errors,
+			      const parser_env_t *penv)
+{
+	List *qos_list = (((void *)obj) + parse->field_offset);
+	for_each_parse_qos_id_t args = {
+		.magic = MAGIC_FOREACH_PARSE_QOS_ID,
+		.errors = errors,
+		.g_qos_list = penv->g_qos_list,
+	};
+
+	xassert(penv->g_qos_list);
+
+	if (!*qos_list)
+		*qos_list = list_create(xfree_ptr);
+
+	args.qos_list = *qos_list;
+
+	if (data_list_for_each(src, _for_each_parse_qos_id, &args) < 0)
+		return ESLURM_REST_FAIL_PARSING;
+
+	if (list_is_empty(*qos_list)) {
+		/*
+		 * If the QOS list is empty, then we need to set this special
+		 * entry to notify slurmdbd that this is explicilty empty and
+		 * not a no change request
+		 */
+		list_append(*qos_list, EMPTY_QOS_ID_ENTRY);
+	}
+
+	return SLURM_SUCCESS;
+}
+
+static int _dump_qos_preempt_list(const parser_t *const parse, void *obj,
+				  data_t *dst, const parser_env_t *penv)
+{
+	bitstr_t **preempt_bitstr =
+		(((void *)obj) + offsetof(slurmdb_qos_rec_t, preempt_bitstr));
+
+#ifndef NDEBUG
+	List *preempt_list =
+		(((void *)obj) + offsetof(slurmdb_qos_rec_t, preempt_list));
+#endif
+
+	xassert(!*preempt_list);
+	xassert(penv->g_qos_list);
+	xassert(data_get_type(dst) == DATA_TYPE_NULL);
+
+	data_set_list(dst);
+
+	if (!penv->g_qos_list)
+		return ESLURM_NOT_SUPPORTED;
+
+	if (!*preempt_bitstr)
+		return SLURM_SUCCESS;
+
+	/* based on get_qos_complete_str_bitstr() */
+	for (int i = 0; (i < bit_size(*preempt_bitstr)); i++) {
+		slurmdb_qos_rec_t *ptr_qos;
+
+		if (!bit_test(*preempt_bitstr, i))
+			continue;
+
+		if (!(ptr_qos = list_find_first(penv->g_qos_list,
+						slurmdb_find_qos_in_list,
+						&i))) {
+			/*
+			 * There is a race condition here where the global
+			 * QOS list could have changed betwen the query of the
+			 * list and the bitstrs. Just error and have the user
+			 * try again.
+			 */
+			error("%s: unable to find QOS with level: %u", __func__,
+			      i);
+			return ESLURM_DATA_CONV_FAILED;
+		}
+
+		data_set_string(data_list_append(dst), ptr_qos->name);
+	}
+
+	return SLURM_SUCCESS;
+}
+
+#define MAGIC_FOREACH_PARSE_ASSOC 0xdbed1a13
+typedef struct {
+	int magic;
+	data_t *errors;
+	List assoc_list;
+	const parser_env_t *penv;
+} foreach_parse_assoc_t;
+
+static data_for_each_cmd_t _foreach_parse_assoc(data_t *data, void *arg)
+{
+	foreach_parse_assoc_t *args = arg;
+	slurmdb_assoc_rec_t *assoc;
+
+	xassert(args->magic == MAGIC_FOREACH_PARSE_ASSOC);
+
+	if (data_get_type(data) != DATA_TYPE_DICT)
+		return DATA_FOR_EACH_FAIL;
+
+	assoc = xmalloc(sizeof(*assoc));
+	slurmdb_init_assoc_rec(assoc, false);
+	list_append(args->assoc_list, assoc);
+
+	if (_parser_run(assoc, parse_assoc, ARRAY_SIZE(parse_assoc), data,
+			args->errors, args->penv))
+		return DATA_FOR_EACH_FAIL;
+
+	return DATA_FOR_EACH_CONT;
+}
+
+static int _parse_assoc_list(const parser_t *const parse, void *obj,
+			     data_t *src, data_t *errors,
+			     const parser_env_t *penv)
+{
+	List *assoc_list = (((void *)obj) + parse->field_offset);
+	foreach_parse_assoc_t assoc_args = {
+		.magic = MAGIC_FOREACH_PARSE_ASSOC,
+		.assoc_list = *assoc_list,
+		.penv = penv,
+	};
+
+	if (data_get_type(src) != DATA_TYPE_LIST)
+		return ESLURM_REST_FAIL_PARSING;
+
+	if (data_list_for_each(src, _foreach_parse_assoc, &assoc_args) < 0)
+		return ESLURM_REST_FAIL_PARSING;
+
+	return SLURM_SUCCESS;
+}
+
+#define MAGIC_FOREACH_ASSOC 0xfefe2af3
+typedef struct {
+	int magic;
+	data_t *assocs;
+	const parser_env_t *penv;
+} foreach_assoc_t;
+
+static int _foreach_assoc(void *x, void *arg)
+{
+	slurmdb_assoc_rec_t *assoc = x;
+	foreach_assoc_t *args = arg;
+
+	xassert(args->magic == MAGIC_FOREACH_ASSOC);
+
+	if (_parser_dump(assoc, parse_assoc, ARRAY_SIZE(parse_assoc),
+			 data_set_dict(data_list_append(args->assocs)),
+			 args->penv) < 0)
+		return -1;
+
+	return 0;
+}
+
+static int _dump_assoc_list(const parser_t *const parse, void *obj, data_t *dst,
+			    const parser_env_t *penv)
+{
+	List *assoc_list = (((void *)obj) + parse->field_offset);
+	foreach_assoc_t args = {
+		.magic = MAGIC_FOREACH_ASSOC,
+		.penv = penv,
+	};
+
+	xassert(data_get_type(dst) == DATA_TYPE_NULL);
+	args.assocs = data_set_list(dst);
+
+	if (!*assoc_list)
+		return 0;
+
+	if (list_for_each(*assoc_list, _foreach_assoc, &args) < 0)
+		return -1;
+
+	return 0;
+}
+
+#define MAGIC_FOREACH_PARSE_ASSOC_SHORT 0x8bbd1a00
+typedef struct {
+	int magic;
+	data_t *errors;
+	List assoc_list;
+	const parser_env_t *penv;
+} foreach_parse_assoc_short_t;
+
+static data_for_each_cmd_t _foreach_parse_assoc_short(data_t *data, void *arg)
+{
+	foreach_parse_assoc_t *args = arg;
+	slurmdb_assoc_rec_t *assoc;
+
+	xassert(args->magic == MAGIC_FOREACH_PARSE_ASSOC_SHORT);
+
+	if (data_get_type(data) != DATA_TYPE_DICT)
+		return DATA_FOR_EACH_FAIL;
+
+	assoc = xmalloc(sizeof(*assoc));
+	slurmdb_init_assoc_rec(assoc, false);
+	list_append(args->assoc_list, assoc);
+
+	if (_parser_run(assoc, parse_assoc_short, ARRAY_SIZE(parse_assoc_short),
+			data, args->errors, args->penv))
+		return DATA_FOR_EACH_FAIL;
+
+	return DATA_FOR_EACH_CONT;
+}
+
+static int _parse_assoc_short_list(const parser_t *const parse, void *obj,
+				   data_t *src, data_t *errors,
+				   const parser_env_t *penv)
+{
+	List *assoc_list = (((void *)obj) + parse->field_offset);
+	foreach_parse_assoc_short_t assoc_args = {
+		.magic = MAGIC_FOREACH_PARSE_ASSOC_SHORT,
+		.assoc_list = *assoc_list,
+		.penv = penv,
+	};
+
+	if (data_get_type(src) != DATA_TYPE_LIST)
+		return ESLURM_REST_FAIL_PARSING;
+
+	if (data_list_for_each(src, _foreach_parse_assoc_short, &assoc_args) <
+	    0)
+		return ESLURM_REST_FAIL_PARSING;
+
+	return SLURM_SUCCESS;
+}
+
+#define MAGIC_FOREACH_ACCT_SHORT 0xaefeb0f1
+typedef struct {
+	int magic;
+	data_t *accts;
+} foreach_acct_short_t;
+
+static int _foreach_acct_short(void *x, void *arg)
+{
+	slurmdb_account_rec_t *acct = x;
+	foreach_acct_short_t *args = arg;
+
+	xassert(args->magic == MAGIC_FOREACH_ACCT_SHORT);
+
+	(void)data_set_string(data_list_append(args->accts), acct->name);
+	return 0;
+}
+
+static int _dump_acct_list(const parser_t *const parse, void *obj, data_t *dst,
+			   const parser_env_t *penv)
+{
+	List *acct_list = (((void *)obj) + parse->field_offset);
+	foreach_acct_short_t args = {
+		.magic = MAGIC_FOREACH_ACCT_SHORT,
+	};
+
+	xassert(data_get_type(dst) == DATA_TYPE_NULL);
+	args.accts = data_set_list(dst);
+
+	if (!*acct_list)
+		return 0;
+
+	if (list_for_each(*acct_list, _foreach_acct_short, &args) < 0)
+		return -1;
+
+	return 0;
+}
+
+#define MAGIC_FOREACH_ACCT_PARSE_SHORT 0x8eaeb0f1
+typedef struct {
+	int magic;
+	List acct_list;
+} foreach_acct_short_parse_t;
+
+static data_for_each_cmd_t _for_each_parse_assoc(data_t *data, void *arg)
+{
+	foreach_acct_short_parse_t *args = arg;
+
+	if (data_get_type(data) == DATA_TYPE_NULL)
+		return DATA_FOR_EACH_FAIL;
+	else if (data_convert_type(data, DATA_TYPE_STRING) ==
+		 DATA_TYPE_STRING) {
+		list_append(args->acct_list, data_get_string(data));
+		return DATA_FOR_EACH_CONT;
+	}
+
+	return DATA_FOR_EACH_FAIL;
+}
+
+static int _parse_acct_list(const parser_t *const parse, void *obj, data_t *src,
+			    data_t *errors, const parser_env_t *penv)
+{
+	List *acct_list = (((void *)obj) + parse->field_offset);
+	foreach_acct_short_parse_t args = {
+		.magic = MAGIC_FOREACH_ACCT_PARSE_SHORT,
+		.acct_list = *acct_list = list_create(xfree_ptr),
+	};
+
+	if (data_get_type(src) != DATA_TYPE_LIST)
+		return ESLURM_REST_FAIL_PARSING;
+
+	if (data_list_for_each(src, _for_each_parse_assoc, &args) < 0)
+		return ESLURM_REST_FAIL_PARSING;
+
+	return SLURM_SUCCESS;
+}
+
+#define MAGIC_FOREACH_ASSOC_SHORT 0xfefe00f0
+typedef struct {
+	int magic;
+	data_t *assocs;
+	const parser_env_t *penv;
+} foreach_assoc_short_t;
+
+static int _foreach_assoc_short(void *x, void *arg)
+{
+	slurmdb_assoc_rec_t *assoc = x;
+	foreach_assoc_t *args = arg;
+
+	xassert(args->magic == MAGIC_FOREACH_ASSOC_SHORT);
+
+	if (_parser_dump(assoc, parse_assoc_short,
+			 ARRAY_SIZE(parse_assoc_short),
+			 data_set_dict(data_list_append(args->assocs)),
+			 args->penv) < 0)
+		return -1;
+
+	return 0;
+}
+
+static int _dump_assoc_short_list(const parser_t *const parse, void *obj,
+				  data_t *dst, const parser_env_t *penv)
+{
+	List *assoc_list = (((void *)obj) + parse->field_offset);
+	foreach_assoc_short_t args = {
+		.magic = MAGIC_FOREACH_ASSOC_SHORT,
+		.penv = penv,
+	};
+
+	xassert(data_get_type(dst) == DATA_TYPE_NULL);
+	args.assocs = data_set_list(dst);
+
+	if (!*assoc_list)
+		return 0;
+
+	if (list_for_each(*assoc_list, _foreach_assoc_short, &args) < 0)
+		return -1;
+
+	return 0;
+}
+
+#define MAGIC_FIND_ASSOC 0xa8ba2c18
+typedef struct {
+	int magic;
+	slurmdb_assoc_rec_t *assoc;
+} find_assoc_id_t;
+
+/* checks for mis-matches and rejects on the spot */
+#define _match(field)                                              \
+	do {                                                       \
+		/* both null */                                    \
+		if (!args->assoc->field && !assoc->field)          \
+			continue;                                  \
+		/* only  1 is null */                              \
+		if (!args->assoc->field != !assoc->field)          \
+			return 0;                                  \
+		if (xstrcasecmp(args->assoc->field, assoc->field)) \
+			return 0;                                  \
+	} while (0)
+
+static int _find_assoc_id(void *x, void *key)
+{
+	slurmdb_assoc_rec_t *assoc = x;
+	find_assoc_id_t *args = key;
+
+	xassert(args->magic == MAGIC_FIND_ASSOC);
+
+	if ((args->assoc->id > 0) && (args->assoc->id == assoc->id))
+		return 1;
+
+	_match(acct);
+	_match(cluster);
+	_match(cluster);
+	_match(partition);
+	_match(user);
+
+	return 1;
+}
+#undef _match
+
+static int _parse_assoc_id(const parser_t *const parse, void *obj, data_t *src,
+			   data_t *errors, const parser_env_t *penv)
+{
+	int rc = SLURM_SUCCESS;
+	uint32_t *associd = (((void *)obj) + parse->field_offset);
+	slurmdb_assoc_rec_t *assoc = xmalloc(sizeof(*assoc));
+	slurmdb_init_assoc_rec(assoc, false);
+
+	rc = _parser_run(assoc, parse_assoc_short,
+			 ARRAY_SIZE(parse_assoc_short), src, errors, penv);
+
+	if (!rc) {
+		find_assoc_id_t args = {
+			.magic = MAGIC_FIND_ASSOC,
+			.assoc = assoc,
+		};
+		slurmdb_assoc_rec_t *match = list_find_first(
+			penv->g_assoc_list, _find_assoc_id, &args);
+
+		if (match)
+			*associd = match->id;
+		else
+			rc = ESLURM_REST_EMPTY_RESULT;
+	}
+
+	slurmdb_destroy_assoc_rec(assoc);
+
+	return rc;
+}
+
+static int _dump_assoc_id(const parser_t *const parse, void *obj, data_t *dst,
+			  const parser_env_t *penv)
+{
+	uint32_t *associd = (((void *)obj) + parse->field_offset);
+	slurmdb_assoc_rec_t *assoc = NULL;
+
+	if (!*associd || (*associd == NO_VAL))
+		return SLURM_SUCCESS;
+
+	xassert(data_get_type(dst) == DATA_TYPE_NULL);
+	xassert(penv->g_assoc_list);
+
+	if (!(assoc = list_find_first(penv->g_assoc_list,
+				      slurmdb_find_assoc_in_list, associd))) {
+		error("%s: unable to resolve assoc_id %u",
+		      __func__, *associd);
+
+		data_set_dict(dst);
+		data_set_int(data_key_set(dst, "id"), *associd);
+		data_set_null(data_key_set(dst, "account"));
+		data_set_null(data_key_set(dst, "cluster"));
+		data_set_null(data_key_set(dst, "partition"));
+		data_set_null(data_key_set(dst, "user"));
+		return SLURM_SUCCESS;
+	}
+
+	return _parser_dump(assoc, parse_assoc_short,
+			    ARRAY_SIZE(parse_assoc_short), dst, penv);
+}
+
+static int _parse_tres(const parser_t *const parse, void *obj, data_t *src,
+		       data_t *errors, const parser_env_t *penv)
+{
+	slurmdb_tres_rec_t **tres = (((void *)obj) + parse->field_offset);
+
+	xassert(!parse->field_offset);
+
+	if (!penv->g_tres_list) {
+		xassert(penv->g_tres_list);
+		return ESLURM_NOT_SUPPORTED;
+	}
+
+	if (data_get_type(src) != DATA_TYPE_DICT)
+		return ESLURM_REST_FAIL_PARSING;
+
+	return _parser_run(*tres, parse_tres, ARRAY_SIZE(parse_tres), src,
+			   errors, penv);
+}
+
+static int _dump_tres(const parser_t *const parse, void *obj, data_t *dst,
+		      const parser_env_t *penv)
+{
+	slurmdb_tres_rec_t **tres = (((void *)obj) + parse->field_offset);
+
+	return _parser_dump(*tres, parse_tres, ARRAY_SIZE(parse_tres), dst,
+			    penv);
+}
+
+#define MAGIC_FOREACH_PARSE_TRES_COUNT 0xfbba2c18
+typedef struct {
+	int magic;
+	List tres;
+	data_t *errors;
+	const parser_env_t *penv;
+} for_each_parse_tres_t;
+
+#define MAGIC_FIND_TRES 0xf4ba2c18
+typedef struct {
+	int magic;
+	slurmdb_tres_rec_t *tres;
+} find_tres_id_t;
+
+static int _find_tres_id(void *x, void *key)
+{
+	find_tres_id_t *args = key;
+	slurmdb_tres_rec_t *tres = x;
+
+	xassert(args->magic == MAGIC_FIND_TRES);
+
+	debug5("Comparing database tres(name:%s, type:%s, id:%u) with requested(name:%s, type:%s, id:%u).",
+	       tres->name, tres->type, tres->id,
+	       args->tres->name, args->tres->type, args->tres->id);
+
+	if ((args->tres->id > 0) &&
+	    ((args->tres->id == tres->id) &&
+	     (!args->tres->type ||
+	      !xstrcasecmp(args->tres->type, tres->type)) &&
+	     (!args->tres->name ||
+	      !xstrcasecmp(args->tres->name, tres->name))))
+		return 1;
+	if ((!args->tres->name || !args->tres->name[0]) &&
+	    !xstrcasecmp(args->tres->type, tres->type))
+		return 1;
+	else if (!xstrcasecmp(args->tres->name, tres->name) &&
+		 !xstrcasecmp(args->tres->type, tres->type))
+		return 1;
+	else
+		return 0;
+}
+
+static data_for_each_cmd_t _for_each_parse_tres_count(data_t *data, void *arg)
+{
+	for_each_parse_tres_t *args = arg;
+	slurmdb_tres_rec_t *tres, *ftres;
+	data_t *errors = args->errors;
+	find_tres_id_t targs = {
+		.magic = MAGIC_FIND_TRES,
+	};
+
+	xassert(args->magic == MAGIC_FOREACH_PARSE_TRES_COUNT);
+
+	if (data_get_type(data) != DATA_TYPE_DICT)
+		return DATA_FOR_EACH_FAIL;
+
+	tres = xmalloc(sizeof(*tres));
+	(void)list_append(args->tres, tres);
+
+	if (_parser_run(tres, parse_tres, ARRAY_SIZE(parse_tres), data,
+			args->errors, args->penv))
+		return DATA_FOR_EACH_FAIL;
+
+	if (tres->count < 0) {
+		resp_error(errors, ESLURM_REST_FAIL_PARSING,
+			   "TRES count below 0", "count");
+		return DATA_FOR_EACH_FAIL;
+	}
+
+	targs.tres = tres;
+
+	/* Lookup from g_tres_list */
+	if ((ftres = list_find_first(args->penv->g_tres_list, _find_tres_id,
+				     &targs))) {
+		if ((tres->id > 0) && tres->id != ftres->id) {
+			char *msg = NULL;
+			xstrfmtcat(msg,
+				   "Requested TRES id(%d) doesn't match TRES type/name(%s/%s) which id is %d",
+				   tres->id, ftres->type, ftres->name,
+				   ftres->id);
+			resp_error(errors, ESLURM_INVALID_TRES, msg, __func__);
+			xfree(msg);
+			return DATA_FOR_EACH_FAIL;
+		}
+
+		if (!tres->id)
+			tres->id = ftres->id;
+	} else {
+		char *msg = NULL;
+		xstrfmtcat(msg,
+			   "Couldn't find TRES matching name:%s type:%s",
+			   targs.tres->name, targs.tres->type);
+		resp_error(errors, ESLURM_INVALID_TRES, msg, __func__);
+		xfree(msg);
+		return DATA_FOR_EACH_FAIL;
+	}
+
+	return DATA_FOR_EACH_CONT;
+}
+
+static int _parse_tres_list(const parser_t *const parse, void *obj, data_t *src,
+			    data_t *errors, const parser_env_t *penv)
+{
+	char **tres = (((void *)obj) + parse->field_offset);
+	int rc;
+	for_each_parse_tres_t args = {
+		.magic = MAGIC_FOREACH_PARSE_TRES_COUNT,
+		.penv = penv,
+		.tres = list_create(slurmdb_destroy_tres_rec),
+		.errors = errors,
+	};
+
+	if (!penv->g_tres_list) {
+		xassert(penv->g_tres_list);
+		rc = ESLURM_NOT_SUPPORTED;
+		goto cleanup;
+	}
+
+	if (data_get_type(src) != DATA_TYPE_LIST) {
+		rc = ESLURM_REST_FAIL_PARSING;
+		goto cleanup;
+	}
+
+	if (data_list_for_each(src, _for_each_parse_tres_count, &args) < 0) {
+		rc = ESLURM_REST_FAIL_PARSING;
+		goto cleanup;
+	}
+
+	if ((*tres = slurmdb_make_tres_string(args.tres, TRES_STR_FLAG_SIMPLE)))
+		rc = SLURM_SUCCESS;
+	else
+		rc = ESLURM_REST_FAIL_PARSING;
+
+cleanup:
+	FREE_NULL_LIST(args.tres);
+	return rc;
+}
+
+#define MAGIC_LIST_PER_TRES 0xf7f8baf0
+typedef struct {
+	int magic;
+	data_t *tres;
+	const parser_env_t *penv;
+} foreach_list_per_tres_t;
+
+static int _dump_tres_list_tres(void *x, void *arg)
+{
+	slurmdb_tres_rec_t *tres = (slurmdb_tres_rec_t *)x;
+	foreach_list_per_tres_t *args = arg;
+
+	if (!tres->type && tres->id) {
+		slurmdb_tres_rec_t *c = list_find_first(
+			args->penv->g_tres_list, slurmdb_find_tres_in_list,
+			&tres->id);
+
+		if (c) {
+			tres->type = xstrdup(c->type);
+			tres->name = xstrdup(c->name);
+		}
+	}
+
+	if (_parser_dump(tres, parse_tres, ARRAY_SIZE(parse_tres),
+			 data_set_dict(data_list_append(args->tres)),
+			 args->penv))
+		return -1;
+
+	return 0;
+}
+
+static int _dump_tres_list(const parser_t *const parse, void *obj, data_t *dst,
+			   const parser_env_t *penv)
+{
+	char **tres = (((void *)obj) + parse->field_offset);
+	List tres_list = NULL;
+	foreach_list_per_tres_t args = {
+		.magic = MAGIC_LIST_PER_TRES,
+		.tres = data_set_list(dst),
+		.penv = penv,
+	};
+
+	xassert(penv->g_tres_list);
+	if (!penv->g_tres_list)
+		return ESLURM_NOT_SUPPORTED;
+
+	if (!*tres || !*tres[0])
+		/* ignore empty TRES strings */
+		return SLURM_SUCCESS;
+
+	slurmdb_tres_list_from_string(&tres_list, *tres, TRES_STR_FLAG_BYTES);
+
+	if (!tres_list)
+		return ESLURM_DATA_CONV_FAILED;
+
+	list_for_each(tres_list, _dump_tres_list_tres, &args);
+
+	FREE_NULL_LIST(tres_list);
+
+	return SLURM_SUCCESS;
+}
+/* based on slurmdb_tres_rec_t but includes node and task */
+typedef struct {
+	uint64_t count;
+	char *node;
+	uint64_t task;
+	uint32_t id;
+	char *name;
+	char *type;
+} slurmdb_tres_nct_rec_t;
+
+#define _add_parse(mtype, field, path) \
+	add_parser(slurmdb_tres_nct_rec_t, mtype, false, field, path)
+#define _add_parse_req(mtype, field, path) \
+	add_parser(slurmdb_tres_nct_rec_t, mtype, true, field, path)
+/* should mirror the structure of slurmdb_tres_nct_rec_t  */
+static const parser_t parse_tres_nct[] = {
+	_add_parse_req(STRING, type, "type"), _add_parse(STRING, name, "name"),
+	_add_parse(UINT32, id, "id"),	      _add_parse(INT64, count, "count"),
+	_add_parse(INT64, task, "task"),      _add_parse(STRING, node, "node"),
+};
+#undef _add_parse
+#undef _add_parse_req
+
+typedef enum {
+	TRES_EXPLODE_COUNT = 1,
+	TRES_EXPLODE_NODE,
+	TRES_EXPLODE_TASK,
+} tres_explode_type_t;
+
+#define MAGIC_LIST_PER_TRES_TYPE_NCT 0xb1d8acd2
+typedef struct {
+	int magic;
+	tres_explode_type_t type;
+	slurmdb_tres_nct_rec_t *tres_nct;
+	int tres_nct_count;
+	hostlist_t host_list;
+} foreach_list_per_tres_type_nct_t;
+
+static int _foreach_list_per_tres_type_nct(void *x, void *arg)
+{
+	slurmdb_tres_rec_t *tres = (slurmdb_tres_rec_t *)x;
+	foreach_list_per_tres_type_nct_t *args = arg;
+	slurmdb_tres_nct_rec_t *tres_nct = NULL;
+
+	xassert(args->magic == MAGIC_LIST_PER_TRES_TYPE_NCT);
+
+	for (int i = 0; i < args->tres_nct_count; i++)
+		if (args->tres_nct[i].id == tres->id)
+			tres_nct = args->tres_nct + i;
+
+	xassert(tres_nct);
+	if (!tres_nct)
+		/* out of sync?? */
+		return -1;
+
+	switch (args->type) {
+	case TRES_EXPLODE_NODE:
+		xassert(!tres_nct->node);
+		free(tres_nct->node);
+		/* based on find_hostname() */
+		tres_nct->node = hostlist_nth(args->host_list, tres->count);
+		return 1;
+	case TRES_EXPLODE_TASK:
+		xassert(!tres_nct->task);
+		tres_nct->task = tres->count;
+		return 1;
+	case TRES_EXPLODE_COUNT:
+		xassert(!tres_nct->count);
+		tres_nct->count = tres->count;
+		return 1;
+	default:
+		fatal("%s: unexpected type", __func__);
+	}
+}
+
+#define MAGIC_FOREACH_POPULATE_GLOBAL_TRES_LIST 0x31b8aad2
+typedef struct {
+	int magic;
+	slurmdb_tres_nct_rec_t *tres_nct;
+	int tres_nct_count;
+	int offset;
+} foreach_populate_g_tres_list;
+
+static int _foreach_populate_g_tres_list(void *x, void *arg)
+{
+	slurmdb_tres_rec_t *tres = x;
+	foreach_populate_g_tres_list *args = arg;
+	slurmdb_tres_nct_rec_t *tres_nct = args->tres_nct + args->offset;
+
+	xassert(args->magic == MAGIC_FOREACH_POPULATE_GLOBAL_TRES_LIST);
+
+	tres_nct->id = tres->id;
+	tres_nct->name = tres->name;
+	tres_nct->type = tres->type;
+
+	xassert(args->offset < args->tres_nct_count);
+	args->offset += 1;
+	return 0;
+}
+
+static int _dump_tres_nct(data_t *dst, char *tres_count, char *tres_node, char
+			  *tres_task, char *nodes, const parser_env_t *penv)
+{
+	const List g_tres_list = penv->g_tres_list;
+	int rc = ESLURM_DATA_CONV_FAILED;
+	foreach_list_per_tres_type_nct_t args = {
+		.magic = MAGIC_LIST_PER_TRES_TYPE_NCT,
+	};
+	foreach_populate_g_tres_list gtres_args = {
+		.magic = MAGIC_FOREACH_POPULATE_GLOBAL_TRES_LIST,
+	};
+	slurmdb_tres_nct_rec_t *tres_nct = NULL;
+	int tres_nct_count = 0;
+	List tres_count_list = NULL;
+	List tres_node_list = NULL;
+	List tres_task_list = NULL;
+
+	xassert(data_get_type(dst) == DATA_TYPE_NULL);
+	data_set_list(dst);
+
+	xassert(g_tres_list);
+	if (!g_tres_list)
+		goto cleanup;
+
+	if (!tres_count && !tres_node && !tres_task)
+		/* ignore empty TRES strings */
+		goto cleanup;
+
+	args.tres_nct_count = gtres_args.tres_nct_count = tres_nct_count =
+		list_count(g_tres_list);
+	args.tres_nct = gtres_args.tres_nct = tres_nct = xcalloc(
+		list_count(g_tres_list), sizeof(*tres_nct));
+	if (list_for_each(g_tres_list, _foreach_populate_g_tres_list,
+			  &gtres_args) < 0)
+		goto cleanup;
+
+	args.host_list = hostlist_create(nodes);
+
+	slurmdb_tres_list_from_string(&tres_count_list, tres_count,
+				      TRES_STR_FLAG_BYTES);
+	slurmdb_tres_list_from_string(&tres_node_list, tres_node,
+				      TRES_STR_FLAG_BYTES);
+	slurmdb_tres_list_from_string(&tres_task_list, tres_task,
+				      TRES_STR_FLAG_BYTES);
+
+	args.type = TRES_EXPLODE_COUNT;
+	if (tres_count_list &&
+	    (list_for_each(tres_count_list, _foreach_list_per_tres_type_nct,
+			   &args) < 0))
+		goto cleanup;
+	args.type = TRES_EXPLODE_NODE;
+	if (tres_node_list &&
+	    (list_for_each(tres_node_list, _foreach_list_per_tres_type_nct,
+			   &args) < 0))
+		goto cleanup;
+	args.type = TRES_EXPLODE_TASK;
+	if (tres_task_list &&
+	    (list_for_each(tres_task_list, _foreach_list_per_tres_type_nct,
+			   &args) < 0))
+		goto cleanup;
+	xassert(!(args.type = 0));
+
+	for (int i = 0; i < tres_nct_count; i++)
+		if (tres_nct[i].count || tres_nct[i].node || tres_nct[i].task)
+			_parser_dump((tres_nct + i), parse_tres_nct,
+				     ARRAY_SIZE(parse_tres_nct),
+				     data_set_dict(data_list_append(dst)),
+				     penv);
+
+	rc = SLURM_SUCCESS;
+cleanup:
+	FREE_NULL_LIST(tres_count_list);
+	FREE_NULL_LIST(tres_node_list);
+	FREE_NULL_LIST(tres_task_list);
+	FREE_NULL_HOSTLIST(args.host_list);
+	for (int i = 0; i < tres_nct_count; i++)
+		/* hostlist_nth doesn't use xfree() */
+		free(tres_nct[i].node);
+	xfree(tres_nct);
+
+	return rc;
+}
+
+static int _parse_admin_lvl(const parser_t *const parse, void *obj, data_t *src,
+			    data_t *errors, const parser_env_t *penv)
+{
+	uint16_t *admin_level = (((void *)obj) + parse->field_offset);
+
+	if (data_convert_type(src, DATA_TYPE_STRING) != DATA_TYPE_STRING)
+		return ESLURM_REST_FAIL_PARSING;
+
+	*admin_level = str_2_slurmdb_admin_level(data_get_string(src));
+
+	if (*admin_level == SLURMDB_ADMIN_NOTSET)
+		return ESLURM_REST_FAIL_PARSING;
+
+	return SLURM_SUCCESS;
+}
+
+static int _dump_admin_lvl(const parser_t *const parse, void *obj, data_t *dst,
+			   const parser_env_t *penv)
+{
+	uint16_t *admin_level = (((void *)obj) + parse->field_offset);
+
+	(void)data_set_string(dst, slurmdb_admin_level_str(*admin_level));
+
+	return SLURM_SUCCESS;
+}
+
+#define MAGIC_FOREACH_PARSE_COORD 0xdeed1a14
+typedef struct {
+	int magic;
+	data_t *errors;
+	List coord_list;
+	const parser_env_t *penv;
+} foreach_update_coord_t;
+
+static data_for_each_cmd_t _foreach_update_coord(data_t *data, void *arg)
+{
+	foreach_update_coord_t *args = arg;
+	slurmdb_coord_rec_t *coord;
+
+	xassert(args->magic == MAGIC_FOREACH_PARSE_COORD);
+
+	if (data_get_type(data) != DATA_TYPE_DICT)
+		return DATA_FOR_EACH_FAIL;
+
+	coord = xmalloc(sizeof(*coord));
+	list_append(args->coord_list, coord);
+
+	if (_parser_run(coord, parse_coord, ARRAY_SIZE(parse_coord), data,
+			args->errors, args->penv))
+		return DATA_FOR_EACH_FAIL;
+
+	return DATA_FOR_EACH_CONT;
+}
+
+static int _parse_coord_list(const parser_t *const parse, void *obj,
+			     data_t *src, data_t *errors,
+			     const parser_env_t *penv)
+{
+	List *coord_list = (((void *)obj) + parse->field_offset);
+	foreach_update_coord_t coord_args = {
+		.magic = MAGIC_FOREACH_PARSE_COORD,
+		.coord_list = *coord_list,
+		.penv = penv,
+	};
+
+	if (data_get_type(src) != DATA_TYPE_LIST)
+		return ESLURM_REST_FAIL_PARSING;
+
+	if (data_list_for_each(src, _foreach_update_coord, &coord_args) < 0)
+		return ESLURM_REST_FAIL_PARSING;
+
+	return SLURM_SUCCESS;
+}
+
+#define MAGIC_FOREACH_COORDINATOR 0xaefef2f5
+typedef struct {
+	int magic;
+	data_t *coordinators;
+	const parser_env_t *penv;
+} foreach_coordinator_t;
+
+static int _foreach_coordinator(void *x, void *arg)
+{
+	slurmdb_coord_rec_t *coor = x;
+	foreach_coordinator_t *args = arg;
+
+	xassert(args->magic == MAGIC_FOREACH_COORDINATOR);
+
+	if (_parser_dump(coor, parse_coord, ARRAY_SIZE(parse_coord),
+			 data_set_dict(data_list_append(args->coordinators)),
+			 args->penv))
+		return -1;
+
+	return 0;
+}
+
+static int _dump_coord_list(const parser_t *const parse, void *obj, data_t *dst,
+			    const parser_env_t *penv)
+{
+	List *coord_list = (((void *)obj) + parse->field_offset);
+	foreach_coordinator_t args = {
+		.magic = MAGIC_FOREACH_COORDINATOR,
+		.coordinators = data_set_list(dst),
+		.penv = penv,
+	};
+
+	if (list_for_each(*coord_list, _foreach_coordinator, &args) < 0)
+		return ESLURM_DATA_CONV_FAILED;
+
+	return SLURM_SUCCESS;
+}
+#define MAGIC_FOREACH_STEP 0x7e2eaef1
+typedef struct {
+	int magic;
+	data_t *steps;
+	const parser_env_t *penv;
+} foreach_step_t;
+
+static int _foreach_step(void *x, void *arg)
+{
+	int rc[4];
+	int trc = 1;
+	slurmdb_step_rec_t *step = x;
+	foreach_step_t *args = arg;
+	data_t *dstep = data_set_dict(data_list_append(args->steps));
+
+	xassert(args->magic == MAGIC_FOREACH_STEP);
+
+	hostlist_t host_list = hostlist_create(step->nodes);
+	if (!host_list) {
+		trc = -1;
+		goto cleanup;
+	}
+
+	xassert(hostlist_count(host_list) == step->nnodes);
+	if (hostlist_count(host_list)) {
+		char *host;
+		data_t *d = data_set_list(
+			data_define_dict_path(dstep, "nodes/list"));
+		hostlist_iterator_t itr = hostlist_iterator_create(host_list);
+
+		while ((host = hostlist_next(itr))) {
+			data_set_string(data_list_append(d), host);
+			free(host);
+		}
+
+		hostlist_iterator_destroy(itr);
+	}
+
+	rc[0] = _dump_tres_nct(data_define_dict_path(dstep,
+						     "tres/requested/max"),
+			       step->stats.tres_usage_in_max,
+			       step->stats.tres_usage_in_max_nodeid,
+			       step->stats.tres_usage_in_max_taskid,
+			       step->nodes, args->penv);
+	rc[1] = _dump_tres_nct(data_define_dict_path(dstep,
+						     "tres/requested/min"),
+			       step->stats.tres_usage_in_min,
+			       step->stats.tres_usage_in_min_nodeid,
+			       step->stats.tres_usage_in_min_taskid,
+			       step->nodes, args->penv);
+	rc[2] = _dump_tres_nct(data_define_dict_path(dstep,
+						     "tres/consumed/max"),
+			       step->stats.tres_usage_out_max,
+			       step->stats.tres_usage_out_max_nodeid,
+			       step->stats.tres_usage_out_max_taskid,
+			       step->nodes, args->penv);
+	rc[3] = _dump_tres_nct(data_define_dict_path(dstep,
+						     "tres/consumed/min"),
+			       step->stats.tres_usage_out_min,
+			       step->stats.tres_usage_out_min_nodeid,
+			       step->stats.tres_usage_out_min_taskid,
+			       step->nodes, args->penv);
+
+	if (rc[0] || rc[1] || rc[2] || rc[3]) {
+		trc = -1;
+		goto cleanup;
+	}
+
+	if (_parser_dump(step, parse_job_step, ARRAY_SIZE(parse_job_step),
+			 dstep, args->penv))
+		trc = -1;
+
+cleanup:
+	FREE_NULL_HOSTLIST(host_list);
+
+	return trc;
+}
+
+static int _dump_job_steps(const parser_t *const parse, void *obj, data_t *dst,
+			   const parser_env_t *penv)
+{
+	foreach_step_t args = {
+		.magic = MAGIC_FOREACH_STEP,
+		.steps = data_set_list(dst),
+		.penv = penv,
+	};
+	List *steps = (((void *)obj) + parse->field_offset);
+
+	if (list_for_each(*steps, _foreach_step, &args) < 0)
+		return ESLURM_DATA_CONV_FAILED;
+
+	return SLURM_SUCCESS;
+}
+
+static int _dump_job_exit_code(const parser_t *const parse, void *obj,
+			       data_t *dst, const parser_env_t *penv)
+{
+	uint32_t *ec = (((void *)obj) + parse->field_offset);
+	data_t *drc, *dsc;
+
+	xassert(data_get_type(dst) == DATA_TYPE_NULL);
+	(void)data_set_dict(dst);
+
+	dsc = data_key_set(dst, "status");
+	drc = data_key_set(dst, "return_code");
+
+	if (*ec == NO_VAL)
+		data_set_string(dsc, "PENDING");
+	else if (WIFEXITED(*ec)) {
+		data_set_string(dsc, "SUCCESS");
+		data_set_int(drc, 0);
+	} else if (WIFSIGNALED(*ec)) {
+		data_t *sig = data_set_dict(data_key_set(dst, "signal"));
+		data_set_string(dsc, "SIGNALED");
+
+		data_set_int(data_key_set(sig, "signal_id"), WTERMSIG(*ec));
+		data_set_string(data_key_set(sig, "name"),
+				strsignal(WTERMSIG(*ec)));
+	} else if (WCOREDUMP(*ec)) {
+		data_set_string(dsc, "CORE_DUMPED");
+	} else {
+		data_set_string(dsc, "ERROR");
+		data_set_int(drc, WEXITSTATUS(*ec));
+	}
+
+	return SLURM_SUCCESS;
+}
+
+#define _add_parse(mtype, field, path) \
+	add_parser(slurmdb_assoc_usage_t, mtype, false, field, path)
+/* should mirror the structure of slurmdb_assoc_usage_t */
+static const parser_t parse_assoc_usage[] = {
+	_add_parse(UINT32, accrue_cnt, "accrue_job_count"),
+	/* skipping children_list (not packed) */
+	/* skipping grp_node_bitmap (not packed) */
+	/* skipping grp_node_job_cnt (not packed) */
+	/* skipping grp_used_tres (not packed) */
+	/* skipping grp_used_tres_run_secs (not packed) */
+	_add_parse(FLOAT64, grp_used_wall, "group_used_wallclock"),
+	_add_parse(FLOAT64, fs_factor, "fairshare_factor"),
+	_add_parse(UINT32, level_shares, "fairshare_shares"),
+	/* skipping parent_assoc_ptr (not packed) */
+	_add_parse(FLOAT64, priority_norm, "normalized_priority"),
+	/* skipping fs_assoc_ptr (not packed) */
+	_add_parse(FLOAT128, shares_norm, "normalized_shares"),
+	/* skipping tres_count (not packed) */
+	_add_parse(FLOAT64, usage_efctv, "effective_normalized_usage"),
+	_add_parse(FLOAT64, usage_norm, "normalized_usage"),
+	_add_parse(UINT64, usage_raw, "raw_usage"),
+	/* skipping fs_assoc_ptr (not packed) */
+	/* skipping raw_TRES_usage (not packed) */
+	_add_parse(UINT32, used_jobs, "active_jobs"),
+	_add_parse(UINT32, used_submit_jobs, "job_count"),
+	_add_parse(FLOAT64, level_fs, "fairshare_level"),
+	/* skipping valid_qos */
+};
+#undef _add_parse
+
+static int _parse_assoc_usage(const parser_t *const parse, void *obj,
+			      data_t *src, data_t *errors,
+			      const parser_env_t *penv)
+{
+	slurmdb_assoc_rec_t *usage = (((void *)obj) + parse->field_offset);
+
+	if (data_get_type(src) != DATA_TYPE_DICT)
+		return ESLURM_REST_FAIL_PARSING;
+
+	return _parser_run(usage, parse_assoc_usage,
+			   ARRAY_SIZE(parse_assoc_usage), src, errors, penv);
+}
+
+static int _dump_assoc_usage(const parser_t *const parse, void *obj,
+			     data_t *dst, const parser_env_t *penv)
+{
+	slurmdb_assoc_rec_t **usage = (((void *)obj) + parse->field_offset);
+
+	xassert(data_get_type(dst) == DATA_TYPE_NULL);
+
+	if (*usage)
+		return _parser_dump(*usage, parse_assoc_usage,
+				    ARRAY_SIZE(parse_assoc_usage),
+				    data_set_dict(dst), penv);
+
+	return SLURM_SUCCESS;
+}
+
+static int _dump_stats_rec_array(const parser_t *const parse, void *obj,
+				 data_t *dst, const parser_env_t *penv)
+{
+	slurmdb_rollup_stats_t **ptr_stats = (((void *)obj) +
+					      parse->field_offset);
+	slurmdb_rollup_stats_t *rollup_stats;
+
+	xassert(data_get_type(dst) == DATA_TYPE_NULL);
+	data_set_list(dst);
+
+	if (!(rollup_stats = *ptr_stats))
+		return ESLURM_DATA_CONV_FAILED;
+
+	for (int i = 0; i < DBD_ROLLUP_COUNT; i++) {
+		data_t *d;
+		uint64_t roll_ave;
+
+		if (rollup_stats->time_total[i] == 0)
+			continue;
+
+		d = data_set_dict(data_list_append(dst));
+
+		if (i == 0)
+			data_set_string(data_key_set(d, "type"), "internal");
+		else if (i == 1)
+			data_set_string(data_key_set(d, "type"), "user");
+		else
+			data_set_string(data_key_set(d, "type"), "unknown");
+
+		data_set_int(data_key_set(d, "last_run"),
+			     rollup_stats->timestamp[i]);
+
+		roll_ave = rollup_stats->time_total[i];
+		if (rollup_stats->count[i] > 1)
+			roll_ave /= rollup_stats->count[i];
+
+		data_set_int(data_key_set(d, "last_cycle"),
+			     rollup_stats->time_last[i]);
+		data_set_int(data_key_set(d, "max_cycle"),
+			     rollup_stats->time_max[i]);
+		data_set_int(data_key_set(d, "total_time"),
+			     rollup_stats->time_total[i]);
+		data_set_int(data_key_set(d, "total_cycles"),
+			     rollup_stats->count[i]);
+		data_set_int(data_key_set(d, "mean_cycles"), roll_ave);
+	}
+
+	return SLURM_SUCCESS;
+}
+
+#define MAGIC_FOREACH_STATS_RPC 0x8a2e3ef1
+typedef struct {
+	int magic;
+	const parser_env_t *penv;
+	data_t *rpcs;
+} foreach_stats_rpc_t;
+
+static int _foreach_stats_rpc(void *x, void *arg)
+{
+	slurmdb_rpc_obj_t *rpc_obj = (slurmdb_rpc_obj_t *)x;
+	foreach_stats_rpc_t *args = arg;
+
+	xassert(args->magic == MAGIC_FOREACH_STATS_RPC);
+
+	if (_parser_dump(
+		    rpc_obj, parse_stats_rpcs, ARRAY_SIZE(parse_stats_rpcs),
+		    data_set_dict(data_list_append(args->rpcs)), args->penv))
+		return -1;
+
+	return 0;
+}
+
+static int _dump_stats_rpc_list(const parser_t *const parse, void *obj,
+				data_t *dst, const parser_env_t *penv)
+{
+	List *rpc_list = (((void *)obj) + parse->field_offset);
+	foreach_stats_rpc_t args = {
+		.magic = MAGIC_FOREACH_STATS_RPC,
+		.penv = penv,
+	};
+
+	xassert(data_get_type(dst) == DATA_TYPE_NULL);
+	args.rpcs = data_set_list(dst);
+
+	if (list_for_each(*rpc_list, _foreach_stats_rpc, &args) < 0)
+		return ESLURM_DATA_CONV_FAILED;
+
+	return SLURM_SUCCESS;
+}
+
+#define MAGIC_FOREACH_STATS_USER_RPC 0x8a2e3ef3
+typedef struct {
+	int magic;
+	const parser_env_t *penv;
+	data_t *users;
+} foreach_stats_user_rpc_t;
+
+static int _foreach_stats_user_rpc(void *x, void *arg)
+{
+	slurmdb_rpc_obj_t *rpc_obj = (slurmdb_rpc_obj_t *)x;
+	foreach_stats_user_rpc_t *args = arg;
+
+	xassert(args->magic == MAGIC_FOREACH_STATS_USER_RPC);
+
+	if (_parser_dump(rpc_obj, parse_stats_user_rpcs,
+			 ARRAY_SIZE(parse_stats_user_rpcs),
+			 data_set_dict(data_list_append(args->users)),
+			 args->penv))
+		return -1;
+
+	return 0;
+}
+
+static int _dump_stats_user_list(const parser_t *const parse, void *obj,
+				 data_t *dst, const parser_env_t *penv)
+{
+	List *user_list = (((void *)obj) + parse->field_offset);
+	foreach_stats_user_rpc_t args = {
+		.magic = MAGIC_FOREACH_STATS_USER_RPC,
+		.penv = penv,
+	};
+
+	xassert(data_get_type(dst) == DATA_TYPE_NULL);
+	args.users = data_set_list(dst);
+
+	if (list_for_each(*user_list, _foreach_stats_user_rpc, &args) < 0)
+		return ESLURM_DATA_CONV_FAILED;
+
+	return SLURM_SUCCESS;
+}
+
+static int _dump_rpc_id(const parser_t *const parse, void *obj, data_t *dst,
+			const parser_env_t *penv)
+{
+	slurmdbd_msg_type_t *id = (((void *)obj) + parse->field_offset);
+
+	xassert(data_get_type(dst) == DATA_TYPE_NULL);
+
+	data_set_string(dst, slurmdbd_msg_type_2_str(*id, 1));
+
+	return SLURM_SUCCESS;
+}
+
+static int _dump_clust_acct_rec(const parser_t *const parse, void *obj,
+				data_t *dst, const parser_env_t *penv)
+{
+	slurmdb_cluster_accounting_rec_t *acct = (((void *)obj) +
+						  parse->field_offset);
+
+	xassert(data_get_type(dst) == DATA_TYPE_NULL);
+	data_set_list(dst);
+
+	if (!acct)
+		return ESLURM_DATA_CONV_FAILED;
+
+	return SLURM_SUCCESS;
+}
+
+static int _parse_clust_acct_rec_list(const parser_t *const parse, void *obj,
+				      data_t *src, data_t *errors,
+				      const parser_env_t *penv)
+{
+	if (data_get_type(src) != DATA_TYPE_LIST)
+		return ESLURM_REST_FAIL_PARSING;
+
+	/*
+	 * List of stats: slurmdb_cluster_accounting_rec_t
+	 * This can not be ingested, so we will ignore it.
+	 */
+	debug("%s: ignoring slurmdb_cluster_accounting_rec_t", __func__);
+
+	return SLURM_SUCCESS;
+}
+
+#define MAGIC_FOREACH_ACCT_REC 0xa22e3ef3
+typedef struct {
+	int magic;
+	const parser_env_t *penv;
+	data_t *list;
+} _foreach_clust_acct_rec_t;
+
+static int _foreach_clust_acct_rec(void *x, void *arg)
+{
+	slurmdb_cluster_accounting_rec_t *obj = x;
+	_foreach_clust_acct_rec_t *args = arg;
+
+	xassert(args->magic == MAGIC_FOREACH_ACCT_REC);
+
+	if (_parser_dump(obj, parse_cluster_accounting_rec,
+			 ARRAY_SIZE(parse_cluster_accounting_rec),
+			 data_set_dict(data_list_append(args->list)),
+			 args->penv))
+		return -1;
+
+	return 0;
+}
+
+static int _dump_clust_acct_rec_list(const parser_t *const parse, void *obj,
+				     data_t *dst, const parser_env_t *penv)
+{
+	List *acct_list = (((void *)obj) + parse->field_offset);
+	_foreach_clust_acct_rec_t args = {
+		.magic = MAGIC_FOREACH_ACCT_REC,
+		.penv = penv,
+		.list = dst,
+	};
+
+	xassert(data_get_type(dst) == DATA_TYPE_NULL);
+
+	if (!acct_list)
+		return ESLURM_REST_FAIL_PARSING;
+
+	data_set_list(dst);
+
+	if (list_for_each(*acct_list, _foreach_clust_acct_rec, &args) < 0)
+		return ESLURM_REST_FAIL_PARSING;
+
+	return SLURM_SUCCESS;
+}
+
+static int _parse_select_plugin_id(const parser_t *const parse, void *obj,
+				   data_t *src, data_t *errors,
+				   const parser_env_t *penv)
+{
+	int *id = (((void *)obj) + parse->field_offset);
+
+	if (data_get_type(src) == DATA_TYPE_NULL)
+		return ESLURM_REST_FAIL_PARSING;
+	else if (data_convert_type(src, DATA_TYPE_STRING) == DATA_TYPE_STRING &&
+		 (*id = select_string_to_plugin_id(data_get_string(src)) > 0))
+		return SLURM_SUCCESS;
+
+	return ESLURM_REST_FAIL_PARSING;
+}
+
+static int _dump_select_plugin_id(const parser_t *const parse, void *obj,
+				  data_t *dst, const parser_env_t *penv)
+{
+	int *id = (((void *)obj) + parse->field_offset);
+	char *s = select_plugin_id_to_string(*id);
+
+	xassert(data_get_type(dst) == DATA_TYPE_NULL);
+
+	if (s) {
+		data_set_string(dst, s);
+	} else
+		data_set_null(dst);
+
+	return SLURM_SUCCESS;
+}
+
+static int _dump_task_distribution(const parser_t *const parse, void *obj,
+				   data_t *dst, const parser_env_t *penv)
+{
+	uint32_t *dist = (((void *)obj) + parse->field_offset);
+	char *d = slurm_step_layout_type_name(*dist);
+
+	xassert(data_get_type(dst) == DATA_TYPE_NULL);
+
+	data_set_string_own(dst, d);
+
+	return SLURM_SUCCESS;
+}
+
+static int _dump_step_id(const parser_t *const parse, void *obj, data_t *dst,
+			 const parser_env_t *penv)
+{
+	uint32_t *id = (((void *)obj) + parse->field_offset);
+
+	// TODO rewrite after bug#9622 resolved
+
+	switch (*id) {
+	case SLURM_EXTERN_CONT:
+		data_set_string(dst, "extern");
+		break;
+	case SLURM_BATCH_SCRIPT:
+		data_set_string(dst, "batch");
+		break;
+	case SLURM_PENDING_STEP:
+		data_set_string(dst, "pending");
+		break;
+	case SLURM_INTERACTIVE_STEP:
+		data_set_string(dst, "interactive");
+		break;
+	default:
+		data_set_int(dst, *id);
+	}
+
+	return SLURM_SUCCESS;
+}
+
+static int _dump_wckey_tag(const parser_t *const parse, void *obj, data_t *dst,
+			   const parser_env_t *penv)
+{
+	char **src = (((void *)obj) + parse->field_offset);
+	data_t *flags, *key;
+
+	xassert(data_get_type(dst) == DATA_TYPE_NULL);
+
+	if (!*src) {
+		data_set_null(dst);
+		return SLURM_SUCCESS;
+	}
+
+	key = data_key_set(data_set_dict(dst), "wckey");
+	flags = data_set_list(data_key_set(dst, "flags"));
+
+	if (*src[0] == '*') {
+		data_set_string(data_list_append(flags), "ASSIGNED_DEFAULT");
+		data_set_string(key, (*src + 1));
+	} else
+		data_set_string(key, *src);
+
+	return SLURM_SUCCESS;
+}
+
+static int _dump_user_id(const parser_t *const parse, void *obj, data_t *dst,
+			 const parser_env_t *penv)
+{
+	uid_t *uid = (((void *)obj) + parse->field_offset);
+	char *u;
+
+	xassert(data_get_type(dst) == DATA_TYPE_NULL);
+
+	if ((u = uid_to_string_or_null(*uid)))
+		data_set_string_own(dst, u);
+	else
+		data_set_null(dst);
+
+	return SLURM_SUCCESS;
+}
+
+static int _parse_user_id(const parser_t *const parse, void *obj, data_t *src,
+			  data_t *errors, const parser_env_t *penv)
+{
+	uid_t *uid = (((void *)obj) + parse->field_offset);
+
+	if (data_get_type(src) == DATA_TYPE_NULL)
+		return ESLURM_REST_FAIL_PARSING;
+	else if (data_convert_type(src, DATA_TYPE_STRING) == DATA_TYPE_STRING &&
+		 !uid_from_string(data_get_string(src), uid))
+		return SLURM_SUCCESS;
+
+	return ESLURM_REST_FAIL_PARSING;
+}
+
+static int _dump_group_id(const parser_t *const parse, void *obj, data_t *dst,
+			  const parser_env_t *penv)
+{
+	gid_t *gid = (((void *)obj) + parse->field_offset);
+	char *g;
+
+	xassert(data_get_type(dst) == DATA_TYPE_NULL);
+
+	if ((g = gid_to_string_or_null(*gid)))
+		data_set_string_own(dst, g);
+	else
+		data_set_null(dst);
+
+	return SLURM_SUCCESS;
+}
+
+static int _dump_job_reason(const parser_t *const parse, void *obj, data_t *dst,
+			    const parser_env_t *penv)
+{
+	uint32_t *state = (((void *)obj) + parse->field_offset);
+
+	xassert(data_get_type(dst) == DATA_TYPE_NULL);
+	data_set_string(dst, job_reason_string(*state));
+
+	return SLURM_SUCCESS;
+}
+
+static int _dump_job_state(const parser_t *const parse, void *obj, data_t *dst,
+			   const parser_env_t *penv)
+{
+	uint32_t *state = (((void *)obj) + parse->field_offset);
+
+	xassert(data_get_type(dst) == DATA_TYPE_NULL);
+	data_set_string(dst, job_state_string(*state));
+
+	return SLURM_SUCCESS;
+}
+
+typedef struct {
+	parse_rfunc_t rfunc;
+	parse_wfunc_t wfunc;
+	parser_type_t type;
+} parser_funcs_t;
+
+#define _add_func(rfuncp, wfuncp, typev)                        \
+	{                                                       \
+		.rfunc = rfuncp, .wfunc = wfuncp, .type = typev \
+	}
+const parser_funcs_t funcs[] = {
+	_add_func(_parse_to_string, _dump_to_string, PARSE_STRING),
+	_add_func(_parse_to_uint32, _dump_to_uint32, PARSE_UINT32),
+	_add_func(_parse_to_int64, _dump_to_int64, PARSE_INT64),
+	_add_func(_parse_to_uint64, _dump_to_uint64, PARSE_UINT64),
+	_add_func(_parse_to_uint16, _dump_to_uint16, PARSE_UINT16),
+	_add_func(_parse_flags, _dump_flags, PARSE_FLAGS),
+	_add_func(_parse_qos_str_id, _dump_qos_str_id, PARSE_QOS_ID),
+	_add_func(_parse_qos_str_list, _dump_qos_str_list, PARSE_QOS_STR_LIST),
+	_add_func(_parse_qos_id_list, _dump_qos_str_list, PARSE_QOS_ID_LIST),
+	_add_func(_parse_qos_id_list, _dump_qos_preempt_list,
+		  PARSE_QOS_PREEMPT_LIST),
+	_add_func(_parse_tres, _dump_tres, PARSE_TRES),
+	_add_func(_parse_tres_list, _dump_tres_list, PARSE_TRES_LIST),
+	_add_func(NULL, _dump_job_steps, PARSE_JOB_STEPS),
+	_add_func(NULL, _dump_job_exit_code, PARSE_JOB_EXIT_CODE),
+	_add_func(_parse_admin_lvl, _dump_admin_lvl, PARSE_ADMIN_LVL),
+	_add_func(_parse_acct_list, _dump_acct_list, PARSE_ACCOUNT_LIST),
+	_add_func(_parse_assoc_list, _dump_assoc_list, PARSE_ASSOC_LIST),
+	_add_func(_parse_assoc_short_list, _dump_assoc_short_list,
+		  PARSE_ASSOC_SHORT_LIST),
+	_add_func(_parse_assoc_usage, _dump_assoc_usage, PARSE_ASSOC_USAGE),
+	_add_func(_parse_assoc_id, _dump_assoc_id, PARSE_ASSOC_ID),
+	_add_func(_parse_coord_list, _dump_coord_list, PARSE_COORD_LIST),
+	_add_func(_parse_to_float64, _dump_to_float64, PARSE_FLOAT64),
+	_add_func(_parse_to_float128, _dump_to_float128, PARSE_FLOAT128),
+	_add_func(NULL, _dump_stats_rec_array, PARSE_STATS_REC_ARRAY),
+	_add_func(NULL, _dump_stats_rpc_list, PARSE_STATS_RPC_LIST),
+	_add_func(NULL, _dump_stats_user_list, PARSE_STATS_USER_LIST),
+	_add_func(NULL, _dump_rpc_id, PARSE_RPC_ID),
+	_add_func(NULL, _dump_clust_acct_rec, PARSE_CLUSTER_ACCT_REC),
+	_add_func(_parse_clust_acct_rec_list, _dump_clust_acct_rec_list,
+		  PARSE_CLUSTER_ACCT_REC_LIST),
+	_add_func(_parse_select_plugin_id, _dump_select_plugin_id,
+		  PARSE_SELECT_PLUGIN_ID),
+	_add_func(NULL, _dump_task_distribution, PARSE_TASK_DISTRIBUTION),
+	_add_func(NULL, _dump_step_id, PARSE_STEP_ID),
+	_add_func(NULL, _dump_wckey_tag, PARSE_WCKEY_TAG),
+	_add_func(NULL, _dump_group_id, PARSE_GROUP_ID),
+	_add_func(NULL, _dump_job_reason, PARSE_JOB_REASON),
+	_add_func(NULL, _dump_job_state, PARSE_JOB_STATE),
+	_add_func(_parse_user_id, _dump_user_id, PARSE_USER_ID),
+};
+#undef _add_func
+
+#define _add_parser(parser, typev)                        \
+	{                                                 \
+		.type = typev, .parse = parser,           \
+		.parse_member_count = ARRAY_SIZE(parser), \
+	}
+const parsers_t parsers[] = {
+	_add_parser(parse_assoc_short, PARSE_ASSOC_SHORT),
+	_add_parser(parse_assoc, PARSE_ASSOC),
+	_add_parser(parse_job_step, PARSE_JOB_STEP),
+	_add_parser(parse_user, PARSE_USER),
+	_add_parser(parse_job, PARSE_JOB),
+	_add_parser(parse_acct, PARSE_ACCOUNT),
+	_add_parser(parse_tres, PARSE_TRES),
+	_add_parser(parse_qos, PARSE_QOS),
+	_add_parser(parse_coord, PARSE_COORD),
+	_add_parser(parse_wckey, PARSE_WCKEY),
+	_add_parser(parse_stats_rec, PARSE_STATS_REC),
+	_add_parser(parse_cluster_rec, PARSE_CLUSTER_REC),
+};
+#undef _add_parser
+
+extern int parse(parser_type_t type, void *obj, data_t *src, data_t *errors,
+		 const parser_env_t *penv)
+{
+	for (int i = 0; i < ARRAY_SIZE(parsers); i++)
+		if (parsers[i].type == type)
+			return _parser_run(obj, parsers[i].parse,
+					   parsers[i].parse_member_count, src,
+					   errors, penv);
+
+	fatal("invalid type?");
+}
+
+extern int dump(parser_type_t type, void *obj, data_t *dst,
+		const parser_env_t *penv)
+{
+	for (int i = 0; i < ARRAY_SIZE(parsers); i++)
+		if (parsers[i].type == type)
+			return _parser_dump(obj, parsers[i].parse,
+					    parsers[i].parse_member_count, dst,
+					    penv);
+
+	fatal("invalid type?");
+}
+
+static int _parser_run(void *obj, const parser_t *const parse,
+		       const size_t parse_member_count, data_t *data,
+		       data_t *errors, const parser_env_t *penv)
+{
+	int rc = SLURM_SUCCESS;
+
+	for (int i = 0; (!rc) && (i < parse_member_count); i++) {
+		data_t *pd = data_resolve_dict_path(data, parse[i].key);
+		for (int f = 0; pd && (f < ARRAY_SIZE(funcs)); f++) {
+
+			if (parse[i].type == funcs[f].type) {
+				xassert(funcs[f].rfunc);
+				rc = funcs[f].rfunc((parse + i), obj, pd,
+						    errors, penv);
+				break;
+			}
+		}
+
+		if (!pd && parse[i].required) {
+			char *tmp_str =
+				xstrdup_printf("Missing required field '%s'",
+					       parse[i].key);
+			resp_error(errors, rc, tmp_str, __func__);
+			xfree(tmp_str);
+			break;
+		} else if (rc) {
+			char *tmp_str =
+				xstrdup_printf("Failed to parse %sfield '%s'",
+					       parse[i].required ?
+					       "required " : "",
+					       parse[i].key);
+			resp_error(errors, rc, tmp_str, __func__);
+			xfree(tmp_str);
+		}
+	}
+
+	return rc;
+}
+
+static int _parser_dump(void *obj, const parser_t *const parse,
+			const size_t parse_member_count, data_t *data,
+			const parser_env_t *penv)
+{
+	int rc = SLURM_SUCCESS;
+
+	for (int i = 0; (!rc) && (i < parse_member_count); i++) {
+		data_t *pd;
+
+		/* make sure we aren't clobbering something */
+		xassert(!data_resolve_dict_path(data, parse[i].key));
+
+		if (!(pd = data_define_dict_path(data, parse[i].key))) {
+			error("%s: failed to define field %s", __func__,
+			      parse[i].key);
+			rc = ESLURM_REST_EMPTY_RESULT;
+			break;
+		}
+
+		for (int f = 0; (!rc) && (f < ARRAY_SIZE(funcs)); f++) {
+			if (parse[i].type == funcs[f].type) {
+				xassert(funcs[f].wfunc);
+
+				if ((rc = funcs[f].wfunc((parse + i), obj, pd,
+							 penv))) {
+					error("%s: failed on field %s: %s",
+					      __func__, parse[i].key,
+					      slurm_strerror(rc));
+					break;
+				}
+			}
+		}
+	}
+
+	return rc;
+}
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/parse.h slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/parse.h
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/parse.h	1970-01-01 01:00:00.000000000 +0100
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/parse.h	2022-11-10 21:49:08.000000000 +0100
@@ -0,0 +1,119 @@
+/*****************************************************************************\
+ *  parse.h - Slurm REST API openapi operations handlers
+ *****************************************************************************
+ *  Copyright (C) 2019-2020 SchedMD LLC.
+ *  Written by Nathan Rini <nate@schedmd.com>
+ *
+ *  This file is part of Slurm, a resource management program.
+ *  For details, see <https://slurm.schedmd.com/>.
+ *  Please also read the included file: DISCLAIMER.
+ *
+ *  Slurm is free software; you can redistribute it and/or modify it under
+ *  the terms of the GNU General Public License as published by the Free
+ *  Software Foundation; either version 2 of the License, or (at your option)
+ *  any later version.
+ *
+ *  In addition, as a special exception, the copyright holders give permission
+ *  to link the code of portions of this program with the OpenSSL library under
+ *  certain conditions as described in each individual source file, and
+ *  distribute linked combinations including the two. You must obey the GNU
+ *  General Public License in all respects for all of the code used other than
+ *  OpenSSL. If you modify file(s) with this exception, you may extend this
+ *  exception to your version of the file(s), but you are not obligated to do
+ *  so. If you do not wish to do so, delete this exception statement from your
+ *  version.  If you delete this exception statement from all source files in
+ *  the program, then also delete it here.
+ *
+ *  Slurm is distributed in the hope that it will be useful, but WITHOUT ANY
+ *  WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+ *  FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
+ *  details.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with Slurm; if not, write to the Free Software Foundation, Inc.,
+ *  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301  USA.
+\*****************************************************************************/
+
+#ifndef SLURMRESTD_OPENAPI_DB_PARSE_V0038
+#define SLURMRESTD_OPENAPI_DB_PARSE_V0038
+
+#include "config.h"
+#include "slurm/slurm.h"
+#include "slurm/slurmdb.h"
+
+#include "src/common/data.h"
+#include "src/common/list.h"
+
+#include "src/slurmrestd/operations.h"
+
+typedef enum {
+	PARSE_INVALID = 0,
+	PARSE_ACCOUNT_LIST, /* list of slurmdb_accounting_rec_t * to id account names */
+	PARSE_ACCOUNT, /* slurmdb_account_rec_t* */
+	PARSE_ADMIN_LVL, /* uint16_t (placeholder for slurmdb_admin_level_t) */
+	PARSE_ASSOC_ID, /* slurmdb_assoc_usage_t */
+	PARSE_ASSOC_LIST, /* list of slurmdb_assoc_rec_t* */
+	PARSE_ASSOC_SHORT_LIST, /* list of slurmdb_assoc_rec_t* only for id */
+	PARSE_ASSOC_SHORT, /* slurmdb_assoc_rec_t* (for id only) */
+	PARSE_ASSOC, /* slurmdb_assoc_rec_t* */
+	PARSE_ASSOC_USAGE, /* slurmdb_assoc_usage_t */
+	PARSE_CLASSIFICATION_TYPE, /* slurmdb_classification_type_t */
+	PARSE_CLUSTER_ACCT_REC_LIST, /* list of slurmdb_cluster_accounting_rec_t* */
+	PARSE_CLUSTER_ACCT_REC, /* slurmdb_cluster_accounting_rec_t* */
+	PARSE_CLUSTER_CLASSIFICATION, /* uint16_t joined with slurmdb_classification_type_t */
+	PARSE_CLUSTER_REC, /* slurmdb_cluster_rec_t* */
+	PARSE_COORD_LIST, /* List of slurmdb_coord_rec_t * */
+	PARSE_COORD, /* slurmdb_coord_rec_t* */
+	PARSE_FLAGS, /* must use with parser_enum_t array */
+	PARSE_FLOAT128, /* long double */
+	PARSE_FLOAT64, /* double */
+	PARSE_GROUP_ID, /* Group from numeric GID <-> gid_t */
+	PARSE_INT64, /* int64_t */
+	PARSE_JOB_EXIT_CODE, /* int32_t */
+	PARSE_JOB_REASON, /* uint32_t <-> enum job_state_reason */
+	PARSE_JOB, /* slurmdb_job_rec_t* */
+	PARSE_JOB_STATE, /* uint32_t <-> JOB_STATE_FLAGS */
+	PARSE_JOB_STEP, /* slurmdb_step_rec_t* */
+	PARSE_JOB_STEPS, /* slurmdb_job_rec_t->steps -> list of slurmdb_step_rec_t *'s*/
+	PARSE_QOS_ID, /* uint32_t of QOS id */
+	PARSE_QOS_PREEMPT_LIST, /* slurmdb_qos_rec_t->preempt_bitstr & preempt_list */
+	PARSE_QOS, /* slurmdb_qos_rec_t* */
+	PARSE_QOS_STR_LIST, /* List of char* of QOS names */
+	PARSE_QOS_ID_LIST, /* List of char* of QOS ids */
+	PARSE_RPC_ID, /* slurmdbd_msg_type_t */
+	PARSE_SELECT_PLUGIN_ID, /* int (SELECT_PLUGIN_*) -> string */
+	PARSE_STATS_REC_ARRAY, /* array of slurmdb_stats_rec_t* */
+	PARSE_STATS_REC, /* slurmdb_stats_rec_t* */
+	PARSE_STATS_RPC_LIST, /* list of slurmdb_rpc_obj_t* */
+	PARSE_STATS_USER_LIST, /* list of slurmdb_rpc_obj_t* */
+	PARSE_STEP_CPUFREQ_GOV, /* slurmdb_step_rec_t.req_cpufreq_gov (uint32_t) of CPU_FREQ_* flags */
+	PARSE_STEP_ID, /* uint32_t of job step id */
+	PARSE_STRING, /* char */
+	PARSE_TASK_DISTRIBUTION, /* uint32_t <-> task_dist_states_t */
+	PARSE_TRES_LIST, /* List of slurmdb_tres_rec_t* combined into a TRES string */
+	PARSE_TRES, /* slurmdb_tres_rec_t* */
+	PARSE_UINT16, /* uint16_t */
+	PARSE_UINT32, /* uint32_t */
+	PARSE_UINT64, /* uint64_t */
+	PARSE_USER_ID, /* User from numeric UID */
+	PARSE_USER, /* slurmdb_user_rec*  */
+	PARSE_WCKEY, /* slurmdb_wckey_rec_t*  */
+	PARSE_WCKEY_TAG, /* uint32_t - * prefix denotes default */
+} parser_type_t;
+
+typedef struct {
+	/* required for PARSE_ASSOC_LIST */
+	rest_auth_context_t *auth;
+	/* required for PARSE_TRES_COUNT */
+	List g_tres_list;
+	/* required for PARSE_QOS_ID */
+	List g_qos_list;
+	List g_assoc_list;
+} parser_env_t;
+
+extern int parse(parser_type_t type, void *obj, data_t *src, data_t *errors,
+		 const parser_env_t *penv);
+extern int dump(parser_type_t type, void *obj, data_t *dst,
+		const parser_env_t *penv);
+
+#endif
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/qos.c slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/qos.c
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/qos.c	1970-01-01 01:00:00.000000000 +0100
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/qos.c	2022-11-25 15:06:09.223023551 +0100
@@ -0,0 +1,396 @@
+/*****************************************************************************\
+ *  qos.c - Slurm REST API accounting QOS http operations handlers
+ *****************************************************************************
+ *  Copyright (C) 2020 SchedMD LLC.
+ *  Written by Nathan Rini <nate@schedmd.com>
+ *
+ *  This file is part of Slurm, a resource management program.
+ *  For details, see <https://slurm.schedmd.com/>.
+ *  Please also read the included file: DISCLAIMER.
+ *
+ *  Slurm is free software; you can redistribute it and/or modify it under
+ *  the terms of the GNU General Public License as published by the Free
+ *  Software Foundation; either version 2 of the License, or (at your option)
+ *  any later version.
+ *
+ *  In addition, as a special exception, the copyright holders give permission
+ *  to link the code of portions of this program with the OpenSSL library under
+ *  certain conditions as described in each individual source file, and
+ *  distribute linked combinations including the two. You must obey the GNU
+ *  General Public License in all respects for all of the code used other than
+ *  OpenSSL. If you modify file(s) with this exception, you may extend this
+ *  exception to your version of the file(s), but you are not obligated to do
+ *  so. If you do not wish to do so, delete this exception statement from your
+ *  version.  If you delete this exception statement from all source files in
+ *  the program, then also delete it here.
+ *
+ *  Slurm is distributed in the hope that it will be useful, but WITHOUT ANY
+ *  WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+ *  FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
+ *  details.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with Slurm; if not, write to the Free Software Foundation, Inc.,
+ *  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301  USA.
+\*****************************************************************************/
+
+#include "config.h"
+
+#include <stdint.h>
+
+#include "slurm/slurm.h"
+#include "slurm/slurmdb.h"
+
+#include "src/common/list.h"
+#include "src/common/log.h"
+#include "src/slurmrestd/openapi.h"
+#include "src/common/parse_time.h"
+#include "src/common/ref.h"
+#include "src/common/slurmdbd_defs.h"
+#include "src/common/slurm_protocol_api.h"
+#include "src/common/strlcpy.h"
+#include "src/common/uid.h"
+#include "src/common/xassert.h"
+#include "src/common/xmalloc.h"
+#include "src/common/xstring.h"
+
+#include "src/slurmrestd/operations.h"
+
+#include "src/slurmrestd/plugins/openapi/dbv0.0.38/api.h"
+
+enum {
+	TAG_ALL_QOS = 0,
+	TAG_SINGLE_QOS,
+};
+
+typedef struct {
+	data_t *errors;
+	slurmdb_qos_cond_t *qos_cond;
+} foreach_query_search_t;
+
+static data_for_each_cmd_t _foreach_query_search(const char *key,
+						 data_t *data,
+						 void *arg)
+{
+	foreach_query_search_t *args = arg;
+	data_t *errors = args->errors;
+
+	if (!xstrcasecmp("with_deleted", key)) {
+		if (data_convert_type(data, DATA_TYPE_BOOL) != DATA_TYPE_BOOL) {
+			resp_error(errors, ESLURM_REST_INVALID_QUERY,
+				   "must be a Boolean", NULL);
+			return DATA_FOR_EACH_FAIL;
+		}
+
+		if (data->data.bool_u)
+			args->qos_cond->with_deleted = true;
+		else
+			args->qos_cond->with_deleted = false;
+
+		return DATA_FOR_EACH_CONT;
+	}
+
+	resp_error(errors, ESLURM_REST_INVALID_QUERY, "Unknown query field",
+		   NULL);
+	return DATA_FOR_EACH_FAIL;
+}
+
+static int _foreach_qos(slurmdb_qos_rec_t *qos, data_t *dqos_list,
+			List qos_list, List g_tres_list)
+{
+	parser_env_t penv = {
+		.g_qos_list = qos_list,
+		.g_tres_list = g_tres_list,
+	};
+
+	return dump(PARSE_QOS, qos, data_set_dict(data_list_append(dqos_list)),
+		    &penv);
+}
+
+static int _dump_qos(data_t *resp, void *auth, List g_qos_list, char *qos_name)
+{
+	int rc = SLURM_SUCCESS;
+	data_t *errors = populate_response_format(resp);
+	slurmdb_qos_rec_t *qos;
+	ListIterator iter = list_iterator_create(g_qos_list);
+	data_t *dqos_list = data_set_list(data_key_set(resp, "QOS"));
+	List tres_list = NULL;
+	slurmdb_tres_cond_t tres_cond = {
+		.with_deleted = 1,
+	};
+
+	rc = db_query_list(errors, auth, &tres_list, slurmdb_tres_get,
+			   &tres_cond);
+
+	/*
+	 * We are forced to use iterator here due to calls inside of
+	 * _foreach_qos() that attempt to lock qos_list.
+	 */
+	while ((!rc) && (qos = list_next(iter)))
+		if (!qos_name || !xstrcmp(qos->name, qos_name))
+			rc = _foreach_qos(qos, dqos_list, g_qos_list,
+					  tres_list);
+
+	list_iterator_destroy(iter);
+	FREE_NULL_LIST(tres_list);
+
+	return SLURM_SUCCESS;
+}
+
+static int _foreach_delete_qos(void *x, void *arg)
+{
+	char *qos = x;
+	data_t *qoslist = arg;
+
+	data_set_string(data_list_append(qoslist), qos);
+
+	return DATA_FOR_EACH_CONT;
+}
+
+static int _delete_qos(data_t *resp, void *auth, data_t *errors,
+		       slurmdb_qos_cond_t *qos_cond)
+{
+	int rc = SLURM_SUCCESS;
+	List qos_list = NULL;
+
+	if (!(rc = db_query_list(errors, auth, &qos_list, slurmdb_qos_remove,
+				 qos_cond)) &&
+	    (list_for_each(qos_list, _foreach_delete_qos,
+			   data_set_list(data_key_set(resp, "removed_qos"))) <
+	     0)) {
+		resp_error(errors, ESLURM_REST_INVALID_QUERY,
+			   "unable to delete QOS", NULL);
+	}
+
+	if (!rc)
+		rc = db_query_commit(errors, auth);
+
+	FREE_NULL_LIST(qos_list);
+
+	return rc;
+}
+
+#define MAGIC_FOREACH_UP_QOS 0xdaebfae8
+typedef struct {
+	int magic;
+	List g_tres_list;
+	List g_qos_list;
+	data_t *errors;
+	rest_auth_context_t *auth;
+} foreach_update_qos_t;
+
+/* If the QOS already exists, update it. If not, create it */
+static data_for_each_cmd_t _foreach_update_qos(data_t *data, void *arg)
+{
+	foreach_update_qos_t *args = arg;
+	slurmdb_qos_rec_t *qos;
+	parser_env_t penv = {
+		.auth = args->auth,
+		.g_tres_list = args->g_tres_list,
+		.g_qos_list = args->g_qos_list,
+	};
+	int rc;
+	List qos_list = NULL;
+	slurmdb_qos_cond_t cond = {0};
+	bool qos_exists;
+
+	xassert(args->magic == MAGIC_FOREACH_UP_QOS);
+
+	if (data_get_type(data) != DATA_TYPE_DICT) {
+		resp_error(args->errors, ESLURM_REST_INVALID_QUERY,
+			   "each QOS entry must be a dictionary", NULL);
+		return DATA_FOR_EACH_FAIL;
+	}
+
+	qos = xmalloc(sizeof(slurmdb_qos_rec_t));
+	slurmdb_init_qos_rec(qos, false, NO_VAL);
+	/*
+	 * Clear the QOS_FLAG_NOTSET by slurmdb_init_qos_rec() so that
+	 * flag updates won't be ignored.
+	 */
+	qos->flags = 0;
+
+	/* force to off instead of NO_VAL */
+	qos->preempt_mode = PREEMPT_MODE_OFF;
+
+	if (parse(PARSE_QOS, qos, data, args->errors, &penv)) {
+		slurmdb_destroy_qos_rec(qos);
+		return DATA_FOR_EACH_FAIL;
+	}
+
+	/* Search for a QOS with the same id and/or name, if set */
+	if (qos->id || qos->name) {
+		data_t *query_errors = data_new();
+		if (qos->id) {
+			/* Need to free string copy of id with xfree_ptr */
+			cond.id_list = list_create(xfree_ptr);
+			list_append(cond.id_list,
+				    xstrdup_printf("%u", qos->id));
+		}
+		if (qos->name) {
+			/* Temporarily alias/borrow qos->name into cond */
+			cond.name_list = list_create(NULL);
+			list_append(cond.name_list, qos->name);
+		}
+
+		/* See if QOS already exists */
+		rc = db_query_list(query_errors, args->auth, &qos_list,
+				   slurmdb_qos_get, &cond);
+		FREE_NULL_DATA(query_errors);
+		qos_exists = ((rc == SLURM_SUCCESS) && qos_list &&
+			      !list_is_empty(qos_list));
+	} else
+		qos_exists = false;
+
+	if (!qos_exists && qos->id) {
+		/* No QOS exists for qos->id. Can't update */
+		rc = resp_error(args->errors, ESLURM_REST_INVALID_QUERY,
+				"QOS was not found for the requested ID",
+				"_foreach_update_qos");
+	} else if (!qos_exists && !qos->name) {
+		/* Can't create a QOS without a name */
+		rc = resp_error(args->errors, ESLURM_REST_INVALID_QUERY,
+				"Cannot create a QOS without a name",
+				"_foreach_update_qos");
+	} else if (!qos_exists) {
+		/* The QOS was not found, so create a new QOS */
+		List qos_add_list = list_create(NULL);
+		debug("%s: adding qos request: name=%s description=%s",
+		      __func__, qos->name, qos->description);
+
+		list_append(qos_add_list, qos);
+		rc = db_query_rc(args->errors, args->auth, qos_add_list,
+				 slurmdb_qos_add);
+		/* Freeing qos_add_list won't free qos, to avoid double free */
+		FREE_NULL_LIST(qos_add_list);
+	} else if (list_count(qos_list) > 1) {
+		/* More than one QOS was found with the search criteria */
+		rc = resp_error(args->errors, ESLURM_REST_INVALID_QUERY,
+				"ambiguous modify request",
+				"_foreach_update_qos");
+	} else {
+		/* Exactly one QOS was found; let's update it */
+		slurmdb_qos_rec_t *qos_found = list_peek(qos_list);
+		debug("%s: modifying qos request: id=%u name=%s",
+		      __func__, qos_found->id, qos_found->name);
+		if (qos->name)
+			xassert(!xstrcmp(qos_found->name, qos->name));
+		if (qos->id)
+			xassert(qos_found->id == qos->id);
+
+		rc = db_modify_rc(args->errors, args->auth, &cond, qos,
+				  slurmdb_qos_modify);
+	}
+
+	FREE_NULL_LIST(qos_list);
+	FREE_NULL_LIST(cond.id_list);
+	FREE_NULL_LIST(cond.name_list);
+	slurmdb_destroy_qos_rec(qos);
+
+	return (rc != SLURM_SUCCESS) ? DATA_FOR_EACH_FAIL : DATA_FOR_EACH_CONT;
+}
+
+static int _update_qos(data_t *query, data_t *resp, void *auth, bool commit)
+{
+	int rc = SLURM_SUCCESS;
+	data_t *errors = populate_response_format(resp);
+	foreach_update_qos_t args = {
+		.magic = MAGIC_FOREACH_UP_QOS,
+		.auth = auth,
+		.errors = errors,
+	};
+	slurmdb_qos_cond_t qos_cond = { 0 };
+
+	slurmdb_tres_cond_t tres_cond = {
+		.with_deleted = 1,
+	};
+	data_t *dqos = get_query_key_list("QOS", errors, query);
+
+	if (!dqos) {
+		return ESLURM_REST_INVALID_QUERY;
+	}
+
+	if (!(rc = db_query_list(errors, auth, &args.g_tres_list,
+				 slurmdb_tres_get, &tres_cond)) &&
+	    !(rc = db_query_list(errors, auth, &args.g_qos_list, slurmdb_qos_get,
+				 &qos_cond)) &&
+	    (data_list_for_each(dqos, _foreach_update_qos, &args) < 0))
+		rc = ESLURM_REST_INVALID_QUERY;
+
+	if (!rc && commit)
+		rc = db_query_commit(errors, auth);
+
+	FREE_NULL_LIST(args.g_tres_list);
+
+	return rc;
+}
+
+extern int op_handler_qos(const char *context_id, http_request_method_t method,
+			  data_t *parameters, data_t *query, int tag,
+			  data_t *resp, rest_auth_context_t *auth)
+{
+	int rc = SLURM_SUCCESS;
+	data_t *errors = populate_response_format(resp);
+	List g_qos_list = NULL;
+	char *qos_name = NULL;
+	slurmdb_qos_cond_t qos_cond = { 0 };
+
+
+	if (method == HTTP_REQUEST_GET) {
+		/* Update qos_cond with requested search parameters */
+		if (query && data_get_dict_length(query)) {
+			foreach_query_search_t args = {
+				.errors = errors,
+				.qos_cond = &qos_cond,
+			};
+
+			if (data_dict_for_each(query, _foreach_query_search,
+					       &args) < 0)
+				return ESLURM_REST_INVALID_QUERY;
+		}
+
+		/* need global list of QOS to dump even a single QOS */
+		rc = db_query_list(errors, auth, &g_qos_list, slurmdb_qos_get,
+				   &qos_cond);
+	}
+
+	if (!rc && (tag == TAG_SINGLE_QOS)) {
+		qos_name = get_str_param("qos_name", errors, parameters);
+
+		if (qos_name) {
+			qos_cond.name_list = list_create(NULL);
+			list_append(qos_cond.name_list, qos_name);
+		} else
+			rc = ESLURM_REST_INVALID_QUERY;
+	}
+
+	if (rc)
+		/* no-op */;
+	else if (method == HTTP_REQUEST_GET)
+		rc = _dump_qos(resp, auth, g_qos_list, qos_name);
+	else if (method == HTTP_REQUEST_DELETE && (tag == TAG_SINGLE_QOS))
+		rc = _delete_qos(resp, auth, errors, &qos_cond);
+	else if (method == HTTP_REQUEST_POST &&
+		 ((tag == TAG_ALL_QOS) || (tag == CONFIG_OP_TAG)))
+		rc = _update_qos(query, resp, auth, (tag != CONFIG_OP_TAG));
+	else
+		rc = ESLURM_REST_INVALID_QUERY;
+
+	FREE_NULL_LIST(qos_cond.name_list);
+	FREE_NULL_LIST(g_qos_list);
+
+	return rc;
+}
+
+extern void init_op_qos(void)
+{
+	bind_operation_handler("/slurmdb/v0.0.38/qos/", op_handler_qos,
+			       TAG_ALL_QOS);
+	bind_operation_handler("/slurmdb/v0.0.38/qos/{qos_name}",
+			       op_handler_qos, TAG_SINGLE_QOS);
+}
+
+extern void destroy_op_qos(void)
+{
+	unbind_operation_handler(op_handler_qos);
+}
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/tres.c slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/tres.c
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/tres.c	1970-01-01 01:00:00.000000000 +0100
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/tres.c	2022-11-25 15:06:09.223023551 +0100
@@ -0,0 +1,183 @@
+/*****************************************************************************\
+ *  tres.c - Slurm REST API accounting TRES http operations handlers
+ *****************************************************************************
+ *  Copyright (C) 2020 SchedMD LLC.
+ *  Written by Nathan Rini <nate@schedmd.com>
+ *
+ *  This file is part of Slurm, a resource management program.
+ *  For details, see <https://slurm.schedmd.com/>.
+ *  Please also read the included file: DISCLAIMER.
+ *
+ *  Slurm is free software; you can redistribute it and/or modify it under
+ *  the terms of the GNU General Public License as published by the Free
+ *  Software Foundation; either version 2 of the License, or (at your option)
+ *  any later version.
+ *
+ *  In addition, as a special exception, the copyright holders give permission
+ *  to link the code of portions of this program with the OpenSSL library under
+ *  certain conditions as described in each individual source file, and
+ *  distribute linked combinations including the two. You must obey the GNU
+ *  General Public License in all respects for all of the code used other than
+ *  OpenSSL. If you modify file(s) with this exception, you may extend this
+ *  exception to your version of the file(s), but you are not obligated to do
+ *  so. If you do not wish to do so, delete this exception statement from your
+ *  version.  If you delete this exception statement from all source files in
+ *  the program, then also delete it here.
+ *
+ *  Slurm is distributed in the hope that it will be useful, but WITHOUT ANY
+ *  WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+ *  FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
+ *  details.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with Slurm; if not, write to the Free Software Foundation, Inc.,
+ *  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301  USA.
+\*****************************************************************************/
+
+#include "config.h"
+
+#include <stdint.h>
+
+#include "slurm/slurm.h"
+#include "slurm/slurmdb.h"
+
+#include "src/common/list.h"
+#include "src/common/log.h"
+#include "src/common/parse_time.h"
+#include "src/common/ref.h"
+#include "src/common/slurm_protocol_api.h"
+#include "src/common/slurmdbd_defs.h"
+#include "src/common/strlcpy.h"
+#include "src/common/uid.h"
+#include "src/common/xassert.h"
+#include "src/common/xmalloc.h"
+#include "src/common/xstring.h"
+
+#include "src/slurmrestd/operations.h"
+
+#include "src/slurmrestd/plugins/openapi/dbv0.0.38/api.h"
+
+static int _foreach_dump_tres(void *x, void *arg)
+{
+	slurmdb_tres_rec_t *t = (slurmdb_tres_rec_t *)x;
+	parser_env_t penv = { 0 };
+
+	if (dump(PARSE_TRES, t, data_set_dict(data_list_append(arg)), &penv))
+		return -1;
+
+	return 0;
+}
+
+static int _dump_tres(data_t *resp, rest_auth_context_t *auth)
+{
+	data_t *errors = populate_response_format(resp);
+	int rc = SLURM_SUCCESS;
+	List tres_list = NULL;
+	slurmdb_tres_cond_t tres_cond = {
+		.with_deleted = 1,
+	};
+
+	if (!(rc = db_query_list(errors, auth, &tres_list, slurmdb_tres_get,
+				 &tres_cond)) &&
+	    (list_for_each(tres_list, _foreach_dump_tres,
+			   data_set_list(data_key_set(resp, "TRES"))) < 0))
+		rc = ESLURM_DATA_CONV_FAILED;
+
+	FREE_NULL_LIST(tres_list);
+
+	return SLURM_SUCCESS;
+}
+
+#define MAGIC_FOREACH_TRES 0xdeed1a11
+typedef struct {
+	int magic;
+	List tres_list;
+	data_t *errors;
+} foreach_tres_t;
+
+static data_for_each_cmd_t _foreach_tres(data_t *data, void *arg)
+{
+	foreach_tres_t *args = arg;
+	data_t *errors = args->errors;
+	parser_env_t penv = { 0 };
+	slurmdb_tres_rec_t *tres;
+
+	xassert(args->magic == MAGIC_FOREACH_TRES);
+
+	if (data_get_type(data) != DATA_TYPE_DICT) {
+		resp_error(errors, ESLURM_NOT_SUPPORTED,
+			   "each TRES entry must be a dictionary", "TRES");
+		return DATA_FOR_EACH_FAIL;
+	}
+
+	tres = xmalloc(sizeof(slurmdb_tres_rec_t));
+	list_append(args->tres_list, tres);
+
+	if (parse(PARSE_TRES, tres, data, args->errors, &penv))
+		return DATA_FOR_EACH_FAIL;
+
+	return DATA_FOR_EACH_CONT;
+}
+
+static int _update_tres(data_t *query, data_t *resp, void *auth,
+			bool commit)
+{
+	data_t *dtres = NULL;
+	int rc = SLURM_SUCCESS;
+	data_t *errors = populate_response_format(resp);
+	List tres_list = list_create(slurmdb_destroy_tres_rec);
+	foreach_tres_t args = {
+		.magic = MAGIC_FOREACH_TRES,
+		.tres_list = tres_list,
+		.errors = errors,
+	};
+
+#ifdef NDEBUG
+	/*
+	 * Updating TRES is not currently supported and is disabled
+	 * except for developer testing
+	 */
+	if (!commit)
+		return SLURM_SUCCESS;
+	else
+		return resp_error(errors, ESLURM_NOT_SUPPORTED,
+				  "Updating TRES is not currently supported.",
+				  NULL);
+#endif /*!NDEBUG*/
+
+	if (!(dtres = get_query_key_list("TRES", errors, query)))
+		rc = ESLURM_REST_INVALID_QUERY;
+
+	if (!rc && (data_list_for_each(dtres, _foreach_tres, &args) < 0))
+		rc = ESLURM_REST_INVALID_QUERY;
+
+	if (!(rc = db_query_rc(errors, auth, tres_list, slurmdb_tres_add)) &&
+	     commit)
+		db_query_commit(errors, auth);
+
+	FREE_NULL_LIST(tres_list);
+
+	return SLURM_SUCCESS;
+}
+
+extern int op_handler_tres(const char *context_id, http_request_method_t method,
+			   data_t *parameters, data_t *query, int tag,
+			   data_t *resp, rest_auth_context_t *auth)
+{
+	if (method == HTTP_REQUEST_GET)
+		return _dump_tres(resp, auth);
+	else if (method == HTTP_REQUEST_POST)
+		return _update_tres(query, resp, auth, (tag != CONFIG_OP_TAG));
+	else
+		return ESLURM_REST_INVALID_QUERY;
+}
+
+extern void init_op_tres(void)
+{
+	bind_operation_handler("/slurmdb/v0.0.38/tres/", op_handler_tres, 0);
+}
+
+extern void destroy_op_tres(void)
+{
+	unbind_operation_handler(op_handler_tres);
+}
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/users.c slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/users.c
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/users.c	1970-01-01 01:00:00.000000000 +0100
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/users.c	2022-11-25 15:06:09.223023551 +0100
@@ -0,0 +1,470 @@
+/*****************************************************************************\
+ *  users.c - Slurm REST API acct user http operations handlers
+ *****************************************************************************
+ *  Copyright (C) 2020 SchedMD LLC.
+ *  Written by Nathan Rini <nate@schedmd.com>
+ *
+ *  This file is part of Slurm, a resource management program.
+ *  For details, see <https://slurm.schedmd.com/>.
+ *  Please also read the included file: DISCLAIMER.
+ *
+ *  Slurm is free software; you can redistribute it and/or modify it under
+ *  the terms of the GNU General Public License as published by the Free
+ *  Software Foundation; either version 2 of the License, or (at your option)
+ *  any later version.
+ *
+ *  In addition, as a special exception, the copyright holders give permission
+ *  to link the code of portions of this program with the OpenSSL library under
+ *  certain conditions as described in each individual source file, and
+ *  distribute linked combinations including the two. You must obey the GNU
+ *  General Public License in all respects for all of the code used other than
+ *  OpenSSL. If you modify file(s) with this exception, you may extend this
+ *  exception to your version of the file(s), but you are not obligated to do
+ *  so. If you do not wish to do so, delete this exception statement from your
+ *  version.  If you delete this exception statement from all source files in
+ *  the program, then also delete it here.
+ *
+ *  Slurm is distributed in the hope that it will be useful, but WITHOUT ANY
+ *  WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+ *  FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
+ *  details.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with Slurm; if not, write to the Free Software Foundation, Inc.,
+ *  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301  USA.
+\*****************************************************************************/
+
+#include "config.h"
+
+#include <stdint.h>
+
+#include "slurm/slurm.h"
+#include "slurm/slurmdb.h"
+
+#include "src/common/list.h"
+#include "src/common/log.h"
+#include "src/common/parse_time.h"
+#include "src/common/ref.h"
+#include "src/common/slurm_protocol_api.h"
+#include "src/common/slurmdbd_defs.h"
+#include "src/common/strlcpy.h"
+#include "src/common/uid.h"
+#include "src/common/xassert.h"
+#include "src/common/xmalloc.h"
+#include "src/common/xstring.h"
+
+#include "src/slurmrestd/operations.h"
+
+#include "src/slurmrestd/plugins/openapi/dbv0.0.38/api.h"
+
+#define MAGIC_FOREACH_USER 0xa13efef2
+typedef struct {
+	int magic;
+	data_t *users;
+	List tres_list;
+	List qos_list;
+} foreach_user_t;
+
+typedef struct {
+	data_t *errors;
+	slurmdb_user_cond_t *user_cond;
+} foreach_query_search_t;
+
+static int _foreach_user(void *x, void *arg)
+{
+	slurmdb_user_rec_t *user = x;
+	foreach_user_t *args = arg;
+	parser_env_t penv = {
+		.g_tres_list = args->tres_list,
+		.g_qos_list = args->qos_list,
+	};
+
+	xassert(args->magic == MAGIC_FOREACH_USER);
+	xassert(user);
+
+	if (dump(PARSE_USER, user, data_set_dict(data_list_append(args->users)),
+		 &penv))
+		return -1;
+	else
+		return 0;
+}
+
+static data_for_each_cmd_t _foreach_query_search(const char *key,
+						 data_t *data,
+						 void *arg)
+{
+	foreach_query_search_t *args = arg;
+	data_t *errors = args->errors;
+
+	if (!xstrcasecmp("with_deleted", key)) {
+		if (data_convert_type(data, DATA_TYPE_BOOL) != DATA_TYPE_BOOL) {
+			resp_error(errors, ESLURM_REST_INVALID_QUERY,
+				   "must be a Boolean", NULL);
+			return DATA_FOR_EACH_FAIL;
+		}
+
+		if (data->data.bool_u)
+			args->user_cond->with_deleted = true;
+		else
+			args->user_cond->with_deleted = false;
+
+		return DATA_FOR_EACH_CONT;
+	}
+
+	resp_error(errors, ESLURM_REST_INVALID_QUERY, "Unknown query field",
+		   NULL);
+	return DATA_FOR_EACH_FAIL;
+}
+
+static int _dump_users(data_t *resp, data_t *errors, void *auth, char *user_name,
+		       slurmdb_user_cond_t *user_cond)
+{
+	int rc = SLURM_SUCCESS;
+	List user_list = NULL;
+	slurmdb_qos_cond_t qos_cond = {
+		.with_deleted = 1,
+	};
+	slurmdb_tres_cond_t tres_cond = {
+		.with_deleted = 1,
+	};
+	foreach_user_t args = {
+		.magic = MAGIC_FOREACH_USER,
+		.users = data_set_list(data_key_set(resp, "users")),
+	};
+	slurmdb_assoc_cond_t assoc_cond = { 0 };
+
+	user_cond->assoc_cond = &assoc_cond;
+	user_cond->with_assocs = true;
+	user_cond->with_coords = true;
+	/* with_deleted defaults to false */
+	user_cond->with_wckeys = true;
+
+	if (user_name) {
+		assoc_cond.user_list = list_create(NULL);
+		list_append(assoc_cond.user_list, user_name);
+	}
+
+	if (!(rc = db_query_list(errors, auth, &user_list, slurmdb_users_get,
+				 user_cond)) &&
+	    !(rc = db_query_list(errors, auth, &args.tres_list,
+				 slurmdb_tres_get, &tres_cond)) &&
+	    !(rc = db_query_list(errors, auth, &args.qos_list, slurmdb_qos_get,
+				 &qos_cond)) &&
+	    (list_for_each(user_list, _foreach_user, &args) < 0))
+		resp_error(errors, ESLURM_DATA_CONV_FAILED, NULL,
+			   "_foreach_user");
+
+	FREE_NULL_LIST(args.tres_list);
+	FREE_NULL_LIST(args.qos_list);
+	FREE_NULL_LIST(user_list);
+	FREE_NULL_LIST(assoc_cond.user_list);
+
+	return rc;
+}
+
+#define MAGIC_USER_COORD 0x8e8dbee1
+typedef struct {
+	int magic;
+	List acct_list; /* list of char *'s of names of accounts */
+	slurmdb_user_cond_t user_cond;
+	slurmdb_assoc_cond_t assoc_cond;
+} add_user_coord_t;
+
+#define MAGIC_FOREACH_UP_USER 0xdbed1a12
+typedef struct {
+	int magic;
+	List user_list;
+	data_t *errors;
+	rest_auth_context_t *auth;
+} foreach_update_user_t;
+
+static data_for_each_cmd_t _foreach_update_user(data_t *data, void *arg)
+{
+	foreach_update_user_t *args = arg;
+	data_t *errors = args->errors;
+	slurmdb_user_rec_t *user;
+	parser_env_t penv = {
+		.auth = args->auth,
+	};
+
+	xassert(args->magic == MAGIC_FOREACH_UP_USER);
+
+	if (data_get_type(data) != DATA_TYPE_DICT) {
+		resp_error(errors, ESLURM_NOT_SUPPORTED,
+			   "each user entry must be a dictionary", NULL);
+		return DATA_FOR_EACH_FAIL;
+	}
+
+	user = xmalloc(sizeof(slurmdb_user_rec_t));
+	user->assoc_list = list_create(slurmdb_destroy_assoc_rec);
+	user->coord_accts = list_create(slurmdb_destroy_coord_rec);
+
+	if (parse(PARSE_USER, user, data, args->errors, &penv)) {
+		slurmdb_destroy_user_rec(user);
+		return DATA_FOR_EACH_FAIL;
+	} else {
+		(void)list_append(args->user_list, user);
+		return DATA_FOR_EACH_CONT;
+	}
+}
+
+#define MAGIC_USER_COORD_SPLIT_COORD 0x8e8dbee3
+typedef struct {
+	int magic;
+	add_user_coord_t *uc;
+} _foreach_user_coord_split_coord_t;
+
+static int _foreach_user_coord_split_coord(void *x, void *arg)
+{
+	slurmdb_coord_rec_t *coord = x;
+	_foreach_user_coord_split_coord_t *args = arg;
+
+	xassert(args->magic == MAGIC_USER_COORD_SPLIT_COORD);
+	xassert(args->uc->magic == MAGIC_USER_COORD);
+
+	if (coord->direct)
+		list_append(args->uc->acct_list, xstrdup(coord->name));
+
+	return 0;
+}
+
+#define MAGIC_USER_COORD_SPLIT 0x8e8dbee2
+typedef struct {
+	int magic;
+	List list_coords; /* list of add_user_coord_t */
+} _foreach_user_coord_split_t;
+
+static int _foreach_user_coord_split(void *x, void *arg)
+{
+	slurmdb_user_rec_t *user = x;
+	_foreach_user_coord_split_t *args = arg;
+	add_user_coord_t *uc = NULL;
+	_foreach_user_coord_split_coord_t c_args = {
+		.magic = MAGIC_USER_COORD_SPLIT_COORD,
+	};
+
+	xassert(args->magic == MAGIC_USER_COORD_SPLIT);
+
+	if (!user->coord_accts || list_is_empty(user->coord_accts))
+		/* nothing to do here */
+		return 0;
+
+	c_args.uc = uc = xmalloc(sizeof(*uc));
+	uc->magic = MAGIC_USER_COORD;
+	uc->acct_list = list_create(xfree_ptr);
+	uc->user_cond.assoc_cond = &uc->assoc_cond;
+	uc->assoc_cond.user_list = list_create(xfree_ptr);
+	list_append(uc->assoc_cond.user_list, xstrdup(user->name));
+
+	if (list_for_each(user->coord_accts, _foreach_user_coord_split_coord,
+			  &c_args) < 0)
+		return -1;
+
+	(void)list_append(args->list_coords, uc);
+
+	return 1;
+}
+
+#define MAGIC_USER_COORD_ADD 0x8e8ffee2
+typedef struct {
+	int magic;
+	rest_auth_context_t *auth;
+	int rc;
+	data_t *errors;
+} _foreach_user_coord_add_t;
+
+static int _foreach_user_coord_add(void *x, void *arg)
+{
+	int rc = SLURM_SUCCESS;
+	add_user_coord_t *uc = x;
+	_foreach_user_coord_add_t *args = arg;
+
+	xassert(uc->magic == MAGIC_USER_COORD);
+	xassert(args->magic == MAGIC_USER_COORD_ADD);
+
+	if ((args->rc = slurmdb_coord_add(rest_auth_g_get_db_conn(args->auth),
+					  uc->acct_list, &uc->user_cond)))
+		rc = resp_error(args->errors, args->rc, NULL,
+				"slurmdb_coord_add");
+
+	return (rc ? -1 : 0);
+}
+
+static void _destroy_user_coord_t(void *x)
+{
+	add_user_coord_t *uc = x;
+	xassert(uc->magic == MAGIC_USER_COORD);
+
+	FREE_NULL_LIST(uc->acct_list);
+	FREE_NULL_LIST(uc->assoc_cond.user_list);
+
+	xfree(uc);
+}
+
+static int _update_users(data_t *query, data_t *resp, void *auth,
+			 bool commit)
+{
+	int rc = SLURM_SUCCESS;
+	data_t *errors = populate_response_format(resp);
+	foreach_update_user_t args = {
+		.magic = MAGIC_FOREACH_UP_USER,
+		.auth = auth,
+		.errors = errors,
+		.user_list = list_create(slurmdb_destroy_user_rec),
+	};
+	_foreach_user_coord_split_t c_args = {
+		.magic = MAGIC_USER_COORD_SPLIT,
+		.list_coords = list_create(_destroy_user_coord_t),
+	};
+	_foreach_user_coord_add_t add_args = {
+		.magic = MAGIC_USER_COORD_ADD,
+		.auth = auth,
+		.errors = errors,
+	};
+	data_t *dusers = get_query_key_list("users", errors, query);
+
+	if (!dusers)
+		rc = ESLURM_REST_INVALID_QUERY;
+	else if (data_list_for_each(dusers, _foreach_update_user, &args) < 0)
+		rc = ESLURM_REST_INVALID_QUERY;
+	/* split out the coordinators until after the users are done */
+	else if (list_for_each(args.user_list, _foreach_user_coord_split,
+			       &c_args) < 0)
+		rc = ESLURM_REST_INVALID_QUERY;
+
+	if (!rc && !(rc = db_query_rc(errors, auth, args.user_list,
+				      slurmdb_users_add))) {
+		(void)list_for_each(c_args.list_coords, _foreach_user_coord_add,
+				    &add_args);
+		rc = add_args.rc;
+	}
+
+	if (!rc && commit)
+		db_query_commit(errors, auth);
+
+	FREE_NULL_LIST(args.user_list);
+	FREE_NULL_LIST(c_args.list_coords);
+
+	return rc;
+}
+
+static int _foreach_delete_user(void *x, void *arg)
+{
+	char *user = x;
+	data_t *users = arg;
+
+	data_set_string(data_list_append(users), user);
+
+	return DATA_FOR_EACH_CONT;
+}
+
+static int _delete_user(data_t *resp, void *auth,
+			char *user_name, data_t *errors)
+{
+	int rc = SLURM_SUCCESS;
+	slurmdb_assoc_cond_t assoc_cond = { .user_list = list_create(NULL) };
+	slurmdb_user_cond_t user_cond = {
+		.assoc_cond = &assoc_cond,
+		.with_assocs = true,
+		.with_coords = true,
+		.with_deleted = false,
+		.with_wckeys = true,
+	};
+	List user_list = NULL;
+
+	list_append(assoc_cond.user_list, user_name);
+
+	if (!(rc = db_query_list(errors, auth, &user_list, slurmdb_users_remove,
+				 &user_cond)) &&
+	    (list_for_each(user_list, _foreach_delete_user,
+			   data_set_list(data_key_set(resp, "removed_users"))) <
+	     0))
+		rc = resp_error(errors, ESLURM_REST_INVALID_QUERY,
+				"_foreach_delete_user unexpectedly failed",
+				NULL);
+
+	if (!rc)
+		rc = db_query_commit(errors, auth);
+
+	FREE_NULL_LIST(user_list);
+	FREE_NULL_LIST(assoc_cond.user_list);
+
+	return rc;
+}
+
+/* based on sacctmgr_list_user() */
+extern int op_handler_users(const char *context_id,
+			    http_request_method_t method,
+			    data_t *parameters, data_t *query, int tag,
+			    data_t *resp, rest_auth_context_t *auth)
+{
+	data_t *errors = populate_response_format(resp);
+
+	if (method == HTTP_REQUEST_GET) {
+		slurmdb_user_cond_t user_cond = {0};
+		if (query && data_get_dict_length(query)) {
+			/* Default to no deleted users */
+			foreach_query_search_t args = {
+				.errors = errors,
+				.user_cond = &user_cond,
+			};
+
+			if (data_dict_for_each(query, _foreach_query_search,
+					       &args) < 0)
+				return ESLURM_REST_INVALID_QUERY;
+		}
+
+		return _dump_users(resp, errors, auth, NULL, &user_cond);
+	} else if (method == HTTP_REQUEST_POST) {
+		return _update_users(query, resp, auth, (tag != CONFIG_OP_TAG));
+	} else {
+		return ESLURM_REST_INVALID_QUERY;
+	}
+}
+
+static int op_handler_user(const char *context_id, http_request_method_t method,
+			   data_t *parameters, data_t *query, int tag,
+			   data_t *resp, rest_auth_context_t *auth)
+{
+	int rc = SLURM_SUCCESS;
+	data_t *errors = populate_response_format(resp);
+	char *user_name = get_str_param("user_name", errors, parameters);
+
+	if (!user_name) {
+		rc = ESLURM_REST_INVALID_QUERY;
+	} else if (method == HTTP_REQUEST_GET) {
+		slurmdb_user_cond_t user_cond = {0};
+		if (query && data_get_dict_length(query)) {
+			/* Default to no deleted users */
+			foreach_query_search_t args = {
+				.errors = errors,
+				.user_cond = &user_cond,
+			};
+
+			if (data_dict_for_each(query, _foreach_query_search,
+					       &args) < 0)
+				return ESLURM_REST_INVALID_QUERY;
+		}
+
+		rc = _dump_users(resp, errors, auth, user_name, &user_cond);
+	} else if (method == HTTP_REQUEST_DELETE) {
+		rc = _delete_user(resp, auth, user_name, errors);
+	} else {
+		rc = ESLURM_REST_INVALID_QUERY;
+	}
+
+	return rc;
+}
+
+extern void init_op_users(void)
+{
+	bind_operation_handler("/slurmdb/v0.0.38/users/", op_handler_users, 0);
+	bind_operation_handler("/slurmdb/v0.0.38/user/{user_name}",
+			       op_handler_user, 0);
+}
+
+extern void destroy_op_users(void)
+{
+	unbind_operation_handler(op_handler_users);
+	unbind_operation_handler(op_handler_user);
+}
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/wckeys.c slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/wckeys.c
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/dbv0.0.38/wckeys.c	1970-01-01 01:00:00.000000000 +0100
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/dbv0.0.38/wckeys.c	2022-11-25 15:06:09.223023551 +0100
@@ -0,0 +1,277 @@
+/*****************************************************************************\
+ *  wckeys.c - Slurm REST API acct wckey http operations handlers
+ *****************************************************************************
+ *  Copyright (C) 2020 SchedMD LLC.
+ *  Written by Nathan Rini <nate@schedmd.com>
+ *
+ *  This file is part of Slurm, a resource management program.
+ *  For details, see <https://slurm.schedmd.com/>.
+ *  Please also read the included file: DISCLAIMER.
+ *
+ *  Slurm is free software; you can redistribute it and/or modify it under
+ *  the terms of the GNU General Public License as published by the Free
+ *  Software Foundation; either version 2 of the License, or (at your option)
+ *  any later version.
+ *
+ *  In addition, as a special exception, the copyright holders give permission
+ *  to link the code of portions of this program with the OpenSSL library under
+ *  certain conditions as described in each individual source file, and
+ *  distribute linked combinations including the two. You must obey the GNU
+ *  General Public License in all respects for all of the code used other than
+ *  OpenSSL. If you modify file(s) with this exception, you may extend this
+ *  exception to your version of the file(s), but you are not obligated to do
+ *  so. If you do not wish to do so, delete this exception statement from your
+ *  version.  If you delete this exception statement from all source files in
+ *  the program, then also delete it here.
+ *
+ *  Slurm is distributed in the hope that it will be useful, but WITHOUT ANY
+ *  WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+ *  FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
+ *  details.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with Slurm; if not, write to the Free Software Foundation, Inc.,
+ *  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301  USA.
+\*****************************************************************************/
+
+#include "config.h"
+
+#include <stdint.h>
+
+#include "slurm/slurm.h"
+#include "slurm/slurmdb.h"
+
+#include "src/common/list.h"
+#include "src/common/log.h"
+#include "src/common/parse_time.h"
+#include "src/common/ref.h"
+#include "src/common/slurm_protocol_api.h"
+#include "src/common/slurmdbd_defs.h"
+#include "src/common/strlcpy.h"
+#include "src/common/uid.h"
+#include "src/common/xassert.h"
+#include "src/common/xmalloc.h"
+#include "src/common/xstring.h"
+
+#include "src/slurmrestd/operations.h"
+
+#include "src/slurmrestd/plugins/openapi/dbv0.0.38/api.h"
+
+#define MAGIC_FOREACH_WCKEY 0xb3a2faf2
+typedef struct {
+	int magic;
+	data_t *wckeys;
+} foreach_wckey_t;
+
+static int _foreach_wckey(void *x, void *arg)
+{
+	slurmdb_wckey_rec_t *wckey = x;
+	foreach_wckey_t *args = arg;
+	parser_env_t penv = { 0 };
+
+	xassert(args->magic == MAGIC_FOREACH_WCKEY);
+
+	if (dump(PARSE_WCKEY, wckey,
+		 data_set_dict(data_list_append(args->wckeys)), &penv))
+		return -1;
+
+	return 1;
+}
+
+static int _dump_wckeys(data_t *resp, data_t *errors, char *wckey,
+			void *auth)
+{
+	int rc = SLURM_SUCCESS;
+	slurmdb_wckey_cond_t wckey_cond = {
+		.with_deleted = true,
+	};
+	foreach_wckey_t args = {
+		.magic = MAGIC_FOREACH_WCKEY,
+		.wckeys = data_set_list(data_key_set(resp, "wckeys")),
+	};
+	List wckey_list = NULL;
+
+	if (wckey) {
+		wckey_cond.name_list = list_create(NULL);
+		list_append(wckey_cond.name_list, wckey);
+	}
+
+	if (!(rc = db_query_list(errors, auth, &wckey_list, slurmdb_wckeys_get,
+				 &wckey_cond)) &&
+	    (list_for_each(wckey_list, _foreach_wckey, &args) < 0))
+		rc = ESLURM_DATA_CONV_FAILED;
+
+	FREE_NULL_LIST(wckey_list);
+	FREE_NULL_LIST(wckey_cond.name_list);
+
+	return rc;
+}
+
+#define MAGIC_FOREACH_DEL_WCKEY 0xb3a2faf1
+typedef struct {
+	int magic;
+	data_t *wckeys;
+} foreach_del_wckey_t;
+
+static int _foreach_del_wckey(void *x, void *arg)
+{
+	char *wckey = x;
+	foreach_del_wckey_t *args = arg;
+
+	data_set_string(data_list_append(args->wckeys), wckey);
+	return 1;
+}
+
+static int _delete_wckey(data_t *resp, data_t *errors, char *wckey,
+			 void *auth)
+{
+	int rc = SLURM_SUCCESS;
+	slurmdb_wckey_cond_t wckey_cond = {
+		.with_deleted = true,
+		.name_list = list_create(NULL),
+	};
+	foreach_del_wckey_t args = {
+		.magic = MAGIC_FOREACH_DEL_WCKEY,
+		.wckeys = data_set_list(data_key_set(resp, "deleted_wckeys")),
+	};
+	List wckey_list = NULL;
+
+	if (!wckey) {
+		rc = ESLURM_REST_EMPTY_RESULT;
+		goto cleanup;
+	}
+
+	list_append(wckey_cond.name_list, wckey);
+
+	if (!(rc = db_query_list(errors, auth, &wckey_list,
+				 slurmdb_wckeys_remove, &wckey_cond)))
+		rc = db_query_commit(errors, auth);
+
+	if (!rc && (list_for_each(wckey_list, _foreach_del_wckey, &args) < 0))
+		rc = ESLURM_DATA_CONV_FAILED;
+cleanup:
+	FREE_NULL_LIST(wckey_list);
+	FREE_NULL_LIST(wckey_cond.name_list);
+
+	return rc;
+}
+
+#define MAGIC_FOREACH_UP_WCKEY 0xdabd1019
+typedef struct {
+	int magic;
+	List wckey_list;
+	data_t *errors;
+	rest_auth_context_t *auth;
+} foreach_update_wckey_t;
+
+static data_for_each_cmd_t _foreach_update_wckey(data_t *data, void *arg)
+{
+	foreach_update_wckey_t *args = arg;
+	slurmdb_wckey_rec_t *wckey;
+	parser_env_t penv = {
+		.auth = args->auth,
+	};
+
+	xassert(args->magic == MAGIC_FOREACH_UP_WCKEY);
+
+	if (data_get_type(data) != DATA_TYPE_DICT) {
+		data_t *e = data_set_dict(data_list_append(args->errors));
+		data_set_string(data_key_set(e, "field"), "wckey");
+		data_set_string(data_key_set(e, "error"),
+				"each wckey entry must be a dictionary");
+		return DATA_FOR_EACH_FAIL;
+	}
+
+	wckey = xmalloc(sizeof(slurmdb_wckey_rec_t));
+	slurmdb_init_wckey_rec(wckey, false);
+
+	wckey->accounting_list = list_create(slurmdb_destroy_account_rec);
+	(void)list_append(args->wckey_list, wckey);
+
+	if (parse(PARSE_WCKEY, wckey, data, args->errors, &penv))
+		return DATA_FOR_EACH_FAIL;
+
+	return DATA_FOR_EACH_CONT;
+}
+
+static int _update_wckeys(data_t *query, data_t *resp, data_t *errors,
+			  void *auth, bool commit)
+{
+	int rc = SLURM_SUCCESS;
+	foreach_update_wckey_t args = {
+		.magic = MAGIC_FOREACH_UP_WCKEY,
+		.auth = auth,
+		.errors = errors,
+		.wckey_list = list_create(slurmdb_destroy_wckey_rec),
+	};
+	data_t *dwckeys = get_query_key_list("wckeys", errors, query);
+
+	if (!dwckeys)
+		rc = ESLURM_REST_INVALID_QUERY;
+	else if (data_list_for_each(dwckeys, _foreach_update_wckey, &args) < 0)
+		rc = ESLURM_REST_INVALID_QUERY;
+
+	if (!rc &&
+	    !(rc = db_query_rc(errors, auth, args.wckey_list,
+			       slurmdb_wckeys_add)) &&
+	    commit)
+		rc = db_query_commit(errors, auth);
+
+	FREE_NULL_LIST(args.wckey_list);
+
+	return rc;
+}
+
+extern int op_handler_wckey(const char *context_id,
+			    http_request_method_t method,
+			    data_t *parameters, data_t *query, int tag,
+			    data_t *resp, rest_auth_context_t *auth)
+{
+	int rc = SLURM_SUCCESS;
+	data_t *errors = populate_response_format(resp);
+	char *wckey = get_str_param("wckey", errors, parameters);
+
+	if (!wckey)
+		rc = ESLURM_REST_INVALID_QUERY;
+	else if (method == HTTP_REQUEST_GET)
+		rc = _dump_wckeys(resp, errors, wckey, auth);
+	else if (!rc && (method == HTTP_REQUEST_DELETE))
+		rc = _delete_wckey(resp, errors, wckey, auth);
+	else
+		rc = ESLURM_REST_INVALID_QUERY;
+
+	return rc;
+}
+
+extern int op_handler_wckeys(const char *context_id,
+			     http_request_method_t method, data_t *parameters,
+			     data_t *query, int tag, data_t *resp,
+			     rest_auth_context_t *auth)
+{
+	data_t *errors = populate_response_format(resp);
+	int rc = SLURM_SUCCESS;
+
+	if (method == HTTP_REQUEST_GET)
+		rc = _dump_wckeys(resp, errors, NULL, auth);
+	else if (method == HTTP_REQUEST_POST)
+		rc = _update_wckeys(query, resp, errors, auth,
+				    (tag != CONFIG_OP_TAG));
+	else
+		rc = ESLURM_REST_INVALID_QUERY;
+
+	return rc;
+}
+
+extern void init_op_wckeys(void)
+{
+	bind_operation_handler("/slurmdb/v0.0.38/wckeys/", op_handler_wckeys,
+			       0);
+	bind_operation_handler("/slurmdb/v0.0.38/wckey/{wckey}",
+			       op_handler_wckey, 0);
+}
+
+extern void destroy_op_wckeys(void)
+{
+	unbind_operation_handler(op_handler_wckeys);
+	unbind_operation_handler(op_handler_wckey);
+}
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/Makefile.am slurm-20.11.9/src/slurmrestd/plugins/openapi/Makefile.am
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/Makefile.am	2022-05-04 21:32:38.000000000 +0200
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/Makefile.am	2022-11-22 13:34:35.694876616 +0100
@@ -1 +1 @@
-SUBDIRS = v0.0.35 v0.0.36 dbv0.0.36
+SUBDIRS = v0.0.35 v0.0.36 dbv0.0.36 v0.0.38 dbv0.0.38
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/v0.0.36/api.c slurm-20.11.9/src/slurmrestd/plugins/openapi/v0.0.36/api.c
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/v0.0.36/api.c	2022-05-04 21:32:38.000000000 +0200
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/v0.0.36/api.c	2022-11-22 14:55:30.160779265 +0100
@@ -36,6 +36,7 @@
 
 #include "config.h"
 
+#include <stdarg.h>
 #include <unistd.h>
 
 #include "slurm/slurm.h"
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/v0.0.38/api.c slurm-20.11.9/src/slurmrestd/plugins/openapi/v0.0.38/api.c
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/v0.0.38/api.c	1970-01-01 01:00:00.000000000 +0100
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/v0.0.38/api.c	2023-01-17 15:43:02.545364937 +0100
@@ -0,0 +1,197 @@
+/*****************************************************************************\
+ *  api.c - Slurm REST API openapi operations handlers
+ *****************************************************************************
+ *  Copyright (C) 2019-2020 SchedMD LLC.
+ *  Written by Nathan Rini <nate@schedmd.com>
+ *
+ *  This file is part of Slurm, a resource management program.
+ *  For details, see <https://slurm.schedmd.com/>.
+ *  Please also read the included file: DISCLAIMER.
+ *
+ *  Slurm is free software; you can redistribute it and/or modify it under
+ *  the terms of the GNU General Public License as published by the Free
+ *  Software Foundation; either version 2 of the License, or (at your option)
+ *  any later version.
+ *
+ *  In addition, as a special exception, the copyright holders give permission
+ *  to link the code of portions of this program with the OpenSSL library under
+ *  certain conditions as described in each individual source file, and
+ *  distribute linked combinations including the two. You must obey the GNU
+ *  General Public License in all respects for all of the code used other than
+ *  OpenSSL. If you modify file(s) with this exception, you may extend this
+ *  exception to your version of the file(s), but you are not obligated to do
+ *  so. If you do not wish to do so, delete this exception statement from your
+ *  version.  If you delete this exception statement from all source files in
+ *  the program, then also delete it here.
+ *
+ *  Slurm is distributed in the hope that it will be useful, but WITHOUT ANY
+ *  WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+ *  FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
+ *  details.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with Slurm; if not, write to the Free Software Foundation, Inc.,
+ *  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301  USA.
+\*****************************************************************************/
+
+#include "config.h"
+
+#include <stdarg.h>
+#include <unistd.h>
+
+#include "slurm/slurm.h"
+
+#include "src/common/data.h"
+#include "src/common/log.h"
+#include "src/common/ref.h"
+#include "src/common/xassert.h"
+#include "src/common/xmalloc.h"
+#include "src/common/xstring.h"
+
+#include "src/slurmrestd/operations.h"
+#include "src/slurmrestd/openapi.h"
+#include "src/slurmrestd/xjson.h"
+
+#include "src/slurmrestd/plugins/openapi/v0.0.38/api.h"
+
+/*
+ * These variables are required by the generic plugin interface.  If they
+ * are not found in the plugin, the plugin loader will ignore it.
+ *
+ * plugin_name - a string giving a human-readable description of the
+ * plugin.  There is no maximum length, but the symbol must refer to
+ * a valid string.
+ *
+ * plugin_type - a string suggesting the type of the plugin or its
+ * applicability to a particular form of data or method of data handling.
+ * If the low-level plugin API is used, the contents of this string are
+ * unimportant and may be anything.  Slurm uses the higher-level plugin
+ * interface which requires this string to be of the form
+ *
+ *	<application>/<method>
+ *
+ * where <application> is a description of the intended application of
+ * the plugin (e.g., "select" for Slurm node selection) and <method>
+ * is a description of how this plugin satisfies that application.  Slurm will
+ * only load select plugins if the plugin_type string has a
+ * prefix of "select/".
+ *
+ * plugin_version - an unsigned 32-bit integer containing the Slurm version
+ * (major.minor.micro combined into a single number).
+ */
+const char plugin_name[] = "Slurm OpenAPI v0.0.38";
+const char plugin_type[] = "openapi/v0.0.38";
+const uint32_t plugin_id = 100;
+const uint32_t plugin_version = SLURM_VERSION_NUMBER;
+
+decl_static_data(openapi_json);
+
+extern int get_date_param(data_t *query, const char *param, time_t *time) {
+	data_t *data_update_time;
+	if ((data_update_time = data_key_get(query, param))) {
+		if (data_convert_type(data_update_time, DATA_TYPE_INT_64) ==
+		    DATA_TYPE_INT_64)
+			*time = data_get_int(data_update_time);
+		else
+			return ESLURM_REST_INVALID_QUERY;
+	}
+	return SLURM_SUCCESS;
+}
+
+extern data_t *populate_response_format(data_t *resp)
+{
+	data_t *plugin, *slurm, *slurmv, *meta;
+
+	if (data_get_type(resp) != DATA_TYPE_NULL) {
+		xassert(data_get_type(resp) == DATA_TYPE_DICT);
+		return data_key_get(resp, "errors");
+	}
+
+	data_set_dict(resp);
+
+	meta = data_set_dict(data_key_set(resp, "meta"));
+	plugin = data_set_dict(data_key_set(meta, "plugin"));
+	slurm = data_set_dict(data_key_set(meta, "Slurm"));
+	slurmv = data_set_dict(data_key_set(slurm, "version"));
+
+	data_set_string(data_key_set(slurm, "release"), SLURM_VERSION_STRING);
+	(void) data_convert_type(data_set_string(data_key_set(slurmv, "major"),
+						 SLURM_MAJOR),
+				 DATA_TYPE_INT_64);
+	(void) data_convert_type(data_set_string(data_key_set(slurmv, "micro"),
+						 SLURM_MICRO),
+				 DATA_TYPE_INT_64);
+	(void) data_convert_type(data_set_string(data_key_set(slurmv, "minor"),
+						 SLURM_MINOR),
+				 DATA_TYPE_INT_64);
+
+	data_set_string(data_key_set(plugin, "type"), plugin_type);
+	data_set_string(data_key_set(plugin, "name"), plugin_name);
+
+	return data_set_list(data_key_set(resp, "errors"));
+}
+
+extern int resp_error(data_t *errors, int error_code, const char *source,
+		      const char *why, ...)
+{
+	data_t *e = data_set_dict(data_list_append(errors));
+
+	if (why) {
+		va_list ap;
+		va_list ap1;
+		char *str;
+		size_t size;
+
+		va_start(ap, why);
+		size = vsnprintf(NULL, 0, why, ap);
+		va_end(ap);
+
+		str = xmalloc(size + 1);
+
+		va_start(ap1, why);
+		vsprintf(str, why, ap1);
+		va_end(ap1);
+
+		data_set_string(data_key_set(e, "description"), str);
+
+		xfree(str);
+	}
+
+	if (error_code) {
+		data_set_int(data_key_set(e, "error_number"), error_code);
+		data_set_string(data_key_set(e, "error"),
+				slurm_strerror(error_code));
+	}
+
+	if (source)
+		data_set_string(data_key_set(e, "source"), source);
+
+	return error_code;
+}
+
+extern data_t *slurm_openapi_p_get_specification(void)
+{
+	data_t *spec = NULL;
+
+	static_ref_json_to_data_t(spec, openapi_json);
+
+	return spec;
+}
+
+extern void slurm_openapi_p_init(void)
+{
+	init_op_diag();
+	init_op_jobs();
+	init_op_nodes();
+	init_op_partitions();
+	init_op_reservations();
+}
+
+extern void slurm_openapi_p_fini(void)
+{
+	destroy_op_diag();
+	destroy_op_jobs();
+	destroy_op_nodes();
+	destroy_op_partitions();
+	destroy_op_reservations();
+}
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/v0.0.38/api.h slurm-20.11.9/src/slurmrestd/plugins/openapi/v0.0.38/api.h
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/v0.0.38/api.h	1970-01-01 01:00:00.000000000 +0100
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/v0.0.38/api.h	2022-11-10 21:49:08.000000000 +0100
@@ -0,0 +1,77 @@
+/*****************************************************************************\
+ *  api.h - Slurm REST API openapi operations handlers
+ *****************************************************************************
+ *  Copyright (C) 2019-2020 SchedMD LLC.
+ *  Written by Nathan Rini <nate@schedmd.com>
+ *
+ *  This file is part of Slurm, a resource management program.
+ *  For details, see <https://slurm.schedmd.com/>.
+ *  Please also read the included file: DISCLAIMER.
+ *
+ *  Slurm is free software; you can redistribute it and/or modify it under
+ *  the terms of the GNU General Public License as published by the Free
+ *  Software Foundation; either version 2 of the License, or (at your option)
+ *  any later version.
+ *
+ *  In addition, as a special exception, the copyright holders give permission
+ *  to link the code of portions of this program with the OpenSSL library under
+ *  certain conditions as described in each individual source file, and
+ *  distribute linked combinations including the two. You must obey the GNU
+ *  General Public License in all respects for all of the code used other than
+ *  OpenSSL. If you modify file(s) with this exception, you may extend this
+ *  exception to your version of the file(s), but you are not obligated to do
+ *  so. If you do not wish to do so, delete this exception statement from your
+ *  version.  If you delete this exception statement from all source files in
+ *  the program, then also delete it here.
+ *
+ *  Slurm is distributed in the hope that it will be useful, but WITHOUT ANY
+ *  WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+ *  FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
+ *  details.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with Slurm; if not, write to the Free Software Foundation, Inc.,
+ *  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301  USA.
+\*****************************************************************************/
+
+#ifndef SLURMRESTD_OPENAPI_V0038
+#define SLURMRESTD_OPENAPI_V0038
+
+#include "config.h"
+
+#include "slurm/slurm.h"
+
+#include "src/common/data.h"
+
+extern int get_date_param(data_t *query, const char *param, time_t *time);
+
+/*
+ * Fill out boilerplate for every data response
+ * RET ptr to errors dict
+ */
+extern data_t *populate_response_format(data_t *resp);
+
+/*
+ * Add a response error to errors
+ * IN errors - data list to append a new error
+ * IN why - description of error or NULL
+ * IN error_code - Error number
+ * IN source - Where the error was generated
+ * RET value of error_code
+ */
+extern int resp_error(data_t *errors, int error_code, const char *source,
+		      const char *why, ...)
+	__attribute__((format(printf, 4, 5)));
+
+extern void init_op_diag(void);
+extern void init_op_jobs(void);
+extern void init_op_nodes(void);
+extern void init_op_partitions(void);
+extern void init_op_reservations(void);
+extern void destroy_op_diag(void);
+extern void destroy_op_jobs(void);
+extern void destroy_op_nodes(void);
+extern void destroy_op_partitions(void);
+extern void destroy_op_reservations(void);
+
+#endif
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/v0.0.38/diag.c slurm-20.11.9/src/slurmrestd/plugins/openapi/v0.0.38/diag.c
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/v0.0.38/diag.c	1970-01-01 01:00:00.000000000 +0100
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/v0.0.38/diag.c	2022-11-25 15:16:56.544071286 +0100
@@ -0,0 +1,329 @@
+/*****************************************************************************\
+ *  diag.c - Slurm REST API diag http operations handlers
+ *****************************************************************************
+ *  Copyright (C) 2019-2020 SchedMD LLC.
+ *  Written by Nathan Rini <nate@schedmd.com>
+ *
+ *  This file is part of Slurm, a resource management program.
+ *  For details, see <https://slurm.schedmd.com/>.
+ *  Please also read the included file: DISCLAIMER.
+ *
+ *  Slurm is free software; you can redistribute it and/or modify it under
+ *  the terms of the GNU General Public License as published by the Free
+ *  Software Foundation; either version 2 of the License, or (at your option)
+ *  any later version.
+ *
+ *  In addition, as a special exception, the copyright holders give permission
+ *  to link the code of portions of this program with the OpenSSL library under
+ *  certain conditions as described in each individual source file, and
+ *  distribute linked combinations including the two. You must obey the GNU
+ *  General Public License in all respects for all of the code used other than
+ *  OpenSSL. If you modify file(s) with this exception, you may extend this
+ *  exception to your version of the file(s), but you are not obligated to do
+ *  so. If you do not wish to do so, delete this exception statement from your
+ *  version.  If you delete this exception statement from all source files in
+ *  the program, then also delete it here.
+ *
+ *  Slurm is distributed in the hope that it will be useful, but WITHOUT ANY
+ *  WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+ *  FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
+ *  details.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with Slurm; if not, write to the Free Software Foundation, Inc.,
+ *  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301  USA.
+\*****************************************************************************/
+
+#include "config.h"
+
+#include <unistd.h>
+
+#include "slurm/slurm.h"
+
+#include "src/common/list.h"
+#include "src/common/log.h"
+#include "src/common/read_config.h"
+#include "src/common/ref.h"
+#include "src/common/uid.h"
+#include "src/common/xassert.h"
+#include "src/common/xmalloc.h"
+#include "src/common/xstring.h"
+
+#include "src/slurmrestd/operations.h"
+
+#include "src/slurmrestd/plugins/openapi/v0.0.38/api.h"
+
+typedef enum {
+	URL_TAG_UNKNOWN = 0,
+	URL_TAG_DIAG,
+	URL_TAG_PING,
+} url_tag_t;
+
+static int _op_handler_diag(const char *context_id,
+			    http_request_method_t method, data_t *parameters,
+			    data_t *query, int tag, data_t *p, rest_auth_context_t *auth)
+{
+	int rc;
+	stats_info_response_msg_t *resp = NULL;
+	stats_info_request_msg_t *req = xmalloc(sizeof(*req));
+	req->command_id = STAT_COMMAND_GET;
+
+	data_t *errors = populate_response_format(p);
+	data_t *d = data_set_dict(data_key_set(p, "statistics"));
+	data_t *rpcm = data_set_list(data_key_set(d, "rpcs_by_message_type"));
+	data_t *rpcu = data_set_list(data_key_set(d, "rpcs_by_user"));
+	debug4("%s:[%s] diag handler called", __func__, context_id);
+
+	if ((rc = slurm_get_statistics(&resp, req))) {
+		resp_error(errors, rc, "slurm_get_statistics",
+			   "request failed");
+		goto cleanup;
+	}
+
+	data_set_int(data_key_set(d, "parts_packed"), resp->parts_packed);
+	data_set_int(data_key_set(d, "req_time"), resp->req_time);
+	data_set_int(data_key_set(d, "req_time_start"), resp->req_time_start);
+	data_set_int(data_key_set(d, "server_thread_count"),
+		     resp->server_thread_count);
+	data_set_int(data_key_set(d, "agent_queue_size"),
+		     resp->agent_queue_size);
+	data_set_int(data_key_set(d, "agent_count"), resp->agent_count);
+	data_set_int(data_key_set(d, "agent_thread_count"),
+		     resp->agent_thread_count);
+	data_set_int(data_key_set(d, "dbd_agent_queue_size"),
+		     resp->dbd_agent_queue_size);
+	data_set_int(data_key_set(d, "gettimeofday_latency"),
+		     resp->gettimeofday_latency);
+	data_set_int(data_key_set(d, "schedule_cycle_max"),
+		     resp->schedule_cycle_max);
+	data_set_int(data_key_set(d, "schedule_cycle_last"),
+		     resp->schedule_cycle_last);
+	data_set_int(data_key_set(d, "schedule_cycle_total"),
+		     resp->schedule_cycle_counter);
+	data_set_int(data_key_set(d, "schedule_cycle_mean"),
+		     (resp->schedule_cycle_counter ?
+		      (resp->schedule_cycle_sum /
+		       resp->schedule_cycle_counter) : 0));
+	data_set_int(data_key_set(d, "schedule_cycle_mean_depth"),
+		     (resp->schedule_cycle_counter ?
+		      (resp->schedule_cycle_depth /
+		       resp->schedule_cycle_counter) : 0));
+	data_set_int(data_key_set(d, "schedule_cycle_per_minute"),
+		     (((resp->req_time - resp->req_time_start) > 60) ?
+		     ((uint32_t)(resp->schedule_cycle_counter /
+		      ((resp->req_time - resp->req_time_start) / 60))) : 0));
+	data_set_int(data_key_set(d, "schedule_queue_length"),
+		     resp->schedule_queue_len);
+	data_set_int(data_key_set(d, "jobs_submitted"), resp->jobs_submitted);
+	data_set_int(data_key_set(d, "jobs_started"), resp->jobs_started);
+	data_set_int(data_key_set(d, "jobs_completed"), resp->jobs_completed);
+	data_set_int(data_key_set(d, "jobs_canceled"), resp->jobs_canceled);
+	data_set_int(data_key_set(d, "jobs_failed"), resp->jobs_failed);
+	data_set_int(data_key_set(d, "jobs_pending"), resp->jobs_pending);
+	data_set_int(data_key_set(d, "jobs_running"), resp->jobs_running);
+	data_set_int(data_key_set(d, "job_states_ts"), resp->job_states_ts);
+	data_set_int(data_key_set(d, "bf_backfilled_jobs"),
+		     resp->bf_backfilled_jobs);
+	data_set_int(data_key_set(d, "bf_last_backfilled_jobs"),
+		     resp->bf_last_backfilled_jobs);
+	data_set_int(data_key_set(d, "bf_backfilled_het_jobs"),
+		     resp->bf_backfilled_het_jobs);
+	data_set_int(data_key_set(d, "bf_cycle_counter"),
+		     resp->bf_cycle_counter);
+	data_set_int(data_key_set(d, "bf_cycle_mean"),
+		     (resp->bf_cycle_counter > 0) ?
+		      (resp->bf_cycle_sum / resp->bf_cycle_counter) : 0);
+	data_set_int(data_key_set(d, "bf_depth_mean"),
+		     (resp->bf_cycle_counter > 0) ?
+		      (resp->bf_depth_sum / resp->bf_cycle_counter) : 0);
+	data_set_int(data_key_set(d, "bf_depth_mean_try"),
+		     (resp->bf_cycle_counter > 0) ?
+		      (resp->bf_depth_try_sum / resp->bf_cycle_counter) : 0);
+	data_set_int(data_key_set(d, "bf_cycle_last"), resp->bf_cycle_last);
+	data_set_int(data_key_set(d, "bf_cycle_max"), resp->bf_cycle_max);
+	data_set_int(data_key_set(d, "bf_queue_len"), resp->bf_queue_len);
+	data_set_int(data_key_set(d, "bf_queue_len_mean"),
+		     (resp->bf_cycle_counter > 0) ?
+		      (resp->bf_queue_len_sum / resp->bf_cycle_counter) : 0);
+	data_set_int(data_key_set(d, "bf_table_size"), resp->bf_table_size);
+	data_set_int(data_key_set(d, "bf_table_size_mean"),
+		     (resp->bf_cycle_counter > 0) ?
+		      (resp->bf_table_size_sum / resp->bf_cycle_counter) : 0);
+	data_set_int(data_key_set(d, "bf_when_last_cycle"),
+		     resp->bf_when_last_cycle);
+	data_set_bool(data_key_set(d, "bf_active"), (resp->bf_active != 0));
+
+	if (resp->rpc_type_size) {
+		uint32_t *rpc_type_ave_time = xcalloc(
+			resp->rpc_type_size, sizeof(*rpc_type_ave_time));
+
+		for (int i = 0; i < resp->rpc_type_size; i++) {
+			rpc_type_ave_time[i] = resp->rpc_type_time[i] /
+					       resp->rpc_type_cnt[i];
+		}
+
+		for (int i = 0; i < resp->rpc_type_size; i++) {
+			data_t *r = data_set_dict(data_list_append(rpcm));
+			data_set_string(data_key_set(r, "message_type"),
+					rpc_num2string(resp->rpc_type_id[i]));
+			data_set_int(data_key_set(r, "type_id"),
+				     resp->rpc_type_id[i]);
+			data_set_int(data_key_set(r, "count"),
+				     resp->rpc_type_cnt[i]);
+			data_set_int(data_key_set(r, "average_time"),
+				     rpc_type_ave_time[i]);
+			data_set_int(data_key_set(r, "total_time"),
+				     resp->rpc_type_time[i]);
+		}
+
+		xfree(rpc_type_ave_time);
+	}
+
+	if (resp->rpc_user_size) {
+		uint32_t *rpc_user_ave_time = xcalloc(
+			resp->rpc_user_size, sizeof(*rpc_user_ave_time));
+
+		for (int i = 0; i < resp->rpc_user_size; i++) {
+			rpc_user_ave_time[i] = resp->rpc_user_time[i] /
+					       resp->rpc_user_cnt[i];
+		}
+
+		for (int i = 0; i < resp->rpc_user_size; i++) {
+			data_t *u = data_set_dict(data_list_append(rpcu));
+			data_t *un = data_key_set(u, "user");
+			char *user = uid_to_string_or_null(
+				resp->rpc_user_id[i]);
+
+			data_set_int(data_key_set(u, "user_id"),
+				     resp->rpc_user_id[i]);
+			data_set_int(data_key_set(u, "count"),
+				     resp->rpc_user_cnt[i]);
+			data_set_int(data_key_set(u, "average_time"),
+				     rpc_user_ave_time[i]);
+			data_set_int(data_key_set(u, "total_time"),
+				     resp->rpc_user_time[i]);
+
+			if (!user)
+				data_set_string_fmt(un, "%u",
+						    resp->rpc_user_id[i]);
+			else
+				data_set_string_own(un, user);
+		}
+
+		xfree(rpc_user_ave_time);
+	}
+
+cleanup:
+	slurm_free_stats_response_msg(resp);
+	xfree(req);
+	return rc;
+}
+
+static int _op_handler_ping(const char *context_id,
+			    http_request_method_t method, data_t *parameters,
+			    data_t *query, int tag, data_t *resp_ptr,
+			    rest_auth_context_t *auth)
+{
+	//based on _print_ping() from scontrol
+	int rc = SLURM_SUCCESS;
+	slurm_ctl_conf_info_msg_t *slurm_ctl_conf_ptr = NULL;
+
+	data_t *errors = populate_response_format(resp_ptr);
+
+	if ((rc = slurm_load_ctl_conf((time_t) NULL, &slurm_ctl_conf_ptr)))
+		return resp_error(errors, rc, "slurm_load_ctl_conf",
+				  "slurmctld config is unable to load");
+
+	if (slurm_ctl_conf_ptr) {
+		data_t *pings = data_key_set(resp_ptr, "pings");
+		data_set_list(pings);
+
+		xassert(slurm_ctl_conf_ptr->control_cnt);
+		for (size_t i = 0; i < slurm_ctl_conf_ptr->control_cnt; i++) {
+			const int status = slurm_ping(i);
+			char mode[64];
+
+			if (i == 0)
+				snprintf(mode, sizeof(mode), "primary");
+			else if ((i == 1) &&
+				 (slurm_ctl_conf_ptr->control_cnt == 2))
+				snprintf(mode, sizeof(mode), "backup");
+			else
+				snprintf(mode, sizeof(mode), "backup%zu", i);
+
+			data_t *ping = data_set_dict(data_list_append(pings));
+
+			data_set_string(data_key_set(ping, "hostname"),
+					slurm_ctl_conf_ptr->control_machine[i]);
+
+			data_set_string(data_key_set(ping, "ping"),
+					(status == SLURM_SUCCESS ? "UP" :
+								   "DOWN"));
+			data_set_int(data_key_set(ping, "status"), status);
+			data_set_string(data_key_set(ping, "mode"), mode);
+		}
+	} else {
+		rc = resp_error(errors, ESLURM_INTERNAL, "slurm_load_ctl_conf",
+				"slurmctld config is missing");
+	}
+
+	slurm_free_ctl_conf(slurm_ctl_conf_ptr);
+
+	return rc;
+}
+
+/* based on _print_license_info() from scontrol */
+static int _op_handler_licenses(const char *context_id,
+				http_request_method_t method,
+				data_t *parameters, data_t *query, int tag,
+				data_t *resp_ptr, rest_auth_context_t *auth)
+{
+	int rc;
+	license_info_msg_t *msg;
+	const uint16_t show_flags = 0;
+	const time_t last_update = 0;
+	data_t *licenses, *errors;
+
+	errors = populate_response_format(resp_ptr);
+
+	if ((rc = slurm_load_licenses(last_update, &msg, show_flags))) {
+		slurm_free_license_info_msg(msg);
+		return resp_error(errors, rc, "slurm_load_licenses",
+				  "slurmctld unable to load licenses");
+	}
+
+	licenses = data_set_list(data_key_set(resp_ptr, "licenses"));
+
+	for (int cc = 0; cc < msg->num_lic; cc++) {
+		data_t *lic = data_set_dict(data_list_append(licenses));
+		data_set_string(data_key_set(lic, "LicenseName"),
+				msg->lic_array[cc].name);
+		data_set_int(data_key_set(lic, "Total"),
+			     msg->lic_array[cc].total);
+		data_set_int(data_key_set(lic, "Used"),
+			     msg->lic_array[cc].in_use);
+		data_set_int(data_key_set(lic, "Free"),
+			     msg->lic_array[cc].available);
+		data_set_int(data_key_set(lic, "Reserved"),
+			     msg->lic_array[cc].reserved);
+		data_set_bool(data_key_set(lic, "Remote"),
+			      msg->lic_array[cc].remote);
+	}
+
+	slurm_free_license_info_msg(msg);
+	return rc;
+}
+
+extern void init_op_diag(void)
+{
+	bind_operation_handler("/slurm/v0.0.38/diag/", _op_handler_diag, 0);
+	bind_operation_handler("/slurm/v0.0.38/ping/", _op_handler_ping, 0);
+	bind_operation_handler("/slurm/v0.0.38/licenses/", _op_handler_licenses, 0);
+}
+
+extern void destroy_op_diag(void)
+{
+	unbind_operation_handler(_op_handler_diag);
+	unbind_operation_handler(_op_handler_ping);
+	unbind_operation_handler(_op_handler_licenses);
+}
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/v0.0.38/jobs.c slurm-20.11.9/src/slurmrestd/plugins/openapi/v0.0.38/jobs.c
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/v0.0.38/jobs.c	1970-01-01 01:00:00.000000000 +0100
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/v0.0.38/jobs.c	2022-12-16 18:00:54.578498378 +0100
@@ -0,0 +1,1675 @@
+/*****************************************************************************\
+ *  jobs.c - Slurm REST API jobs http operations handlers
+ *****************************************************************************
+ *  Copyright (C) 2019-2020 SchedMD LLC.
+ *  Written by Nathan Rini <nate@schedmd.com>
+ *
+ *  This file is part of Slurm, a resource management program.
+ *  For details, see <https://slurm.schedmd.com/>.
+ *  Please also read the included file: DISCLAIMER.
+ *
+ *  Slurm is free software; you can redistribute it and/or modify it under
+ *  the terms of the GNU General Public License as published by the Free
+ *  Software Foundation; either version 2 of the License, or (at your option)
+ *  any later version.
+ *
+ *  In addition, as a special exception, the copyright holders give permission
+ *  to link the code of portions of this program with the OpenSSL library under
+ *  certain conditions as described in each individual source file, and
+ *  distribute linked combinations including the two. You must obey the GNU
+ *  General Public License in all respects for all of the code used other than
+ *  OpenSSL. If you modify file(s) with this exception, you may extend this
+ *  exception to your version of the file(s), but you are not obligated to do
+ *  so. If you do not wish to do so, delete this exception statement from your
+ *  version.  If you delete this exception statement from all source files in
+ *  the program, then also delete it here.
+ *
+ *  Slurm is distributed in the hope that it will be useful, but WITHOUT ANY
+ *  WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+ *  FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
+ *  details.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with Slurm; if not, write to the Free Software Foundation, Inc.,
+ *  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301  USA.
+\*****************************************************************************/
+
+#include "config.h"
+
+#define _GNU_SOURCE
+
+#include <search.h>
+#include <signal.h>
+#include <stdint.h>
+#include <unistd.h>
+
+#include "slurm/slurm.h"
+
+#include "src/common/cpu_frequency.h"
+#include "src/common/env.h"
+#include "src/common/list.h"
+#include "src/common/log.h"
+#include "src/common/parse_time.h"
+#include "src/common/proc_args.h"
+#include "src/common/ref.h"
+#include "src/common/node_select.h"
+#include "src/common/slurm_acct_gather_profile.h"
+#include "src/common/slurm_protocol_api.h"
+#include "src/common/slurm_protocol_defs.h"
+#include "src/common/slurm_resource_info.h"
+#include "src/common/strlcpy.h"
+#include "src/common/tres_bind.h"
+#include "src/common/tres_frequency.h"
+#include "src/common/uid.h"
+#include "src/common/xassert.h"
+#include "src/common/xmalloc.h"
+#include "src/common/xstring.h"
+
+#include "src/slurmrestd/operations.h"
+
+#include "src/slurmrestd/plugins/openapi/v0.0.38/api.h"
+
+typedef struct {
+	const char *param;
+	int optval;
+	bool disabled;
+} params_t;
+
+static struct hsearch_data hash_params = { 0 };
+/* track array of parameter names that have been forced to lower case */
+static char **lower_param_names = NULL;
+
+/*
+ * This needs to match common_options in src/common/slurm_opt.c
+ * for every optval (aliases allowed)
+ *
+ * This list *should* be alphabetically sorted by the param string.
+ *
+ * Some of these options have been disabled as they are inappropriate for
+ * use through slurmrestd (e.g., "burst_buffer_file"), others were disabled
+ * by executive fiat.
+ */
+const params_t job_params[] = {
+	{ "accelerator_binding", LONG_OPT_ACCEL_BIND, true },
+	{ "account", 'A' },
+	{ "account_gather_frequency", LONG_OPT_ACCTG_FREQ },
+	{ "allocation_node_list", LONG_OPT_ALLOC_NODELIST, true },
+	{ "array", 'a' },
+	{ "argv", LONG_OPT_ARGV },
+	{ "batch_features", LONG_OPT_BATCH },
+	{ "begin_time", 'b' },
+	{ "bell", LONG_OPT_BELL, true },
+	{ "burst_buffer_file", LONG_OPT_BURST_BUFFER_FILE, true },
+	{ "burst_buffer", LONG_OPT_BURST_BUFFER_SPEC },
+	{ "cluster_constraint", LONG_OPT_CLUSTER_CONSTRAINT },
+	{ "cluster", LONG_OPT_CLUSTER, true },
+	{ "clusters", 'M', true },
+	{ "comment", LONG_OPT_COMMENT },
+	{ "compress", LONG_OPT_COMPRESS, true },
+	{ "constraint", 'C' },
+	{ "constraints", 'C' },
+	{ "contiguous", LONG_OPT_CONTIGUOUS, true },
+	//{ "container", LONG_OPT_CONTAINER },
+	{ "core_specification", 'S' },
+	{ "cores_per_socket", LONG_OPT_CORESPERSOCKET },
+	{ "cpu_binding_hint", LONG_OPT_HINT },
+	{ "cpu_binding", LONG_OPT_CPU_BIND, true },
+	{ "cpu_frequency", LONG_OPT_CPU_FREQ },
+	{ "cpus_per_gpu", LONG_OPT_CPUS_PER_GPU },
+	{ "cpus_per_task", 'c' },
+	{ "current_working_directory", 'D' },
+	{ "cwd", 'D' },
+	{ "deadline", LONG_OPT_DEADLINE },
+	{ "debugger_test", LONG_OPT_DEBUGGER_TEST, true },
+	{ "delay_boot", LONG_OPT_DELAY_BOOT },
+	{ "dependency", 'd' },
+	{ "disable_status", 'X', true },
+	{ "distribution", 'm' },
+	{ "environment", LONG_OPT_ENVIRONMENT },
+	{ "epilog", LONG_OPT_EPILOG, true },
+	{ "exclude_nodes", 'x' },
+	{ "excluded_nodes", 'x', true },
+	{ "exclusive", LONG_OPT_EXCLUSIVE },
+	{ "export_file", LONG_OPT_EXPORT_FILE, true },
+	{ "export", LONG_OPT_EXPORT, true },
+	{ "extra_node_info", 'B', true },
+	{ "get_user_environment", LONG_OPT_GET_USER_ENV },
+	{ "gpu_binding", LONG_OPT_GPU_BIND },
+	{ "gpu_frequency", LONG_OPT_GPU_FREQ },
+	{ "gpus", 'G' },
+	{ "gpus_per_node", LONG_OPT_GPUS_PER_NODE },
+	{ "gpus_per_socket", LONG_OPT_GPUS_PER_SOCKET },
+	{ "gpus_per_task", LONG_OPT_GPUS_PER_TASK },
+	{ "gres_flags", LONG_OPT_GRES_FLAGS },
+	{ "gres", LONG_OPT_GRES },
+	{ "group_id", LONG_OPT_GID, true },
+	{ "help", 'h', true },
+	{ "hold", 'H' },
+	{ "ignore_pbs", LONG_OPT_IGNORE_PBS, true },
+	{ "immediate", 'I', true },
+	{ "job_id", LONG_OPT_JOBID, true },
+	{ "job_name", 'J' },
+	{ "kill_command", 'K', true },
+	{ "kill_on_bad_exit", 'K', true },
+	{ "kill_on_invalid_dependency", LONG_OPT_KILL_INV_DEP },
+	{ "label", 'l', true },
+	{ "license", 'L' },
+	{ "licenses", 'L' },
+	{ "mail_type", LONG_OPT_MAIL_TYPE },
+	{ "mail_user", LONG_OPT_MAIL_USER },
+	{ "max_threads", 'T', true },
+	{ "mcs_label", LONG_OPT_MCS_LABEL },
+	{ "memory_binding", LONG_OPT_MEM_BIND },
+	{ "memory_per_cpu", LONG_OPT_MEM_PER_CPU },
+	{ "memory_per_gpu", LONG_OPT_MEM_PER_GPU },
+	{ "memory_per_node", LONG_OPT_MEM },
+	{ "message_timeout", LONG_OPT_MSG_TIMEOUT, true },
+	{ "minimum_cpus_per_node", LONG_OPT_MINCPUS },
+	{ "minimum_nodes", LONG_OPT_USE_MIN_NODES },
+	{ "mpi", LONG_OPT_MPI, true },
+	{ "multiple_program", LONG_OPT_MULTI, true },
+	{ "name", 'J' },
+	{ "network", LONG_OPT_NETWORK, true },
+	{ "nice", LONG_OPT_NICE },
+	{ "no_allocation", 'Z', true },
+	{ "no_bell", LONG_OPT_NO_BELL, true },
+	/* security implications to trying to read a user file */
+	{ "nodefile", 'F', true },
+	{ "nodelist", 'w' },
+	{ "node_list", 'w' },
+	{ "nodes", 'N' },
+	{ "no_kill", 'k' },
+	{ "no_requeue", LONG_OPT_NO_REQUEUE }, /* not in OAS */
+	{ "no_shell", LONG_OPT_NO_SHELL, true },
+	{ "open_mode", LONG_OPT_OPEN_MODE },
+	{ "overcommit", 'O', true },
+	{ "oversubscribe", 's', true },
+	{ "hetjob_group", LONG_OPT_HET_GROUP, true },
+	{ "parsable", LONG_OPT_PARSABLE, true },
+	{ "partition", 'p' },
+	//{ "prefer", LONG_OPT_PREFER },
+	{ "power_flags", LONG_OPT_POWER, true },
+	{ "preserve_environment", 'E', true },
+	{ "priority", LONG_OPT_PRIORITY, false },
+	{ "profile", LONG_OPT_PROFILE },
+	{ "prolog", LONG_OPT_PROLOG, true },
+	{ "propagate", LONG_OPT_PROPAGATE, true },
+	{ "pty", LONG_OPT_PTY, true },
+	{ "qos", 'q' },
+	{ "quiet", 'Q', true },
+	{ "quit_on_interrupt", LONG_OPT_QUIT_ON_INTR, true },
+	{ "reboot", LONG_OPT_REBOOT, true },
+	{ "relative", 'r', true },
+	{ "requeue", LONG_OPT_REQUEUE },
+	{ "required_nodes", 'w', true },
+	{ "required_switches", LONG_OPT_SWITCHES, true },
+	{ "reservation", LONG_OPT_RESERVATION },
+	{ "reserve_port", LONG_OPT_RESV_PORTS, true },
+	{ "reserve_ports", LONG_OPT_RESV_PORTS, true },
+	{ "signal", LONG_OPT_SIGNAL },
+	{ "slurmd_debug", LONG_OPT_SLURMD_DEBUG, true },
+	{ "sockets_per_node", LONG_OPT_SOCKETSPERNODE },
+	{ "spread_job", LONG_OPT_SPREAD_JOB },
+	{ "standard_error", 'e' },
+	{ "standard_input", 'i' },
+	{ "standard_output", 'o' },
+	{ "task_epilog", LONG_OPT_TASK_EPILOG, true },
+	{ "task_prolog", LONG_OPT_TASK_PROLOG, true },
+	{ "tasks", 'n' },
+	{ "ntasks", 'n' },
+	{ "tasks_per_core", LONG_OPT_NTASKSPERCORE },
+	{ "ntasks_per_core", LONG_OPT_NTASKSPERCORE },
+	{ "ntasks_per_gpu", LONG_OPT_NTASKSPERGPU },
+	{ "tasks_per_node", LONG_OPT_NTASKSPERNODE },
+	{ "ntasks_per_node", LONG_OPT_NTASKSPERNODE },
+	{ "tasks_per_socket", LONG_OPT_NTASKSPERSOCKET },
+	{ "ntasks_per_socket", LONG_OPT_NTASKSPERSOCKET },
+	{ "ntasks_per_tres", LONG_OPT_NTASKSPERTRES },
+	{ "temporary_disk_per_node", LONG_OPT_TMP },
+	{ "test_only", LONG_OPT_TEST_ONLY },
+	{ "thread_specification", LONG_OPT_THREAD_SPEC },
+	{ "threads_per_core", LONG_OPT_THREADSPERCORE },
+	{ "threads", 'T', true },
+	{ "time_limit", 't' },
+	{ "time_minimum", LONG_OPT_TIME_MIN },
+	/* Handler for LONG_OPT_TRES_PER_JOB never defined
+	 * { "TRES per job", LONG_OPT_TRES_PER_JOB, true },
+	 */
+	{ "umask", LONG_OPT_UMASK },
+	{ "unbuffered", 'u', true },
+	{ "unknown", '?', true },
+	{ "usage", LONG_OPT_USAGE, true },
+	{ "user_id", LONG_OPT_UID, true },
+	{ "version", 'V', true },
+	{ "verbose", 'v', true },
+	{ "wait_all_nodes", LONG_OPT_WAIT_ALL_NODES },
+	{ "wait_for_switch", LONG_OPT_SWITCH_WAIT, true },
+	{ "wait", 'W', true },
+	{ "wckey", LONG_OPT_WCKEY },
+	{ "wrap", LONG_OPT_WCKEY, true },
+	{ "x11", LONG_OPT_X11, true },
+};
+static const int param_count = (sizeof(job_params) / sizeof(params_t));
+
+typedef enum {
+	URL_TAG_UNKNOWN = 0,
+	URL_TAG_JOBS,
+	URL_TAG_JOB,
+	URL_TAG_JOB_SUBMIT,
+} url_tag_t;
+
+typedef struct {
+	int rc;
+	bool het_job;
+	List jobs;
+	job_desc_msg_t *job;
+} job_parse_list_t;
+
+static void _list_delete_job_desc_msg_t(void *_p)
+{
+	xassert(_p);
+	slurm_free_job_desc_msg(_p);
+}
+
+typedef struct {
+	slurm_opt_t *opt;
+	data_t *errors;
+} job_foreach_params_t;
+
+static data_for_each_cmd_t _per_job_param(const char *key, const data_t *data,
+					  void *arg)
+{
+	int rc;
+	char lkey[256];
+	job_foreach_params_t *args = arg;
+	data_t *errors = args->errors;
+	params_t *p = NULL;
+	ENTRY e = { .key = lkey };
+	ENTRY *re = NULL;
+
+	/* clone key to force all lower characters */
+	strlcpy(lkey, key, sizeof(lkey));
+	xstrtolower(lkey);
+
+	if (!(rc = hsearch_r(e, FIND, &re, &hash_params))) {
+		resp_error(errors, rc, "hsearch_r",
+			   "Unknown key \"%s\"", lkey);
+		return DATA_FOR_EACH_FAIL;
+	}
+
+	p = re->data;
+	if (p->disabled) {
+		resp_error(errors, rc, "openapi specification",
+			   "Disabled key: \"%s\"", p->param);
+		return DATA_FOR_EACH_FAIL;
+	}
+
+	if ((rc = slurm_process_option_data(args->opt, p->optval, data,
+					    errors))) {
+		resp_error(errors, rc, "slurm_process_option_data",
+			   "Unable to process key \"%s\"", lkey);
+		return DATA_FOR_EACH_FAIL;
+	}
+
+	return DATA_FOR_EACH_CONT;
+}
+
+/*
+ * copied from _fill_job_desc_from_opts() in src/sbatch/sbatch.c
+ * Returns 0 on success, -1 on failure
+ */
+static int _fill_job_desc_from_sbatch_opts(slurm_opt_t *opt,
+					   job_desc_msg_t *desc,
+					   bool update_only)
+{
+	const sbatch_opt_t *sbopt = opt->sbatch_opt;
+	int i;
+
+	/*
+	 * Handle environment first since many of the options will
+	 * append to the environment
+	 */
+	env_array_free(desc->environment);
+	if (opt->environment)
+		desc->environment = env_array_copy(
+			(const char **)opt->environment);
+	else
+		desc->environment = env_array_create();
+
+	if (opt->get_user_env_time >= 0)
+		env_array_overwrite(&desc->environment, "SLURM_GET_USER_ENV",
+				    "1");
+
+	if (slurm_option_isset(opt, "contiguous"))
+		desc->contiguous = opt->contiguous ? 1 : 0;
+	else
+		desc->contiguous = NO_VAL16;
+
+	if (opt->core_spec != NO_VAL16)
+		desc->core_spec = opt->core_spec;
+	desc->features = xstrdup(opt->constraint);
+	desc->cluster_features = xstrdup(opt->c_constraint);
+	if (opt->job_name)
+		desc->name = xstrdup(opt->job_name);
+	else if (!update_only)
+		desc->name = xstrdup("sbatch");
+	desc->reservation = xstrdup(opt->reservation);
+	desc->wckey = xstrdup(opt->wckey);
+
+	desc->req_nodes = xstrdup(opt->nodelist);
+	desc->extra = xstrdup(opt->extra);
+	desc->exc_nodes = xstrdup(opt->exclude);
+	desc->partition = xstrdup(opt->partition);
+	desc->profile = opt->profile;
+	if (opt->licenses)
+		desc->licenses = xstrdup(opt->licenses);
+	if (opt->nodes_set) {
+		desc->min_nodes = opt->min_nodes;
+		if (opt->max_nodes)
+			desc->max_nodes = opt->max_nodes;
+	} else if (opt->ntasks_set && (opt->ntasks == 0))
+		desc->min_nodes = 0;
+	if (opt->ntasks_per_node)
+		desc->ntasks_per_node = opt->ntasks_per_node;
+	if (opt->ntasks_per_tres != NO_VAL)
+		desc->ntasks_per_tres = opt->ntasks_per_tres;
+	else if (opt->ntasks_per_gpu != NO_VAL)
+		desc->ntasks_per_tres = opt->ntasks_per_gpu;
+
+	/* Disable sending uid/gid as it is handled by auth layer */
+	/* desc->user_id = opt->uid; */
+	/* desc->group_id = opt->gid; */
+	if (opt->dependency)
+		desc->dependency = xstrdup(opt->dependency);
+
+	if (sbopt->array_inx)
+		desc->array_inx = xstrdup(sbopt->array_inx);
+	if (sbopt->batch_features)
+		desc->batch_features = xstrdup(sbopt->batch_features);
+	if (opt->mem_bind)
+		desc->mem_bind = xstrdup(opt->mem_bind);
+	if (opt->mem_bind_type)
+		desc->mem_bind_type = opt->mem_bind_type;
+	if (opt->plane_size != NO_VAL)
+		desc->plane_size = opt->plane_size;
+	desc->task_dist = opt->distribution;
+	if ((opt->distribution & SLURM_DIST_STATE_BASE) ==
+	    SLURM_DIST_ARBITRARY) {
+		env_array_overwrite_fmt(&desc->environment,
+					"SLURM_ARBITRARY_NODELIST", "%s",
+					opt->nodelist);
+	}
+
+	desc->network = xstrdup(opt->network);
+	if (opt->nice != NO_VAL)
+		desc->nice = NICE_OFFSET + opt->nice;
+	if (opt->priority)
+		desc->priority = opt->priority;
+
+	if (slurm_option_isset(opt, "mail_type"))
+		desc->mail_type = opt->mail_type;
+	if (opt->mail_user)
+		desc->mail_user = xstrdup(opt->mail_user);
+	if (opt->begin)
+		desc->begin_time = opt->begin;
+	if (opt->deadline)
+		desc->deadline = opt->deadline;
+	if (opt->delay_boot != NO_VAL)
+		desc->delay_boot = opt->delay_boot;
+	if (opt->account)
+		desc->account = xstrdup(opt->account);
+	if (opt->burst_buffer)
+		desc->burst_buffer = opt->burst_buffer;
+	if (opt->comment)
+		desc->comment = xstrdup(opt->comment);
+	if (opt->qos)
+		desc->qos = xstrdup(opt->qos);
+
+	if (slurm_option_isset(opt, "hold")) {
+		if (opt->hold)
+			desc->priority = 0;
+		else
+			desc->priority = INFINITE;
+	} else
+		desc->priority = NO_VAL;
+
+	if (opt->reboot)
+		desc->reboot = 1;
+
+	/* job constraints */
+	if (opt->pn_min_cpus > -1)
+		desc->pn_min_cpus = opt->pn_min_cpus;
+	if (opt->pn_min_memory != NO_VAL64)
+		desc->pn_min_memory = opt->pn_min_memory;
+	else if (opt->mem_per_cpu != NO_VAL64)
+		desc->pn_min_memory = opt->mem_per_cpu | MEM_PER_CPU;
+	if (opt->pn_min_tmp_disk != NO_VAL64)
+		desc->pn_min_tmp_disk = opt->pn_min_tmp_disk;
+	if (opt->overcommit) {
+		desc->min_cpus = MAX(opt->min_nodes, 1);
+		desc->overcommit = opt->overcommit;
+	} else if (opt->cpus_set)
+		desc->min_cpus = opt->ntasks * opt->cpus_per_task;
+	else if (opt->nodes_set && (opt->min_nodes == 0))
+		desc->min_cpus = 0;
+	else if (!update_only)
+		desc->min_cpus = opt->ntasks;
+
+	if (opt->ntasks_set)
+		desc->num_tasks = opt->ntasks;
+	if (opt->cpus_set)
+		desc->cpus_per_task = opt->cpus_per_task;
+	if (opt->ntasks_per_socket > -1)
+		desc->ntasks_per_socket = opt->ntasks_per_socket;
+	if (opt->ntasks_per_core > -1)
+		desc->ntasks_per_core = opt->ntasks_per_core;
+
+	/* node constraints */
+	if (opt->sockets_per_node != NO_VAL)
+		desc->sockets_per_node = opt->sockets_per_node;
+	if (opt->cores_per_socket != NO_VAL)
+		desc->cores_per_socket = opt->cores_per_socket;
+	if (opt->threads_per_core != NO_VAL)
+		desc->threads_per_core = opt->threads_per_core;
+
+	if (opt->no_kill)
+		desc->kill_on_node_fail = 0;
+	if (opt->time_limit != NO_VAL)
+		desc->time_limit = opt->time_limit;
+	if (opt->time_min != NO_VAL)
+		desc->time_min = opt->time_min;
+	if (opt->shared != NO_VAL16)
+		desc->shared = opt->shared;
+
+	desc->wait_all_nodes = sbopt->wait_all_nodes;
+	if (opt->warn_flags)
+		desc->warn_flags = opt->warn_flags;
+	if (opt->warn_signal)
+		desc->warn_signal = opt->warn_signal;
+	if (opt->warn_time)
+		desc->warn_time = opt->warn_time;
+
+	desc->argc = sbopt->script_argc;
+	desc->argv = xmalloc(sizeof(char *) * sbopt->script_argc);
+	for (i = 0; i < sbopt->script_argc; i++)
+		desc->argv[i] = xstrdup(sbopt->script_argv[i]);
+	desc->std_err = xstrdup(opt->efname);
+	desc->std_in = xstrdup(opt->ifname);
+	desc->std_out = xstrdup(opt->ofname);
+
+	if (slurm_option_isset(opt, "chdir"))
+		desc->work_dir = xstrdup(opt->chdir);
+	if (sbopt->requeue != NO_VAL)
+		desc->requeue = sbopt->requeue;
+	if (opt->open_mode)
+		desc->open_mode = opt->open_mode;
+	if (opt->acctg_freq)
+		desc->acctg_freq = xstrdup(opt->acctg_freq);
+
+	if (opt->spank_job_env_size) {
+		desc->spank_job_env_size = opt->spank_job_env_size;
+		desc->spank_job_env = xmalloc(sizeof(char *) *
+					      opt->spank_job_env_size);
+		for (i = 0; i < opt->spank_job_env_size; i++)
+			desc->spank_job_env[i] = xstrdup(opt->spank_job_env[i]);
+	}
+
+	desc->cpu_freq_min = opt->cpu_freq_min;
+	desc->cpu_freq_max = opt->cpu_freq_max;
+	desc->cpu_freq_gov = opt->cpu_freq_gov;
+
+	if (opt->req_switch >= 0)
+		desc->req_switch = opt->req_switch;
+	if (opt->wait4switch >= 0)
+		desc->wait4switch = opt->wait4switch;
+
+	desc->power_flags = opt->power;
+	if (opt->job_flags)
+		desc->bitflags = opt->job_flags;
+	if (opt->mcs_label)
+		desc->mcs_label = xstrdup(opt->mcs_label);
+
+	if (opt->cpus_per_gpu)
+		xstrfmtcat(desc->cpus_per_tres, "gpu:%d", opt->cpus_per_gpu);
+	if (opt->gpu_bind)
+		xstrfmtcat(opt->tres_bind, "gpu:%s", opt->gpu_bind);
+	if (tres_bind_verify_cmdline(opt->tres_bind)) {
+		error("Invalid --tres-bind argument: %s. Ignored",
+		      opt->tres_bind);
+		xfree(opt->tres_bind);
+	}
+	desc->tres_bind = xstrdup(opt->tres_bind);
+	xfmt_tres_freq(&opt->tres_freq, "gpu", opt->gpu_freq);
+	if (tres_freq_verify_cmdline(opt->tres_freq)) {
+		error("Invalid --tres-freq argument: %s. Ignored",
+		      opt->tres_freq);
+		xfree(opt->tres_freq);
+	}
+	desc->tres_freq = xstrdup(opt->tres_freq);
+	xfmt_tres(&desc->tres_per_job, "gpu", opt->gpus);
+	xfmt_tres(&desc->tres_per_node, "gpu", opt->gpus_per_node);
+	if (opt->gres) {
+		if (desc->tres_per_node)
+			xstrfmtcat(desc->tres_per_node, ",%s", opt->gres);
+		else
+			desc->tres_per_node = xstrdup(opt->gres);
+	}
+	xfmt_tres(&desc->tres_per_socket, "gpu", opt->gpus_per_socket);
+	xfmt_tres(&desc->tres_per_task, "gpu", opt->gpus_per_task);
+	if (opt->mem_per_gpu != NO_VAL64)
+		xstrfmtcat(desc->mem_per_tres, "gpu:%" PRIu64,
+			   opt->mem_per_gpu);
+
+	desc->clusters = xstrdup(opt->clusters);
+
+	/* update env size after all changes */
+	desc->env_size = envcount(desc->environment);
+
+	return 0;
+}
+
+/*
+ * based on _fill_job_desc_from_opts from sbatch
+ */
+static job_desc_msg_t *_parse_job_desc(const data_t *job, data_t *errors,
+				       bool update_only)
+{
+	int rc = SLURM_SUCCESS;
+	job_desc_msg_t *req = xmalloc(sizeof(*req));
+	char *opt_string = NULL;
+	sbatch_opt_t sbopt = { 0 };
+	slurm_opt_t opt = { .sbatch_opt = &sbopt };
+	struct option *spanked = slurm_option_table_create(&opt, &opt_string);
+
+	job_foreach_params_t args = {
+		.opt = &opt,
+		.errors = errors,
+	};
+
+	slurm_reset_all_options(&opt, true);
+	if (data_dict_for_each_const(job, _per_job_param, &args) < 0) {
+		rc = ESLURM_REST_FAIL_PARSING;
+		goto cleanup;
+	}
+
+	slurm_init_job_desc_msg(req);
+	if (!update_only)
+		req->task_dist = SLURM_DIST_UNKNOWN;
+
+	if (_fill_job_desc_from_sbatch_opts(&opt, req, update_only)) {
+		rc = SLURM_ERROR;
+		goto cleanup;
+	}
+
+	if (!update_only && (!req->environment || !req->env_size)) {
+		/*
+		 * Jobs provided via data must have their environment
+		 * setup or they will simply be rejected. Error now instead of
+		 * bothering the controller.
+		 */
+		data_t *err = data_set_dict(data_list_append(errors));
+		rc = ESLURM_ENVIRONMENT_MISSING;
+		data_set_string(data_key_set(err, "error"),
+				"environment must be set");
+		data_set_int(data_key_set(err, "error_code"), rc);
+		goto cleanup;
+	}
+	xassert(req->env_size == envcount(req->environment));
+
+cleanup:
+	slurm_free_options_members(&opt);
+	slurm_option_table_destroy(spanked);
+	xfree(opt_string);
+
+	if (rc) {
+		slurm_free_job_desc_msg(req);
+		return NULL;
+	}
+
+	/*
+	 * Add generated environment variables to match
+	 * _opt_verify() in src/sbatch/opt.c
+	 */
+	if (req->name)
+		env_array_overwrite(&req->environment, "SLURM_JOB_NAME",
+				    req->name);
+
+	if (req->open_mode) {
+		/* Propage mode to spawned job using environment variable */
+		if (req->open_mode == OPEN_MODE_APPEND)
+			env_array_overwrite(&req->environment,
+					    "SLURM_OPEN_MODE", "a");
+		else
+			env_array_overwrite(&req->environment,
+					    "SLURM_OPEN_MODE", "t");
+	}
+
+	if (req->dependency)
+		env_array_overwrite(&req->environment, "SLURM_JOB_DEPENDENCY",
+				    req->dependency);
+
+	/* intentionally skipping SLURM_EXPORT_ENV */
+
+	if (req->profile) {
+		char tmp[128];
+		acct_gather_profile_to_string_r(req->profile, tmp);
+		env_array_overwrite(&req->environment, "SLURM_PROFILE", tmp);
+	}
+
+	if (req->acctg_freq)
+		env_array_overwrite(&req->environment, "SLURM_ACCTG_FREQ",
+				    req->acctg_freq);
+
+#ifdef HAVE_NATIVE_CRAY
+	if (req->network)
+		env_array_overwrite(&req->environment, "SLURM_NETWORK",
+				    req->network);
+#endif
+
+	if (req->cpu_freq_min || req->cpu_freq_max || req->cpu_freq_gov) {
+		char *tmp = cpu_freq_to_cmdline(req->cpu_freq_min,
+						req->cpu_freq_max,
+						req->cpu_freq_gov);
+
+		if (tmp)
+			env_array_overwrite(&req->environment,
+					    "SLURM_CPU_FREQ_REQ", tmp);
+
+		xfree(tmp);
+	}
+
+	/* update size of env in case it changed */
+	req->env_size = envcount(req->environment);
+
+	return req;
+}
+
+typedef struct {
+	size_t i;
+	data_t *errors;
+	char *script;
+	bool update_only;
+	job_parse_list_t *rc;
+} _parse_job_component_t;
+
+static data_for_each_cmd_t _parse_job_component(const data_t *data, void *arg)
+{
+	_parse_job_component_t *j = arg;
+	job_desc_msg_t *job_desc;
+	job_parse_list_t *rc = j->rc;
+
+	if ((job_desc = _parse_job_desc(data, j->errors, j->update_only))) {
+		if (j->script) {
+			//assign script to first job of het jobs
+			job_desc->script = j->script;
+			j->script = NULL;
+		}
+
+		list_append(rc->jobs, job_desc);
+	} else { /* parsing failed */
+		rc->rc = resp_error(j->errors, ESLURM_REST_FAIL_PARSING,
+				    "_parse_job_desc",
+				    "unexpected failure parsing het job: %zd",
+				    j->i);
+		return DATA_FOR_EACH_FAIL;
+	}
+
+	(j->i)++;
+	return DATA_FOR_EACH_CONT;
+}
+
+static job_parse_list_t _parse_job_list(const data_t *jobs, char *script,
+					data_t *errors, bool update_only)
+{
+	job_parse_list_t rc = { 0 };
+	xassert(update_only || script);
+
+	if (jobs == NULL)
+		rc.rc = ESLURM_REST_INVALID_JOBS_DESC;
+	else if (data_get_type(jobs) == DATA_TYPE_LIST) {
+		_parse_job_component_t j = {
+			.rc = &rc,
+			.script = script,
+			.update_only = update_only,
+			.errors = errors,
+		};
+
+		rc.het_job = true;
+		rc.jobs = list_create(_list_delete_job_desc_msg_t);
+		rc.rc = SLURM_SUCCESS;
+
+		data_list_for_each_const(jobs, _parse_job_component, &j);
+
+		if (rc.rc)
+			FREE_NULL_LIST(rc.jobs);
+	} else if (data_get_type(jobs) == DATA_TYPE_DICT) {
+		rc.het_job = false;
+		rc.job = _parse_job_desc(jobs, errors, update_only);
+
+		if (rc.job) {
+			rc.job->script = script;
+			rc.rc = SLURM_SUCCESS;
+		} else
+			rc.rc = ESLURM_REST_FAIL_PARSING;
+	} else
+		rc.rc = ESLURM_REST_INVALID_JOBS_DESC;
+
+	return rc;
+}
+
+static void _dump_node_res(data_t *dnodes, job_resources_t *j,
+			   const size_t node_inx, const char *nodename,
+			   const size_t sock_inx, size_t *bit_inx,
+			   const size_t array_size)
+{
+	size_t bit_reps;
+	data_t *dnode = data_set_dict(data_list_append(dnodes));
+	data_t *dsockets = data_set_dict(data_key_set(dnode, "sockets"));
+	data_t **sockets;
+
+	sockets = xcalloc(j->sockets_per_node[sock_inx], sizeof(*sockets));
+
+	/* per node */
+
+	data_set_string(data_key_set(dnode, "nodename"), nodename);
+
+	data_set_int(data_key_set(dnode, "cpus_used"), j->cpus_used[node_inx]);
+	data_set_int(data_key_set(dnode, "memory_used"),
+		     j->memory_used[node_inx]);
+	data_set_int(data_key_set(dnode, "memory_allocated"),
+		     j->memory_allocated[node_inx]);
+
+	/* set the used cores as found */
+
+	bit_reps = j->sockets_per_node[sock_inx] *
+		   j->cores_per_socket[sock_inx];
+	for (size_t i = 0; i < bit_reps; i++) {
+		size_t socket_inx = i / j->cores_per_socket[sock_inx];
+		size_t core_inx = i % j->cores_per_socket[sock_inx];
+
+		xassert(*bit_inx < array_size);
+
+		if (*bit_inx >= array_size) {
+			error("%s: unexpected invalid bit index:%zu/%zu",
+			      __func__, *bit_inx, array_size);
+			break;
+		}
+
+		if (bit_test(j->core_bitmap, *bit_inx)) {
+			data_t *dcores;
+
+			if (!sockets[socket_inx]) {
+				sockets[socket_inx] = data_set_dict(
+					data_key_set_int(dsockets, socket_inx));
+				dcores = data_set_dict(data_key_set(
+					sockets[socket_inx], "cores"));
+			} else {
+				dcores = data_key_get(sockets[socket_inx],
+						      "cores");
+			}
+
+			if (bit_test(j->core_bitmap_used, *bit_inx)) {
+				data_set_string(data_key_set_int(dcores,
+								 core_inx),
+						"allocated_and_in_use");
+			} else {
+				data_set_string(data_key_set_int(dcores,
+								 core_inx),
+						"allocated");
+			}
+		}
+
+		(*bit_inx)++;
+	}
+
+	xfree(sockets);
+}
+
+/* log_job_resources() used as an example */
+static void _dump_nodes_res(data_t *dnodes, job_resources_t *j)
+{
+	hostlist_t hl = NULL;
+	size_t bit_inx = 0;
+	size_t array_size;
+	size_t sock_inx = 0, sock_reps = 0;
+
+	if (!j->cores_per_socket || !j->nhosts) {
+		/* not enough info present */
+		return;
+	}
+
+	hl = hostlist_create(j->nodes);
+	array_size = bit_size(j->core_bitmap);
+
+	for (size_t node_inx = 0; node_inx < j->nhosts; node_inx++) {
+		char *nodename = hostlist_nth(hl, node_inx);
+
+		if (sock_reps >= j->sock_core_rep_count[sock_inx]) {
+			sock_inx++;
+			sock_reps = 0;
+		}
+		sock_reps++;
+
+		_dump_node_res(dnodes, j, node_inx, nodename, sock_inx,
+			       &bit_inx, array_size);
+
+		free(nodename);
+	}
+
+	FREE_NULL_HOSTLIST(hl);
+}
+
+static data_t *dump_job_info(slurm_job_info_t *job, data_t *jd)
+{
+	xassert(data_get_type(jd) == DATA_TYPE_NULL);
+	data_set_dict(jd);
+	data_set_string(data_key_set(jd, "account"), job->account);
+	data_set_int(data_key_set(jd, "accrue_time"), job->accrue_time);
+	data_set_string(data_key_set(jd, "admin_comment"), job->admin_comment);
+	/* alloc_node intentionally skipped */
+	data_set_int(data_key_set(jd, "array_job_id"), job->array_job_id);
+	if (job->array_task_id == NO_VAL)
+		data_set_null(data_key_set(jd, "array_task_id"));
+	else
+		data_set_int(data_key_set(jd, "array_task_id"),
+			     job->array_task_id);
+	data_set_int(data_key_set(jd, "array_max_tasks"), job->array_max_tasks);
+	data_set_string(data_key_set(jd, "array_task_string"),
+			job->array_task_str);
+	data_set_int(data_key_set(jd, "association_id"), job->assoc_id);
+	data_set_string(data_key_set(jd, "batch_features"),
+			job->batch_features);
+	data_set_bool(data_key_set(jd, "batch_flag"), job->batch_flag == 1);
+	data_set_string(data_key_set(jd, "batch_host"), job->batch_host);
+	data_t *bitflags = data_key_set(jd, "flags");
+	data_set_list(bitflags);
+	if (job->bitflags & KILL_INV_DEP)
+		data_set_string(data_list_append(bitflags), "KILL_INV_DEP");
+	if (job->bitflags & NO_KILL_INV_DEP)
+		data_set_string(data_list_append(bitflags), "NO_KILL_INV_DEP");
+	if (job->bitflags & HAS_STATE_DIR)
+		data_set_string(data_list_append(bitflags), "HAS_STATE_DIR");
+	if (job->bitflags & BACKFILL_TEST)
+		data_set_string(data_list_append(bitflags), "BACKFILL_TEST");
+	if (job->bitflags & GRES_ENFORCE_BIND)
+		data_set_string(data_list_append(bitflags),
+				"GRES_ENFORCE_BIND");
+	if (job->bitflags & TEST_NOW_ONLY)
+		data_set_string(data_list_append(bitflags), "TEST_NOW_ONLY");
+	if (job->bitflags & SPREAD_JOB)
+		data_set_string(data_list_append(bitflags), "SPREAD_JOB");
+	if (job->bitflags & USE_MIN_NODES)
+		data_set_string(data_list_append(bitflags), "USE_MIN_NODES");
+	if (job->bitflags & JOB_KILL_HURRY)
+		data_set_string(data_list_append(bitflags), "JOB_KILL_HURRY");
+	if (job->bitflags & TRES_STR_CALC)
+		data_set_string(data_list_append(bitflags), "TRES_STR_CALC");
+	if (job->bitflags & SIB_JOB_FLUSH)
+		data_set_string(data_list_append(bitflags), "SIB_JOB_FLUSH");
+	if (job->bitflags & HET_JOB_FLAG)
+		data_set_string(data_list_append(bitflags), "HET_JOB_FLAG");
+	if (job->bitflags & JOB_CPUS_SET)
+		data_set_string(data_list_append(bitflags), "JOB_CPUS_SET");
+	if (job->bitflags & TOP_PRIO_TMP)
+		data_set_string(data_list_append(bitflags), "TOP_PRIO_TMP");
+	if (job->bitflags & JOB_ACCRUE_OVER)
+		data_set_string(data_list_append(bitflags), "JOB_ACCRUE_OVER");
+	if (job->bitflags & GRES_DISABLE_BIND)
+		data_set_string(data_list_append(bitflags),
+				"GRES_DISABLE_BIND");
+	if (job->bitflags & JOB_WAS_RUNNING)
+		data_set_string(data_list_append(bitflags), "JOB_WAS_RUNNING");
+	if (job->bitflags & JOB_MEM_SET)
+		data_set_string(data_list_append(bitflags), "JOB_MEM_SET");
+	if (job->bitflags & JOB_RESIZED)
+		data_set_string(data_list_append(bitflags), "JOB_RESIZED");
+	/* boards_per_node intentionally omitted */
+	data_set_string(data_key_set(jd, "burst_buffer"), job->burst_buffer);
+	data_set_string(data_key_set(jd, "burst_buffer_state"),
+			job->burst_buffer_state);
+	data_set_string(data_key_set(jd, "cluster"), job->cluster);
+	data_set_string(data_key_set(jd, "cluster_features"),
+			job->cluster_features);
+	data_set_string(data_key_set(jd, "command"), job->command);
+	data_set_string(data_key_set(jd, "comment"), job->comment);
+	//data_set_string(data_key_set(jd, "container"), job->container);
+	if (job->contiguous != NO_VAL16)
+		data_set_bool(data_key_set(jd, "contiguous"),
+			      job->contiguous == 1);
+	else
+		data_set_null(data_key_set(jd, "contiguous"));
+	if (job->core_spec == NO_VAL16) {
+		data_set_null(data_key_set(jd, "core_spec"));
+		data_set_null(data_key_set(jd, "thread_spec"));
+	} else {
+		if (CORE_SPEC_THREAD & job->core_spec) {
+			data_set_int(data_key_set(jd, "core_spec"),
+				     job->core_spec);
+			data_set_null(data_key_set(jd, "thread_spec"));
+		} else {
+			data_set_int(data_key_set(jd, "thread_spec"),
+				     (job->core_spec & ~CORE_SPEC_THREAD));
+			data_set_null(data_key_set(jd, "core_spec"));
+		}
+	}
+	if (job->cores_per_socket == NO_VAL16)
+		data_set_null(data_key_set(jd, "cores_per_socket"));
+	else
+		data_set_int(data_key_set(jd, "cores_per_socket"),
+			     job->cores_per_socket);
+	//skipped cpu_bind and cpu_bind_type per description
+	if (job->billable_tres == (double)NO_VAL)
+		data_set_null(data_key_set(jd, "billable_tres"));
+	else
+		data_set_float(data_key_set(jd, "billable_tres"),
+			       job->billable_tres);
+	if (job->cpu_freq_min == NO_VAL)
+		data_set_null(data_key_set(jd, "cpus_per_task"));
+	else
+		data_set_int(data_key_set(jd, "cpus_per_task"),
+			     job->cpus_per_task);
+	if (job->cpu_freq_min == NO_VAL)
+		data_set_null(data_key_set(jd, "cpu_frequency_minimum"));
+	else
+		data_set_int(data_key_set(jd, "cpu_frequency_minumum"),
+			     job->cpu_freq_min);
+	if (job->cpu_freq_max == NO_VAL)
+		data_set_null(data_key_set(jd, "cpu_frequency_maximum"));
+	else
+		data_set_int(data_key_set(jd, "cpu_frequency_maximum"),
+			     job->cpu_freq_max);
+	if (job->cpu_freq_gov == NO_VAL)
+		data_set_null(data_key_set(jd, "cpu_frequency_governor"));
+	else
+		data_set_int(data_key_set(jd, "cpu_frequency_governor"),
+			     job->cpu_freq_gov);
+	data_set_string(data_key_set(jd, "cpus_per_tres"), job->cpus_per_tres);
+	data_set_int(data_key_set(jd, "deadline"), job->deadline);
+	if (job->delay_boot == NO_VAL)
+		data_set_null(data_key_set(jd, "delay_boot"));
+	else
+		data_set_int(data_key_set(jd, "delay_boot"), job->delay_boot);
+	data_set_string(data_key_set(jd, "dependency"), job->dependency);
+	data_set_int(data_key_set(jd, "derived_exit_code"), job->derived_ec);
+	data_set_int(data_key_set(jd, "eligible_time"), job->eligible_time);
+	data_set_int(data_key_set(jd, "end_time"), job->end_time);
+	data_set_string(data_key_set(jd, "excluded_nodes"), job->exc_nodes);
+	/* exc_node_inx intentionally omitted */
+	data_set_int(data_key_set(jd, "exit_code"), job->exit_code);
+	data_set_string(data_key_set(jd, "features"), job->features);
+	data_set_string(data_key_set(jd, "federation_origin"),
+			job->fed_origin_str);
+	data_set_string(data_key_set(jd, "federation_siblings_active"),
+			job->fed_siblings_active_str);
+	data_set_string(data_key_set(jd, "federation_siblings_viable"),
+			job->fed_siblings_viable_str);
+	data_t *gres_detail = data_key_set(jd, "gres_detail");
+	data_set_list(gres_detail);
+	for (size_t i = 0; i < job->gres_detail_cnt; ++i)
+		data_set_string(data_list_append(gres_detail),
+				job->gres_detail_str[i]);
+	if (job->group_id == NO_VAL)
+		data_set_null(data_key_set(jd, "group_id"));
+	else {
+		data_set_int(data_key_set(jd, "group_id"), job->group_id);
+		data_set_string_own(
+			data_key_set(jd, "group_name"),
+			gid_to_string_or_null((gid_t) job->group_id));
+	}
+	if (job->job_id == NO_VAL)
+		data_set_null(data_key_set(jd, "job_id"));
+	else
+		data_set_int(data_key_set(jd, "job_id"), job->job_id);
+
+	if (job->job_resrcs) {
+		/* based on log_job_resources() */
+		data_t *jrsc = data_set_dict(data_key_set(jd, "job_resources"));
+		job_resources_t *j = job->job_resrcs;
+		data_set_string(data_key_set(jrsc, "nodes"), j->nodes);
+
+		if (slurm_conf.select_type_param & (CR_CORE|CR_SOCKET))
+			data_set_int(data_key_set(jrsc, "allocated_cores"),
+				     j->ncpus);
+		else if (slurm_conf.select_type_param & CR_CPU)
+			data_set_int(data_key_set(jrsc, "allocated_cpus"),
+				     j->ncpus);
+
+		data_set_int(data_key_set(jrsc, "allocated_hosts"), j->nhosts);
+
+		_dump_nodes_res(
+			data_set_list(data_key_set(jrsc, "allocated_nodes")),
+			j);
+	}
+	data_set_string(data_key_set(jd, "job_state"),
+			job_state_string(job->job_state));
+	data_set_int(data_key_set(jd, "last_sched_evaluation"), job->last_sched_eval);
+	data_set_string(data_key_set(jd, "licenses"), job->licenses);
+	if (job->max_cpus == NO_VAL)
+		data_set_null(data_key_set(jd, "max_cpus"));
+	else
+		data_set_int(data_key_set(jd, "max_cpus"), job->max_cpus);
+	if (job->max_nodes == NO_VAL)
+		data_set_null(data_key_set(jd, "max_nodes"));
+	else
+		data_set_int(data_key_set(jd, "max_nodes"), job->max_nodes);
+	data_set_string(data_key_set(jd, "mcs_label"), job->mcs_label);
+	data_set_string(data_key_set(jd, "memory_per_tres"), job->mem_per_tres);
+	data_set_string(data_key_set(jd, "name"), job->name);
+	/* network intentionally omitted */
+	data_set_string(data_key_set(jd, "nodes"), job->nodes);
+	if (job->nice == NO_VAL || job->nice == NICE_OFFSET)
+		data_set_null(data_key_set(jd, "nice"));
+	else
+		data_set_int(data_key_set(jd, "nice"), job->nice - NICE_OFFSET);
+	/* node_index intentionally omitted */
+	if (job->ntasks_per_core == NO_VAL16 ||
+	    job->ntasks_per_core == INFINITE16)
+		data_set_null(data_key_set(jd, "tasks_per_core"));
+	else
+		data_set_int(data_key_set(jd, "tasks_per_core"),
+			     job->ntasks_per_core);
+	data_set_int(data_key_set(jd, "tasks_per_node"), job->ntasks_per_node);
+	if (job->ntasks_per_socket == NO_VAL16 ||
+	    job->ntasks_per_socket == INFINITE16)
+		data_set_null(data_key_set(jd, "tasks_per_socket"));
+	else
+		data_set_int(data_key_set(jd, "tasks_per_socket"),
+			     job->ntasks_per_socket);
+	data_set_int(data_key_set(jd, "tasks_per_board"),
+		     job->ntasks_per_board);
+	if (job->num_tasks != NO_VAL && job->num_tasks != INFINITE)
+		data_set_int(data_key_set(jd, "cpus"), job->num_cpus);
+	else
+		data_set_null(data_key_set(jd, "cpus"));
+	data_set_int(data_key_set(jd, "node_count"), job->num_nodes);
+	if (job->num_tasks != NO_VAL && job->num_tasks != INFINITE)
+		data_set_int(data_key_set(jd, "tasks"), job->num_tasks);
+	else
+		data_set_null(data_key_set(jd, "tasks"));
+	data_set_int(data_key_set(jd, "het_job_id"), job->het_job_id);
+	data_set_string(data_key_set(jd, "het_job_id_set"),
+			job->het_job_id_set);
+	data_set_int(data_key_set(jd, "het_job_offset"), job->het_job_offset);
+	data_set_string(data_key_set(jd, "partition"), job->partition);
+	//data_set_string(data_key_set(jd, "prefer"), job->prefer);
+	if (job->pn_min_memory & MEM_PER_CPU) {
+		data_set_null(data_key_set(jd, "memory_per_node"));
+		data_set_int(data_key_set(jd, "memory_per_cpu"),
+			     (job->pn_min_memory & ~MEM_PER_CPU));
+	} else if (job->pn_min_memory) {
+		data_set_int(data_key_set(jd, "memory_per_node"),
+			     job->pn_min_memory);
+		data_set_null(data_key_set(jd, "memory_per_cpu"));
+	} else {
+		data_set_null(data_key_set(jd, "memory_per_node"));
+		data_set_null(data_key_set(jd, "memory_per_cpu"));
+	}
+	data_set_int(data_key_set(jd, "minimum_cpus_per_node"), job->pn_min_cpus);
+	data_set_int(data_key_set(jd, "minimum_tmp_disk_per_node"),
+		     job->pn_min_tmp_disk);
+	/* power_flags intentionally omitted */
+	data_set_int(data_key_set(jd, "preempt_time"), job->preempt_time);
+	data_set_int(data_key_set(jd, "pre_sus_time"), job->pre_sus_time);
+	if (job->priority == NO_VAL || job->priority == INFINITE)
+		data_set_null(data_key_set(jd, "priority"));
+	else
+		data_set_int(data_key_set(jd, "priority"), job->priority);
+	if (job->profile == ACCT_GATHER_PROFILE_NOT_SET)
+		data_set_null(data_key_set(jd, "profile"));
+	else {
+		//based on acct_gather_profile_to_string
+		data_t *profile = data_key_set(jd, "profile");
+		data_set_list(profile);
+
+		if (job->profile == ACCT_GATHER_PROFILE_NONE)
+			data_set_string(data_list_append(profile), "None");
+		if (job->profile & ACCT_GATHER_PROFILE_ENERGY)
+			data_set_string(data_list_append(profile), "Energy");
+		if (job->profile & ACCT_GATHER_PROFILE_LUSTRE)
+			data_set_string(data_list_append(profile), "Lustre");
+		if (job->profile & ACCT_GATHER_PROFILE_NETWORK)
+			data_set_string(data_list_append(profile), "Network");
+		if (job->profile & ACCT_GATHER_PROFILE_TASK)
+			data_set_string(data_list_append(profile), "Task");
+	}
+	data_set_string(data_key_set(jd, "qos"), job->qos);
+	data_set_bool(data_key_set(jd, "reboot"), job->reboot);
+	data_set_string(data_key_set(jd, "required_nodes"), job->req_nodes);
+	/* skipping req_node_inx */
+	data_set_bool(data_key_set(jd, "requeue"), job->requeue);
+	data_set_int(data_key_set(jd, "resize_time"), job->resize_time);
+	data_set_int(data_key_set(jd, "restart_cnt"), job->restart_cnt);
+	data_set_string(data_key_set(jd, "resv_name"), job->resv_name);
+	/* sched_nodes intentionally omitted */
+	/* select_jobinfo intentionally omitted */
+	switch (job->shared) {
+	case JOB_SHARED_NONE:
+		data_set_string(data_key_set(jd, "shared"), "none");
+		break;
+	case JOB_SHARED_OK:
+		data_set_string(data_key_set(jd, "shared"), "shared");
+		break;
+	case JOB_SHARED_USER:
+		data_set_string(data_key_set(jd, "shared"), "user");
+		break;
+	case JOB_SHARED_MCS:
+		data_set_string(data_key_set(jd, "shared"), "mcs");
+		break;
+	case NO_VAL16:
+		data_set_null(data_key_set(jd, "shared"));
+		break;
+	default:
+		data_set_int(data_key_set(jd, "shared"), job->shared);
+		xassert(false);
+		break;
+	}
+	data_t *sflags = data_key_set(jd, "show_flags");
+	data_set_list(sflags);
+	if (job->show_flags & SHOW_ALL)
+		data_set_string(data_list_append(sflags), "SHOW_ALL");
+	if (job->show_flags & SHOW_DETAIL)
+		data_set_string(data_list_append(sflags), "SHOW_DETAIL");
+	if (job->show_flags & SHOW_MIXED)
+		data_set_string(data_list_append(sflags), "SHOW_MIXED");
+	if (job->show_flags & SHOW_LOCAL)
+		data_set_string(data_list_append(sflags), "SHOW_LOCAL");
+	if (job->show_flags & SHOW_SIBLING)
+		data_set_string(data_list_append(sflags), "SHOW_SIBLING");
+	if (job->show_flags & SHOW_FEDERATION)
+		data_set_string(data_list_append(sflags), "SHOW_FEDERATION");
+	if (job->show_flags & SHOW_FUTURE)
+		data_set_string(data_list_append(sflags), "SHOW_FUTURE");
+	data_set_int(data_key_set(jd, "sockets_per_board"),
+		     job->sockets_per_board);
+	if (job->sockets_per_node == NO_VAL16)
+		data_set_null(data_key_set(jd, "sockets_per_node"));
+	else
+		data_set_int(data_key_set(jd, "sockets_per_node"),
+			     job->sockets_per_node);
+	data_set_int(data_key_set(jd, "start_time"), job->start_time);
+	/* start_protocol_ver intentionally omitted */
+	data_set_string(data_key_set(jd, "state_description"), job->state_desc);
+	data_set_string(data_key_set(jd, "state_reason"),
+			job_reason_string(job->state_reason));
+	data_set_string(data_key_set(jd, "standard_error"), job->std_err);
+	data_set_string(data_key_set(jd, "standard_input"), job->std_in);
+	data_set_string(data_key_set(jd, "standard_output"), job->std_out);
+	data_set_int(data_key_set(jd, "submit_time"), job->submit_time);
+	data_set_int(data_key_set(jd, "suspend_time"), job->suspend_time);
+	data_set_string(data_key_set(jd, "system_comment"),
+			job->system_comment);
+	//data_set_string(data_key_set(jd, "container"), job->container);
+	if (job->time_limit != INFINITE)
+		data_set_int(data_key_set(jd, "time_limit"), job->time_limit);
+	else
+		data_set_null(data_key_set(jd, "time_limit"));
+	if (job->time_min != INFINITE)
+		data_set_int(data_key_set(jd, "time_minimum"), job->time_min);
+	else
+		data_set_null(data_key_set(jd, "time_minimum"));
+	if (job->threads_per_core == NO_VAL16)
+		data_set_null(data_key_set(jd, "threads_per_core"));
+	else
+		data_set_int(data_key_set(jd, "threads_per_core"),
+			     job->threads_per_core);
+	data_set_string(data_key_set(jd, "tres_bind"), job->tres_bind);
+	data_set_string(data_key_set(jd, "tres_freq"), job->tres_freq);
+	data_set_string(data_key_set(jd, "tres_per_job"), job->tres_per_job);
+	data_set_string(data_key_set(jd, "tres_per_node"), job->tres_per_node);
+	data_set_string(data_key_set(jd, "tres_per_socket"),
+			job->tres_per_socket);
+	data_set_string(data_key_set(jd, "tres_per_task"), job->tres_per_task);
+	data_set_string(data_key_set(jd, "tres_req_str"), job->tres_req_str);
+	data_set_string(data_key_set(jd, "tres_alloc_str"),
+			job->tres_alloc_str);
+	data_set_int(data_key_set(jd, "user_id"), job->user_id);
+	if (job->user_name) {
+		data_set_string(data_key_set(jd, "user_name"), job->user_name);
+	} else {
+		data_set_string_own(
+			data_key_set(jd, "user_name"),
+			uid_to_string_or_null((uid_t) job->user_id));
+	}
+	/* wait4switch intentionally omitted */
+	data_set_string(data_key_set(jd, "wckey"), job->wckey);
+	data_set_string(data_key_set(jd, "current_working_directory"),
+			job->work_dir);
+
+	return jd;
+}
+
+static int _op_handler_jobs(const char *context_id,
+			    http_request_method_t method, data_t *parameters,
+			    data_t *query, int tag, data_t *resp, rest_auth_context_t *auth)
+{
+	int rc = SLURM_SUCCESS;
+	job_info_msg_t *job_info_ptr = NULL;
+	data_t *errors = populate_response_format(resp);
+	data_t *jobs = data_set_list(data_key_set(resp, "jobs"));
+	time_t update_time = 0; /* default to unix epoch */
+
+	debug4("%s: jobs handler called by %s", __func__, context_id);
+
+	if ((rc = get_date_param(query, "update_time", &update_time)))
+	    goto done;
+
+	rc = slurm_load_jobs(update_time, &job_info_ptr,
+			     SHOW_ALL | SHOW_DETAIL);
+
+	if (rc == SLURM_NO_CHANGE_IN_DATA) {
+		/* no-op: nothing to do here */
+	} else if ((rc == SLURM_SUCCESS) && job_info_ptr &&
+		   job_info_ptr->record_count) {
+		for (size_t i = 0; i < job_info_ptr->record_count; ++i) {
+			dump_job_info(job_info_ptr->job_array + i,
+				      data_list_append(jobs));
+		}
+	} else if (rc) {
+		resp_error(errors, rc, "slurm_load_jobs",
+			   "Failed while looking for jobs");
+	}
+
+done:
+	slurm_free_job_info_msg(job_info_ptr);
+
+	return rc;
+}
+
+static int _handle_job_get(const char *context_id, http_request_method_t method,
+			   data_t *parameters, data_t *query, int tag,
+			   data_t *resp, const char *job_id_str,
+			   data_t *const errors)
+{
+	int rc = SLURM_SUCCESS;
+	job_info_msg_t *job_info_ptr = NULL;
+	uint32_t job_id = slurm_xlate_job_id((char *) job_id_str);
+
+	if (!job_id) {
+		rc = ESLURM_REST_INVALID_JOBS_DESC;
+		resp_error(errors, rc, "_handle_job_get",
+			   "Unable to find JobId=%s", job_id_str);
+		return rc;
+	}
+
+	rc = slurm_load_job(&job_info_ptr, job_id, SHOW_ALL|SHOW_DETAIL);
+	data_t *jobs = data_set_list(data_key_set(resp, "jobs"));
+
+	if (!rc && job_info_ptr && job_info_ptr->record_count) {
+		for (size_t i = 0; i < job_info_ptr->record_count; ++i) {
+			dump_job_info(job_info_ptr->job_array + i,
+				      data_list_append(jobs));
+		}
+	} else {
+		resp_error(errors, rc, "slurm_load_job",
+			   "Unable to find JobId=%s", job_id_str);
+	}
+
+	slurm_free_job_info_msg(job_info_ptr);
+
+	return rc;
+}
+
+static int _handle_job_delete(const char *context_id,
+			      http_request_method_t method,
+			      data_t *parameters, data_t *query, int tag,
+			      data_t *resp, const char *job_id,
+			      data_t *const errors, const int signal)
+{
+	if (slurm_kill_job2(job_id, signal, KILL_FULL_JOB)) {
+		/* Already signaled jobs are considered a success here */
+		if (errno == ESLURM_ALREADY_DONE)
+			return SLURM_SUCCESS;
+
+		return resp_error(errors, errno,"slurm_kill_job",
+				  "unable to kill JobId=%s with signal %d: %s",
+				  job_id, signal, slurm_strerror(errno));
+	}
+
+	return SLURM_SUCCESS;
+}
+
+static int _handle_job_post(const char *context_id,
+			    http_request_method_t method, data_t *parameters,
+			    data_t *query, int tag, data_t *resp,
+			    const char *job_id, data_t *const errors)
+{
+	int rc = SLURM_SUCCESS;
+	job_parse_list_t jobs_rc;
+
+	if (get_log_level() >= LOG_LEVEL_DEBUG5) {
+		char *buffer = NULL;
+
+		//data_g_serialize(&buffer, query, MIME_TYPE_JSON,
+		//		 DATA_SER_FLAGS_COMPACT);
+		debug5("%s: job update from %s: %s",
+		       __func__, context_id, buffer);
+		xfree(buffer);
+	}
+
+	jobs_rc = _parse_job_list(query, NULL, errors, true);
+
+	if (jobs_rc.rc) {
+		resp_error(errors, jobs_rc.rc, "_parse_job_list",
+			   "job parsing failed for %s", context_id);
+	} else {
+		debug3("%s: job parsing successful for %s",
+		       __func__, context_id);
+		rc = jobs_rc.rc;
+		if (jobs_rc.het_job) {
+			resp_error(errors, rc, "_parse_job_list",
+				   "unexpected het job request from %s",
+				   context_id);
+		} else {
+			job_array_resp_msg_t *resp = NULL;
+			errno = 0;
+			jobs_rc.job->job_id_str = xstrdup(job_id);
+			debug5("%s: sending JobId=%s update for %s",
+			       __func__, job_id, context_id);
+
+			rc = slurm_update_job2(jobs_rc.job, &resp);
+
+			if (rc) {
+				resp_error(errors, errno, "_parse_job_list",
+					   "job update from %s failed",
+					   context_id);
+			} else if (resp && resp->error_code &&
+				   resp->error_code) {
+				resp_error(errors, *resp->error_code,
+					   "_parse_job_list",
+					   "job array update from %s failed",
+					   context_id);
+			}
+
+			slurm_free_job_desc_msg(jobs_rc.job);
+			slurm_free_job_array_resp(resp);
+		}
+	}
+
+	return rc;
+}
+
+static int _op_handler_job(const char *context_id, http_request_method_t method,
+			   data_t *parameters, data_t *query, int tag,
+			   data_t *resp, rest_auth_context_t *auth)
+{
+	int rc = SLURM_SUCCESS;
+	data_t *data_jobid;
+
+	const char *job_id_str = NULL;
+	data_t *errors = populate_response_format(resp);
+	debug4("%s: job handler %s called by %s with tag %d",
+	       __func__, get_http_method_string(method), context_id, tag);
+
+	if (!parameters) {
+		return resp_error(errors, ESLURM_REST_INVALID_QUERY,
+				  "HTTP request",
+				  "[%s] missing request parameters",
+				  context_id);
+	} else if (!(data_jobid = data_key_get(parameters, "job_id"))) {
+		return resp_error(errors, ESLURM_REST_INVALID_QUERY,
+				  "HTTP request",
+				  "[%s] missing job_id in parameters",
+				  context_id);
+	} else if (data_convert_type(data_jobid, DATA_TYPE_STRING) !=
+		   DATA_TYPE_STRING) {
+		return resp_error(errors, ESLURM_REST_INVALID_QUERY,
+				  "HTTP request",
+				  "[%s] job_id is invalid",
+				  context_id);
+	} else {
+		job_id_str = data_get_string(data_jobid);
+	}
+
+	if (rc) {
+		/* do nothing */
+	} else if (!job_id_str || !job_id_str[0]) {
+		return resp_error(errors, ESLURM_REST_INVALID_QUERY,
+				  "HTTP request",
+				  "[%s] job_id is empty",
+				  context_id);
+	} else if (tag == URL_TAG_JOB && method == HTTP_REQUEST_GET) {
+		rc = _handle_job_get(context_id, method, parameters, query, tag,
+				     resp, job_id_str, errors);
+	} else if (tag == URL_TAG_JOB &&
+		   method == HTTP_REQUEST_DELETE) {
+		int signal = 0;
+		data_t *dsignal = data_key_get(query, "signal");
+
+		if (data_get_type(dsignal) == DATA_TYPE_INT_64)
+			signal = data_get_int(dsignal);
+		else if (data_get_type(dsignal) == DATA_TYPE_STRING)
+			signal = sig_name2num(data_get_string(dsignal));
+		else
+			signal = SIGKILL;
+
+		if (signal < 1 || signal >= SIGRTMAX) {
+			rc = resp_error(errors, ESLURM_REST_INVALID_QUERY,
+					"HTTP request: signal",
+					"invalid signal: %d", signal);
+		} else {
+			rc = _handle_job_delete(context_id, method, parameters,
+						query, tag, resp, job_id_str,
+						errors, signal);
+		}
+	} else if (tag == URL_TAG_JOB &&
+		   method == HTTP_REQUEST_POST) {
+		rc = _handle_job_post(context_id, method, parameters, query,
+				      tag, resp, job_id_str, errors);
+	} else {
+		rc = resp_error(errors, ESLURM_REST_INVALID_QUERY,
+				"HTTP request", "%s: unknown request",
+				context_id);
+	}
+
+	return rc;
+}
+
+static int _op_handler_submit_job_post(const char *context_id,
+				       http_request_method_t method,
+				       data_t *parameters, data_t *query,
+				       int tag, data_t *d, data_t *errors)
+{
+	int rc = SLURM_SUCCESS;
+	submit_response_msg_t *resp = NULL;
+	char *script = NULL;
+
+	if (!query) {
+		error("%s: [%s] unexpected empty query for job",
+		      __func__, context_id);
+		rc = ESLURM_REST_INVALID_QUERY;
+		goto finish;
+	}
+
+	if (get_log_level() >= LOG_LEVEL_DEBUG5) {
+		char *buffer = NULL;
+
+		//data_g_serialize(&buffer, query, MIME_TYPE_JSON,
+		//		 DATA_SER_FLAGS_COMPACT);
+		debug5("%s: job submit query from %s: %s",
+		       __func__, context_id, buffer);
+		xfree(buffer);
+
+		//data_g_serialize(&buffer, parameters, MIME_TYPE_JSON,
+		//		 DATA_SER_FLAGS_COMPACT);
+		debug5("%s: job submit parameters from %s: %s",
+		       __func__, context_id, buffer);
+		xfree(buffer);
+	}
+
+	if (data_retrieve_dict_path_string(query, "script", &script)) {
+		error("%s: unexpected missing script for job from %s",
+		      __func__, context_id);
+		rc = ESLURM_JOB_SCRIPT_MISSING;
+		goto finish;
+	}
+
+	if (!rc) {
+		job_parse_list_t jobs_rc = { 0 };
+		data_t *jobs = data_key_get(query, "job");
+
+		if (!jobs)
+			/* allow jobs too since we can take a list of jobs */
+			jobs = data_key_get(query, "jobs");
+
+		if (!jobs) {
+			error("%s: [%s] missing job specification field",
+			      __func__, context_id);
+			rc = ESLURM_REST_INVALID_JOBS_DESC;
+		} else if ((jobs_rc = _parse_job_list(jobs, script, errors,
+						      false))
+				   .rc) {
+			error("%s: job parsing failed for %s",
+			      __func__, context_id);
+			rc = jobs_rc.rc;
+		} else {
+			debug3("%s: job parsing successful for %s",
+			       __func__, context_id);
+			rc = jobs_rc.rc;
+			if (jobs_rc.het_job) {
+				if (slurm_submit_batch_het_job(jobs_rc.jobs,
+							       &resp))
+					rc = errno;
+				list_destroy(jobs_rc.jobs);
+			} else {
+				if (slurm_submit_batch_job(jobs_rc.job, &resp))
+					rc = errno;
+				slurm_free_job_desc_msg(jobs_rc.job);
+			}
+		}
+	}
+
+	if (!rc) {
+		xassert(resp);
+		debug5("%s: job_id:%d step_id:%d error_code:%d message: %s for job submission from %s",
+		       __func__, resp->job_id, resp->step_id, resp->error_code,
+		       resp->job_submit_user_msg, context_id);
+
+		data_set_int(data_key_set(d, "job_id"), resp->job_id);
+		switch (resp->step_id) {
+		case SLURM_PENDING_STEP:
+			data_set_string(data_key_set(d, "step_id"),
+					"PENDING");
+			break;
+		case SLURM_BATCH_SCRIPT:
+			data_set_string(data_key_set(d, "step_id"),
+					"BATCH");
+			break;
+		case SLURM_EXTERN_CONT:
+			data_set_string(data_key_set(d, "step_id"),
+					"EXTERN");
+			break;
+		case SLURM_INTERACTIVE_STEP:
+			data_set_string(data_key_set(d, "step_id"),
+					"INTERACTIVE");
+			break;
+		default:
+			data_set_int(data_key_set(d, "step_id"),
+				     resp->step_id);
+			break;
+		}
+		if (resp->error_code) {
+			data_t *error = data_list_append(errors);
+			data_set_dict(error);
+			data_set_int(data_key_set(error, "error_code"),
+				     resp->error_code);
+			data_set_string(data_key_set(error, "error"),
+					slurm_strerror(resp->error_code));
+		}
+		data_set_string(data_key_set(d, "job_submit_user_msg"),
+				resp->job_submit_user_msg);
+	}
+
+finish:
+	if (rc) {
+		data_t *error = data_set_dict(data_list_append(errors));
+		data_set_int(data_key_set(error, "error_code"), rc);
+		data_set_string(data_key_set(error, "error"),
+				slurm_strerror(rc));
+
+		debug5("%s: [%s] job submission failed with %d: %s",
+		       __func__, context_id, rc, slurm_strerror(rc));
+	}
+
+	slurm_free_submit_response_response_msg(resp);
+
+	return rc;
+}
+
+static int _op_handler_submit_job(const char *context_id,
+				  http_request_method_t method,
+				  data_t *parameters, data_t *query,
+				  int tag, data_t *resp, rest_auth_context_t *auth)
+{
+	int rc = SLURM_SUCCESS;
+	data_t *errors = populate_response_format(resp);
+
+	debug4("%s: job submit handler %s called by %s with tag %d",
+	       __func__, get_http_method_string(method), context_id, tag);
+
+	if (tag == URL_TAG_JOB_SUBMIT && method == HTTP_REQUEST_POST)
+		rc = _op_handler_submit_job_post(context_id, method, parameters,
+						 query, tag, resp, errors);
+	else {
+		data_t *errord = data_list_append(errors);
+		data_set_dict(errord);
+		data_set_int(data_key_set(errord, "error_code"),
+			     ESLURM_INVALID_JOB_ID);
+		data_set_string(data_key_set(errord, "error"),
+				"unexpected HTTP method");
+
+		error("%s: [%s] job submission failed unexpected method:%s tag:%d",
+		      __func__, context_id,
+		      get_http_method_string(method), tag);
+
+		/* TODO: find better error */
+		rc = ESLURM_INVALID_JOB_ID;
+	}
+
+	return rc;
+}
+
+extern void init_op_jobs(void)
+{
+	lower_param_names = xcalloc(sizeof(char *), param_count);
+
+	if (!hcreate_r(param_count, &hash_params))
+		fatal("%s: unable to create hash table: %m",
+		      __func__);
+
+	/* populate hash table with all parameter names */
+	for (int i = 0; i < param_count; i++) {
+		ENTRY e = {
+			.key = xstrdup(job_params[i].param),
+			.data = (void *)&job_params[i],
+		};
+		ENTRY *re = NULL;
+
+		lower_param_names[i] = e.key;
+
+		/* force all lower characters */
+		xstrtolower(e.key);
+
+		if (!hsearch_r(e, ENTER, &re, &hash_params))
+			fatal("%s: unable to populate hash table: %m",
+			      __func__);
+	}
+
+	bind_operation_handler("/slurm/v0.0.38/job/submit",
+			       _op_handler_submit_job, URL_TAG_JOB_SUBMIT);
+	bind_operation_handler("/slurm/v0.0.38/jobs/", _op_handler_jobs,
+			       URL_TAG_JOBS);
+	bind_operation_handler("/slurm/v0.0.38/job/{job_id}", _op_handler_job,
+			       URL_TAG_JOB);
+}
+
+extern void destroy_op_jobs(void)
+{
+	hdestroy_r(&hash_params);
+	for (int i = 0; i < param_count; i++)
+		xfree(lower_param_names[i]);
+	xfree(lower_param_names);
+
+	unbind_operation_handler(_op_handler_submit_job);
+	unbind_operation_handler(_op_handler_job);
+	unbind_operation_handler(_op_handler_jobs);
+}
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/v0.0.38/LICENSE-openapi.json slurm-20.11.9/src/slurmrestd/plugins/openapi/v0.0.38/LICENSE-openapi.json
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/v0.0.38/LICENSE-openapi.json	1970-01-01 01:00:00.000000000 +0100
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/v0.0.38/LICENSE-openapi.json	2022-11-10 21:49:08.000000000 +0100
@@ -0,0 +1,19 @@
+The OpenAPI specification for Slurm's RESTful interface, as described in the
+openapi.json file in this directory, is made available under the following
+license terms:
+
+Under Slurm's license described in LICENSE.OpenSSL in the top level of the
+Slurm source, and generally referred to as "GPLv2+ with OpenSSL linking
+exception".
+
+Or, under the terms of The Apache License, Version 2.0, and abbreviated as
+"Apache 2.0" within the OpenAPI specification itself. A copy of this license is
+available at https://www.apache.org/licenses/LICENSE-2.0.html.
+
+As an additional requirement for changes to this specification: Contributors
+wishing to submit changes to these files must sign SchedMD's Contributor
+License Agreement ("CLA"). (https://slurm.schedmd.com/contributor.html).
+By signing the CLA, contributors are specifically licensing any changes,
+modifications, revisions, or edits to the OpenAPI specification for Slurms
+RESTful interface back to SchedMD to the maximum extent permitted by the
+relevant law and the CLA.
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/v0.0.38/Makefile.am slurm-20.11.9/src/slurmrestd/plugins/openapi/v0.0.38/Makefile.am
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/v0.0.38/Makefile.am	1970-01-01 01:00:00.000000000 +0100
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/v0.0.38/Makefile.am	2022-11-10 21:49:08.000000000 +0100
@@ -0,0 +1,45 @@
+# Makefile for openapi/v0.0.38 plugin
+
+AUTOMAKE_OPTIONS = foreign
+CLEANFILES = *.bino
+
+EXTRA_DIST = LICENSE-openapi.json
+
+REF = openapi.json
+
+PLUGIN_FLAGS = -module -avoid-version --export-dynamic
+
+AM_CPPFLAGS = -DSLURM_PLUGIN_DEBUG -I$(top_srcdir) -I$(top_srcdir)/src/common $(JSON_CPPFLAGS)
+
+BIN_REF = $(REF:.json=.bino)
+
+%.bino: %.json
+	$(AM_V_GEN)pushd $(abs_srcdir); $(LD) -r -o "$(abs_builddir)/$*.bino" -z noexecstack --format=binary "$(notdir $<)"; popd
+	$(AM_V_at)@OBJCOPY@ --rename-section .data=.rodata,alloc,load,readonly,data,contents "$*.bino"
+
+openapi_ref.lo: $(BIN_REF)
+	$(AM_V_at)echo "# $@ - a libtool object file" >"$@"
+	$(AM_V_at)echo "# Generated by $(shell @LIBTOOL@ --version | head -n 1)" >>"$@"
+	$(AM_V_at)echo "#" >>"$@"
+	$(AM_V_at)echo "# Please DO NOT delete this file!" >>"$@"
+	$(AM_V_at)echo "# It is necessary for linking the library." >>"$@"
+	$(AM_V_at)echo >>"$@"
+	$(AM_V_at)echo "# Name of the PIC object." >>"$@"
+	$(AM_V_at)echo "pic_object='$(BIN_REF)'" >>"$@"
+	$(AM_V_at)echo >>"$@"
+	$(AM_V_at)echo "# Name of the non-PIC object" >>"$@"
+	$(AM_V_at)echo "non_pic_object=''" >>"$@"
+	$(AM_V_at)echo >>"$@"
+
+libopenapi_ref_la_SOURCES =
+libopenapi_ref_la_DEPENDENCIES = openapi_ref.lo
+
+pkglib_LTLIBRARIES = openapi_v0_0_38.la
+noinst_LTLIBRARIES = libopenapi_ref.la
+
+openapi_v0_0_38_la_SOURCES = \
+	api.c api.h diag.c jobs.c nodes.c partitions.c reservations.c
+
+openapi_v0_0_38_la_DEPENDENCIES = $(LIB_SLURM_BUILD)
+openapi_v0_0_38_la_LDFLAGS = $(PLUGIN_FLAGS)
+openapi_v0_0_38_la_LIBADD = $(libslurmfull_la_LIBADD) openapi_ref.lo
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/v0.0.38/nodes.c slurm-20.11.9/src/slurmrestd/plugins/openapi/v0.0.38/nodes.c
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/v0.0.38/nodes.c	1970-01-01 01:00:00.000000000 +0100
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/v0.0.38/nodes.c	2022-11-25 15:16:56.544071286 +0100
@@ -0,0 +1,380 @@
+/*****************************************************************************\
+ *  nodes.c - Slurm REST API nodes http operations handlers
+ *****************************************************************************
+ *  Copyright (C) 2019-2020 SchedMD LLC.
+ *  Written by Nathan Rini <nate@schedmd.com>
+ *
+ *  This file is part of Slurm, a resource management program.
+ *  For details, see <https://slurm.schedmd.com/>.
+ *  Please also read the included file: DISCLAIMER.
+ *
+ *  Slurm is free software; you can redistribute it and/or modify it under
+ *  the terms of the GNU General Public License as published by the Free
+ *  Software Foundation; either version 2 of the License, or (at your option)
+ *  any later version.
+ *
+ *  In addition, as a special exception, the copyright holders give permission
+ *  to link the code of portions of this program with the OpenSSL library under
+ *  certain conditions as described in each individual source file, and
+ *  distribute linked combinations including the two. You must obey the GNU
+ *  General Public License in all respects for all of the code used other than
+ *  OpenSSL. If you modify file(s) with this exception, you may extend this
+ *  exception to your version of the file(s), but you are not obligated to do
+ *  so. If you do not wish to do so, delete this exception statement from your
+ *  version.  If you delete this exception statement from all source files in
+ *  the program, then also delete it here.
+ *
+ *  Slurm is distributed in the hope that it will be useful, but WITHOUT ANY
+ *  WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+ *  FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
+ *  details.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with Slurm; if not, write to the Free Software Foundation, Inc.,
+ *  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301  USA.
+\*****************************************************************************/
+
+#include "config.h"
+
+#define _GNU_SOURCE
+
+#include <search.h>
+#include <stdint.h>
+#include <unistd.h>
+
+#include "slurm/slurm.h"
+
+#include "src/common/data.h"
+#include "src/common/ref.h"
+#include "src/common/node_select.h"
+#include "src/common/uid.h"
+#include "src/common/xassert.h"
+#include "src/common/xmalloc.h"
+#include "src/common/xstring.h"
+#include "src/common/slurm_protocol_defs.h"
+
+#include "src/slurmrestd/operations.h"
+
+#include "src/slurmrestd/plugins/openapi/v0.0.38/api.h"
+
+typedef struct {
+	uint32_t flag;
+	const char *str;
+} node_state_flags_t;
+
+static const node_state_flags_t node_states[] = {
+	{ NODE_STATE_DOWN, "DOWN" },
+	{ NODE_STATE_IDLE, "IDLE" },
+	{ NODE_STATE_ALLOCATED, "ALLOCATED" },
+	{ NODE_STATE_ERROR, "ERROR" },
+	{ NODE_STATE_MIXED, "MIXED" },
+	{ NODE_STATE_FUTURE, "FUTURE" },
+	{ NODE_STATE_UNKNOWN, "UNKNOWN" },
+};
+
+static const node_state_flags_t node_state_flags[] = {
+	{ NODE_STATE_CLOUD, "CLOUD" },
+	{ NODE_STATE_COMPLETING, "COMPLETING" },
+	{ NODE_STATE_DRAIN, "DRAIN" },
+	{ NODE_STATE_FAIL, "FAIL" },
+	{ NODE_STATE_MAINT, "MAINTENANCE" },
+	{ NODE_STATE_POWER_UP, "POWER_UP" },
+	{ NODE_STATE_NET, "PERFCTRS" }, /* net performance counters */
+	{ NODE_STATE_REBOOT_ISSUED, "REBOOT_ISSUED" },
+	{ NODE_STATE_RES, "RESERVED" },
+	{ NODE_RESUME, "RESUME" },
+	{ NODE_STATE_NO_RESPOND, "NOT_RESPONDING" },
+	{ NODE_STATE_POWERING_DOWN, "POWERING_DOWN" },
+};
+
+static bool valid_base_state(uint32_t state)
+{
+	for (int i = 0; i < ARRAY_SIZE(node_states); i++) {
+		if (node_states[i].flag == (state & NODE_STATE_BASE))
+			return true;
+	}
+	return false;
+}
+
+static const char *node_state_base_string(uint32_t state)
+{
+	state &= NODE_STATE_BASE;
+
+	for (int i = 0; i < ARRAY_SIZE(node_states); i++)
+		if (node_states[i].flag == state)
+			return node_states[i].str;
+
+	return "INVALID";
+}
+
+static const char *node_state_flag_string_single(uint32_t *state)
+{
+	uint32_t flags = *state & NODE_STATE_FLAGS;
+
+	if (!flags)
+		return NULL;
+
+	for (int i = 0; i < ARRAY_SIZE(node_state_flags); i++) {
+		if (flags & node_state_flags[i].flag) {
+			*state &= ~node_state_flags[i].flag;
+			return node_state_flags[i].str;
+		}
+	}
+	/*
+	 * clear lowest flag bit, in order to guarantee that flags goes to 0 on
+	 * repeated calls. Any uncaught flags are unknown here.
+	 */
+	*state &= ~(flags & -flags);
+	return "?";
+}
+
+typedef enum {
+	URL_TAG_UNKNOWN = 0,
+	URL_TAG_NODE,
+	URL_TAG_NODES,
+} url_tag_t;
+
+static char *_get_long_node_state(uint32_t state)
+{
+	char *state_str = NULL;
+	/* caller must free */
+	state_str = xstrdup(node_state_base_string(state));
+	xstrtolower(state_str);
+	return state_str;
+}
+
+static void _add_node_state_flags(data_t *flags, uint32_t state)
+{
+	xassert(data_get_type(flags) == DATA_TYPE_LIST);
+
+	/* Only give flags if state is known */
+	if (!valid_base_state(state))
+		return;
+
+	const char *flag_str = NULL;
+	while ((flag_str = node_state_flag_string_single(&state))) {
+		data_set_string(data_list_append(flags),
+				flag_str);
+	}
+}
+
+static int _dump_node(data_t *p, node_info_t *node)
+{
+	int rc;
+	uint16_t alloc_cpus = 0;
+	uint64_t alloc_memory = 0;
+	char *node_alloc_tres = NULL;
+	double node_tres_weighted = 0;
+	data_t *d;
+
+	if (!node->name) {
+		debug2("%s: ignoring defunct node: %s",
+		       __func__, node->node_hostname);
+		return SLURM_SUCCESS;
+	}
+
+	d = data_set_dict(data_list_append(p));
+
+	data_set_string(data_key_set(d, "architecture"), node->arch);
+	data_set_string(data_key_set(d, "burstbuffer_network_address"),
+			node->bcast_address);
+	data_set_int(data_key_set(d, "boards"), node->boards);
+	data_set_int(data_key_set(d, "boot_time"), node->boot_time);
+	/* cluster_name intentionally omitted */
+	data_set_string(data_key_set(d, "comment"), node->comment);
+	data_set_int(data_key_set(d, "cores"), node->cores);
+	/* core_spec_cnt intentionally omitted */
+	data_set_int(data_key_set(d, "cpu_binding"), node->cpu_bind);
+	data_set_int(data_key_set(d, "cpu_load"), node->cpu_load);
+	//data_set_string(data_key_set(d, "extra"), node->extra);
+	data_set_int(data_key_set(d, "free_memory"), node->free_mem);
+	data_set_int(data_key_set(d, "cpus"), node->cpus);
+	//data_set_int(data_key_set(d, "last_busy"), node->last_busy);
+	/* cpu_spec_list intentionally omitted */
+	/* energy intentionally omitted */
+	/* ext_sensors intentionally omitted */
+	/* power intentionally omitted */
+	data_set_string(data_key_set(d, "features"), node->features);
+	data_set_string(data_key_set(d, "active_features"), node->features_act);
+	data_set_string(data_key_set(d, "gres"), node->gres);
+	data_set_string(data_key_set(d, "gres_drained"), node->gres_drain);
+	data_set_string(data_key_set(d, "gres_used"), node->gres_used);
+	data_set_string(data_key_set(d, "mcs_label"), node->mcs_label);
+	/* mem_spec_limit intentionally omitted */
+	data_set_string(data_key_set(d, "name"), node->name);
+	data_set_string_own(data_key_set(d, "next_state_after_reboot"),
+			    _get_long_node_state(node->next_state));
+	data_set_string(data_key_set(d, "address"), node->node_addr);
+	data_set_string(data_key_set(d, "hostname"), node->node_hostname);
+
+	data_set_string_own(data_key_set(d, "state"),
+			    _get_long_node_state(node->node_state));
+	_add_node_state_flags(data_set_list(data_key_set(d, "state_flags")),
+			      node->node_state);
+
+	data_set_string_own(data_key_set(d, "next_state_after_reboot"),
+			    _get_long_node_state(node->next_state));
+	_add_node_state_flags(
+		data_set_list(data_key_set(d, "next_state_after_reboot_flags")),
+		node->next_state);
+
+	data_set_string(data_key_set(d, "operating_system"), node->os);
+	if (node->owner == NO_VAL) {
+		data_set_null(data_key_set(d, "owner"));
+	} else {
+		data_set_string_own(data_key_set(d, "owner"),
+				    uid_to_string_or_null(node->owner));
+	}
+
+	if (node->partitions) {
+		data_t *p = data_set_list(data_key_set(d, "partitions"));
+		char *str = xstrdup(node->partitions);
+		char *save_ptr = NULL;
+		char *token = NULL;
+
+		/* API provides as a CSV list */
+		token = strtok_r(str, ",", &save_ptr);
+		while (token) {
+			data_set_string(data_list_append(p), token);
+			token = strtok_r(NULL, ",", &save_ptr);
+		}
+
+		xfree(str);
+	} else {
+		data_set_list(data_key_set(d, "partitions"));
+	}
+
+	data_set_int(data_key_set(d, "port"), node->port);
+	data_set_int(data_key_set(d, "real_memory"), node->real_memory);
+	data_set_string(data_key_set(d, "reason"), node->reason);
+	data_set_int(data_key_set(d, "reason_changed_at"), node->reason_time);
+	data_set_string_own(data_key_set(d, "reason_set_by_user"),
+			    uid_to_string_or_null(node->reason_uid));
+	data_set_int(data_key_set(d, "slurmd_start_time"), node->slurmd_start_time);
+	data_set_int(data_key_set(d, "sockets"), node->sockets);
+	data_set_int(data_key_set(d, "threads"), node->threads);
+	data_set_int(data_key_set(d, "temporary_disk"), node->tmp_disk);
+	data_set_int(data_key_set(d, "weight"), node->weight);
+	data_set_string(data_key_set(d, "tres"), node->tres_fmt_str);
+	data_set_string(data_key_set(d, "slurmd_version"), node->version);
+
+	/* Data from node->select_nodeinfo */
+	if ((rc = slurm_get_select_nodeinfo(
+		     node->select_nodeinfo, SELECT_NODEDATA_SUBCNT,
+		     NODE_STATE_ALLOCATED, &alloc_cpus))) {
+		error("%s: slurm_get_select_nodeinfo(%s, SELECT_NODEDATA_SUBCNT): %s",
+		      __func__, node->node_hostname, slurm_strerror(rc));
+		return rc;
+	}
+	if ((rc = slurm_get_select_nodeinfo(
+		     node->select_nodeinfo, SELECT_NODEDATA_MEM_ALLOC,
+		     NODE_STATE_ALLOCATED, &alloc_memory))) {
+		error("%s: slurm_get_select_nodeinfo(%s, SELECT_NODEDATA_MEM_ALLOC): %s",
+		      __func__, node->node_hostname, slurm_strerror(rc));
+		return rc;
+	}
+	if ((rc = select_g_select_nodeinfo_get(
+		     node->select_nodeinfo, SELECT_NODEDATA_TRES_ALLOC_FMT_STR,
+		     NODE_STATE_ALLOCATED, &node_alloc_tres))) {
+		error("%s: slurm_get_select_nodeinfo(%s, SELECT_NODEDATA_TRES_ALLOC_FMT_STR): %s",
+		      __func__, node->node_hostname, slurm_strerror(rc));
+		return rc;
+	}
+	if ((rc = select_g_select_nodeinfo_get(
+		     node->select_nodeinfo, SELECT_NODEDATA_TRES_ALLOC_WEIGHTED,
+		     NODE_STATE_ALLOCATED, &node_tres_weighted))) {
+		error("%s: slurm_get_select_nodeinfo(%s, SELECT_NODEDATA_TRES_ALLOC_WEIGHTED): %s",
+		      __func__, node->node_hostname, slurm_strerror(rc));
+		return rc;
+	}
+
+	data_set_int(data_key_set(d, "alloc_memory"), alloc_memory);
+	data_set_int(data_key_set(d, "alloc_cpus"), alloc_cpus);
+	data_set_int(data_key_set(d, "idle_cpus"), (node->cpus - alloc_cpus));
+	if (node_alloc_tres)
+		data_set_string_own(data_key_set(d, "tres_used"),
+				    node_alloc_tres);
+	else
+		data_set_null(data_key_set(d, "tres_used"));
+	data_set_float(data_key_set(d, "tres_weighted"), node_tres_weighted);
+
+	return SLURM_SUCCESS;
+}
+
+static int _op_handler_nodes(const char *context_id,
+			     http_request_method_t method, data_t *parameters,
+			     data_t *query, int tag, data_t *d, rest_auth_context_t *auth)
+{
+	int rc = SLURM_SUCCESS;
+	data_t *errors = populate_response_format(d);
+	data_t *nodes = data_set_list(data_key_set(d, "nodes"));
+	node_info_msg_t *node_info_ptr = NULL;
+	time_t update_time = 0;
+
+	if (tag == URL_TAG_NODES) {
+		if ((rc = get_date_param(query, "update_time", &update_time)))
+			goto done;
+		rc = slurm_load_node(update_time, &node_info_ptr,
+				     SHOW_ALL|SHOW_DETAIL|SHOW_MIXED);
+	} else if (tag == URL_TAG_NODE) {
+		const data_t *node_name = data_key_get_const(parameters,
+							     "node_name");
+		char *name = NULL;
+
+		if (!node_name || data_get_string_converted(node_name, &name))
+			rc = ESLURM_INVALID_NODE_NAME;
+		else
+			rc = slurm_load_node_single(&node_info_ptr, name,
+				SHOW_ALL|SHOW_DETAIL|SHOW_MIXED);
+
+		xfree(name);
+	} else
+		rc = SLURM_ERROR;
+
+	if (errno == SLURM_NO_CHANGE_IN_DATA) {
+		/* no-op: nothing to do here */
+		rc = errno;
+		goto done;
+	} else if (!rc && node_info_ptr && node_info_ptr->record_count) {
+		partition_info_msg_t *part_info_ptr = NULL;
+
+		if (!(rc = slurm_load_partitions(update_time, &part_info_ptr,
+						 SHOW_ALL))) {
+			slurm_populate_node_partitions(node_info_ptr,
+						       part_info_ptr);
+
+			slurm_free_partition_info_msg(part_info_ptr);
+		}
+
+		for (int i = 0; !rc && i < node_info_ptr->record_count; i++)
+			rc = _dump_node(nodes,
+					   &node_info_ptr->node_array[i]);
+	}
+
+	if (!rc && (!node_info_ptr || node_info_ptr->record_count == 0))
+		rc = ESLURM_INVALID_NODE_NAME;
+
+	if (rc) {
+		data_t *e = data_set_dict(data_list_append(errors));
+		data_set_string(data_key_set(e, "error"),
+				slurm_strerror(rc));
+		data_set_int(data_key_set(e, "errno"), rc);
+	}
+
+done:
+	slurm_free_node_info_msg(node_info_ptr);
+	return rc;
+}
+
+extern void init_op_nodes(void)
+{
+	bind_operation_handler("/slurm/v0.0.38/nodes/", _op_handler_nodes,
+			       URL_TAG_NODES);
+	bind_operation_handler("/slurm/v0.0.38/node/{node_name}",
+			       _op_handler_nodes, URL_TAG_NODE);
+}
+
+extern void destroy_op_nodes(void)
+{
+	unbind_operation_handler(_op_handler_nodes);
+}
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/v0.0.38/openapi.json slurm-20.11.9/src/slurmrestd/plugins/openapi/v0.0.38/openapi.json
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/v0.0.38/openapi.json	1970-01-01 01:00:00.000000000 +0100
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/v0.0.38/openapi.json	2022-11-10 21:49:08.000000000 +0100
@@ -0,0 +1,2640 @@
+{
+  "openapi": "3.0.2",
+  "info": {
+    "title": "Slurm Rest API",
+    "description": "API to access and control Slurm.",
+    "termsOfService": "https://github.com/SchedMD/slurm/blob/master/DISCLAIMER",
+    "contact": {
+      "name": "SchedMD LLC",
+      "url": "https://www.schedmd.com/",
+      "email": "sales@schedmd.com"
+    },
+    "license": {
+      "name": "Apache 2.0",
+      "url": "https://www.apache.org/licenses/LICENSE-2.0.html"
+    },
+    "version": "0.0.38"
+  },
+  "tags": [
+    {
+      "name": "slurm",
+      "description": "methods that query slurmctld"
+    },
+    {
+      "name": "openapi",
+      "description": "methods that query for OpenAPI specifications"
+    }
+  ],
+  "servers": [
+    {
+      "url": "/slurm/v0.0.38/"
+    }
+  ],
+  "security": [
+    {
+      "user": [],
+      "token": []
+    }
+  ],
+  "paths": {
+    "/diag/": {
+      "get": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "diag",
+        "summary": "get diagnostics",
+        "responses": {
+          "200": {
+            "description": "diagnostic results",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_diag"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_diag"
+                }
+              }
+            }
+          },
+          "default": {
+            "description": "unable to request ping test",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_errors"
+                }
+              }
+            }
+          }
+        }
+      }
+    },
+    "/licenses/": {
+      "get": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "slurmctld_get_licenses",
+        "summary": "get all Slurm tracked license info",
+        "responses": {
+          "200": {
+            "description": "results of get all licenses",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_licenses"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_licenses"
+                }
+              }
+            }
+          },
+          "default": {
+            "description": "unable to request licenses"
+          }
+        }
+      }
+    },
+    "/ping/": {
+      "get": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "ping",
+        "summary": "ping test",
+        "responses": {
+          "200": {
+            "description": "results of ping test",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_pings"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_pings"
+                }
+              }
+            }
+          },
+          "default": {
+            "description": "unable to request ping test",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_errors"
+                }
+              }
+            }
+          }
+        }
+      }
+    },
+    "/jobs/": {
+      "get": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "get_jobs",
+        "summary": "get list of jobs",
+        "parameters": [
+          {
+            "name": "update_time",
+            "in": "query",
+            "description": "Filter if changed since update_time. Use of this parameter can result in faster replies.",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "integer",
+              "format": "int64"
+            }
+          }
+        ],
+        "responses": {
+          "200": {
+            "description": "job(s) information",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_jobs_response"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_jobs_response"
+                }
+              }
+            }
+          },
+          "default": {
+            "description": "job not found",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_errors"
+                }
+              }
+            }
+          }
+        }
+      }
+    },
+    "/job/{job_id}": {
+      "get": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "get_job",
+        "summary": "get job info",
+        "parameters": [
+          {
+            "name": "job_id",
+            "in": "path",
+            "description": "Slurm JobID",
+            "required": true,
+            "style": "simple",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          }
+        ],
+        "responses": {
+          "200": {
+            "description": "job(s) information",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_jobs_response"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_jobs_response"
+                }
+              }
+            }
+          },
+          "default": {
+            "description": "job matching JobId not found"
+          }
+        }
+      },
+      "post": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "update_job",
+        "summary": "update job",
+        "parameters": [
+          {
+            "name": "job_id",
+            "in": "path",
+            "description": "Slurm Job ID",
+            "required": true,
+            "style": "simple",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          }
+        ],
+        "requestBody": {
+          "description": "update job",
+          "content": {
+            "application/json": {
+              "schema": {
+                "$ref": "#/components/schemas/v0.0.38_job_properties"
+              }
+            },
+            "application/x-yaml": {
+              "schema": {
+                "$ref": "#/components/schemas/v0.0.38_job_properties"
+              }
+            }
+          },
+          "required": true
+        },
+        "responses": {
+          "200": {
+            "description": "job information"
+          },
+          "500": {
+            "description": "job not found"
+          }
+        }
+      },
+      "delete": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "cancel_job",
+        "summary": "cancel or signal job",
+        "parameters": [
+          {
+            "name": "job_id",
+            "in": "path",
+            "description": "Slurm Job ID",
+            "required": true,
+            "style": "simple",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "name": "signal",
+            "in": "query",
+            "description": "signal to send to job",
+            "required": false,
+            "style": "form",
+            "explode": true,
+            "schema": {
+              "$ref": "#/components/schemas/v0.0.38_signal"
+            }
+          }
+        ],
+        "responses": {
+          "200": {
+            "description": "job cancelled or sent signal"
+          },
+          "500": {
+            "description": "job not found"
+          }
+        }
+      }
+    },
+    "/job/submit": {
+      "post": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "submit_job",
+        "summary": "submit new job",
+        "requestBody": {
+          "description": "submit new job",
+          "content": {
+            "application/json": {
+              "schema": {
+                "$ref": "#/components/schemas/v0.0.38_job_submission"
+              }
+            },
+            "application/x-yaml": {
+              "schema": {
+                "$ref": "#/components/schemas/v0.0.38_job_submission"
+              }
+            }
+          },
+          "required": true
+        },
+        "responses": {
+          "200": {
+            "description": "job submitted",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_job_submission_response"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_job_submission_response"
+                }
+              }
+            }
+          },
+          "default": {
+            "description": "job rejected",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_errors"
+                }
+              }
+            }
+          }
+        }
+      }
+    },
+    "/nodes/": {
+      "get": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "get_nodes",
+        "summary": "get all node info",
+        "parameters": [
+          {
+            "name": "update_time",
+            "in": "query",
+            "description": "Filter if changed since update_time. Use of this parameter can result in faster replies.",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "integer",
+              "format": "int64"
+            }
+          }
+        ],
+        "responses": {
+          "200": {
+            "description": "node information",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_nodes_response"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_nodes_response"
+                }
+              }
+            }
+          },
+          "default": {
+            "description": "no nodes in cluster",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_errors"
+                }
+              }
+            }
+          }
+        }
+      }
+    },
+    "/node/{node_name}": {
+      "get": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "get_node",
+        "summary": "get node info",
+        "parameters": [
+          {
+            "name": "node_name",
+            "in": "path",
+            "description": "Slurm Node Name",
+            "required": true,
+            "style": "simple",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          }
+        ],
+        "responses": {
+          "200": {
+            "description": "node information",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_nodes_response"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_nodes_response"
+                }
+              }
+            }
+          },
+          "default": {
+            "description": "node not found",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_errors"
+                }
+              }
+            }
+          }
+        }
+      }
+    },
+    "/partitions/": {
+      "get": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "get_partitions",
+        "summary": "get all partition info",
+        "parameters": [
+          {
+            "name": "update_time",
+            "in": "query",
+            "description": "Filter if changed since update_time. Use of this parameter can result in faster replies.",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "integer",
+              "format": "int64"
+            }
+          }
+        ],
+        "responses": {
+          "200": {
+            "description": "partition information",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_partitions_response"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_partitions_response"
+                }
+              }
+            }
+          },
+          "default": {
+            "description": "no partitions found",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_errors"
+                }
+              }
+            }
+          }
+        }
+      }
+    },
+    "/partition/{partition_name}": {
+      "get": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "get_partition",
+        "summary": "get partition info",
+        "parameters": [
+          {
+            "name": "partition_name",
+            "in": "path",
+            "description": "Slurm Partition Name",
+            "required": true,
+            "style": "simple",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "name": "update_time",
+            "in": "query",
+            "description": "Filter if there were no partition changes (not limited to partition in URL endpoint) since update_time.",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "integer",
+              "format": "int64"
+            }
+          }
+        ],
+        "responses": {
+          "200": {
+            "description": "partition information",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_partitions_response"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_partitions_response"
+                }
+              }
+            }
+          },
+          "default": {
+            "description": "no partitions found",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_errors"
+                }
+              }
+            }
+          }
+        }
+      }
+    },
+    "/reservations/": {
+      "get": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "get_reservations",
+        "summary": "get all reservation info",
+        "parameters": [
+          {
+            "name": "update_time",
+            "in": "query",
+            "description": "Filter if changed since update_time. Use of this parameter can result in faster replies.",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "integer",
+              "format": "int64"
+            }
+          }
+        ],
+        "responses": {
+          "200": {
+            "description": "reservation information",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_reservations_response"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_reservations_response"
+                }
+              }
+            }
+          },
+          "default": {
+            "description": "no reservations found",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_errors"
+                }
+              }
+            }
+          }
+        }
+      }
+    },
+    "/reservation/{reservation_name}": {
+      "get": {
+        "tags": [
+          "slurm"
+        ],
+        "operationId": "get_reservation",
+        "summary": "get reservation info",
+        "parameters": [
+          {
+            "name": "reservation_name",
+            "in": "path",
+            "description": "Slurm Reservation Name",
+            "required": true,
+            "style": "simple",
+            "explode": false,
+            "schema": {
+              "type": "string"
+            }
+          },
+          {
+            "name": "update_time",
+            "in": "query",
+            "description": "Filter if no reservation (not limited to reservation in URL) changed since update_time.",
+            "required": false,
+            "style": "form",
+            "explode": false,
+            "schema": {
+              "type": "integer",
+              "format": "int64"
+            }
+          }
+        ],
+        "responses": {
+          "200": {
+            "description": "reservation information",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_reservations_response"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_reservations_response"
+                }
+              }
+            }
+          },
+          "default": {
+            "description": "no reservations found",
+            "content": {
+              "application/json": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_errors"
+                }
+              },
+              "application/x-yaml": {
+                "schema": {
+                  "$ref": "#/components/schemas/v0.0.38_errors"
+                }
+              }
+            }
+          }
+        }
+      }
+    },
+    "/openapi.yaml": {
+      "get": {
+        "tags": [
+          "openapi"
+        ],
+        "summary": "Retrieve OpenAPI Specification",
+        "responses": {
+          "200": {
+            "description": "OpenAPI Specification"
+          }
+        }
+      },
+      "servers": [
+        {
+          "url": "/"
+        }
+      ]
+    },
+    "/openapi.json": {
+      "get": {
+        "tags": [
+          "openapi"
+        ],
+        "summary": "Retrieve OpenAPI Specification",
+        "responses": {
+          "200": {
+            "description": "OpenAPI Specification"
+          }
+        }
+      },
+      "servers": [
+        {
+          "url": "/"
+        }
+      ]
+    },
+    "/openapi": {
+      "get": {
+        "tags": [
+          "openapi"
+        ],
+        "summary": "Retrieve OpenAPI Specification",
+        "responses": {
+          "200": {
+            "description": "OpenAPI Specification"
+          }
+        }
+      },
+      "servers": [
+        {
+          "url": "/"
+        }
+      ]
+    },
+    "/openapi/v3": {
+      "get": {
+        "tags": [
+          "openapi"
+        ],
+        "summary": "Retrieve OpenAPI Specification",
+        "responses": {
+          "200": {
+            "description": "OpenAPI Specification"
+          }
+        }
+      },
+      "servers": [
+        {
+          "url": "/"
+        }
+      ]
+    }
+  },
+  "components": {
+    "securitySchemes": {
+      "user": {
+        "type": "apiKey",
+        "description": "User name",
+        "name": "X-SLURM-USER-NAME",
+        "in": "header"
+      },
+      "token": {
+        "type": "apiKey",
+        "description": "User access token",
+        "name": "X-SLURM-USER-TOKEN",
+        "in": "header"
+      }
+    },
+    "schemas": {
+      "v0.0.38_diag": {
+        "type": "object",
+        "properties": {
+          "meta": {
+            "$ref": "#/components/schemas/v0.0.38_meta"
+          },
+          "errors": {
+            "type": "array",
+            "description": "slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/v0.0.38_error"
+            }
+          },
+          "statistics": {
+            "type": "object",
+            "description": "Slurm statistics",
+            "properties": {
+              "parts_packed": {
+                "type": "integer",
+                "description": "partition records packed"
+              },
+              "req_time": {
+                "type": "integer",
+                "description": "generation time"
+              },
+              "req_time_start": {
+                "type": "integer",
+                "description": "data since"
+              },
+              "server_thread_count": {
+                "type": "integer",
+                "description": "Server thread count"
+              },
+              "agent_queue_size": {
+                "type": "integer",
+                "description": "Agent queue size"
+              },
+              "agent_count": {
+                "type": "integer",
+                "description": "Agent count"
+              },
+              "agent_thread_count": {
+                "type": "integer",
+                "description": "Agent thread count"
+              },
+              "dbd_agent_queue_size": {
+                "type": "integer",
+                "description": "DBD Agent queue size"
+              },
+              "gettimeofday_latency": {
+                "type": "integer",
+                "description": "Latency for 1000 calls to gettimeofday()"
+              },
+              "schedule_cycle_max": {
+                "type": "integer",
+                "description": "Main Schedule max cycle"
+              },
+              "schedule_cycle_last": {
+                "type": "integer",
+                "description": "Main Schedule last cycle"
+              },
+              "schedule_cycle_total": {
+                "type": "integer",
+                "description": "Main Schedule cycle iterations"
+              },
+              "schedule_cycle_mean": {
+                "type": "integer",
+                "description": "Average time for Schedule Max cycle"
+              },
+              "schedule_cycle_mean_depth": {
+                "type": "integer",
+                "description": "Average depth for Schedule Max cycle"
+              },
+              "schedule_cycle_per_minute": {
+                "type": "integer",
+                "description": "Main Schedule Cycles per minute"
+              },
+              "schedule_queue_length": {
+                "type": "integer",
+                "description": "Main Schedule Last queue length"
+              },
+              "jobs_submitted": {
+                "type": "integer",
+                "description": "Job submitted"
+              },
+              "jobs_started": {
+                "type": "integer",
+                "description": "Job started"
+              },
+              "jobs_completed": {
+                "type": "integer",
+                "description": "Job completed"
+              },
+              "jobs_canceled": {
+                "type": "integer",
+                "description": "Job cancelled"
+              },
+              "jobs_failed": {
+                "type": "integer",
+                "description": "Job failed"
+              },
+              "jobs_pending": {
+                "type": "integer",
+                "description": "Job pending"
+              },
+              "jobs_running": {
+                "type": "integer",
+                "description": "Job running"
+              },
+              "job_states_ts": {
+                "type": "integer",
+                "description": "Job states timestamp"
+              },
+              "bf_backfilled_jobs": {
+                "type": "integer",
+                "description": "Total backfilled jobs (since last slurm start)"
+              },
+              "bf_last_backfilled_jobs": {
+                "type": "integer",
+                "description": "Total backfilled jobs (since last stats cycle start)"
+              },
+              "bf_backfilled_het_jobs": {
+                "type": "integer",
+                "description": "Total backfilled heterogeneous job components"
+              },
+              "bf_cycle_counter": {
+                "type": "integer",
+                "description": "Backfill Schedule Total cycles"
+              },
+              "bf_cycle_mean": {
+                "type": "integer",
+                "description": "Backfill Schedule Mean cycle"
+              },
+              "bf_cycle_max": {
+                "type": "integer",
+                "description": "Backfill Schedule Max cycle time"
+              },
+              "bf_last_depth": {
+                "type": "integer",
+                "description": "Backfill Schedule Last depth cycle"
+              },
+              "bf_last_depth_try": {
+                "type": "integer",
+                "description": "Backfill Schedule Mean cycle (try sched)"
+              },
+              "bf_depth_mean": {
+                "type": "integer",
+                "description": "Backfill Schedule Depth Mean"
+              },
+              "bf_depth_mean_try": {
+                "type": "integer",
+                "description": "Backfill Schedule Depth Mean (try sched)"
+              },
+              "bf_cycle_last": {
+                "type": "integer",
+                "description": "Backfill Schedule Last cycle time"
+              },
+              "bf_queue_len": {
+                "type": "integer",
+                "description": "Backfill Schedule Last queue length"
+              },
+              "bf_queue_len_mean": {
+                "type": "integer",
+                "description": "Backfill Schedule Mean queue length"
+              },
+              "bf_table_size": {
+                "type": "integer",
+                "description": "Backfill Schedule Last table size"
+              },
+              "bf_table_size_mean": {
+                "type": "integer",
+                "description": "Backfill Schedule Mean table size"
+              },
+              "bf_when_last_cycle": {
+                "type": "integer",
+                "description": "Last cycle timestamp"
+              },
+              "bf_active": {
+                "type": "boolean",
+                "description": "Backfill Schedule currently active"
+              },
+              "rpcs_by_message_type": {
+                "type": "array",
+                "description": "Remote Procedure Call statistics by message type",
+                "items": {
+                  "$ref": "#/components/schemas/v0.0.38_diag_rpcm"
+                }
+              },
+              "rpcs_by_user": {
+                "type": "array",
+                "description": "Remote Procedure Call statistics by user",
+                "items": {
+                  "$ref": "#/components/schemas/v0.0.38_diag_rpcu"
+                }
+              }
+            }
+          }
+        }
+      },
+      "v0.0.38_diag_rpcm": {
+        "type": "object",
+        "properties": {
+          "message_type": {
+            "type": "string",
+            "description": "message type"
+          },
+          "type_id": {
+            "type": "integer",
+            "description": "message type id"
+          },
+          "count": {
+            "type": "integer",
+            "description": "rpc count"
+          },
+          "average_time": {
+            "type": "integer",
+            "description": "average time"
+          },
+          "total_time": {
+            "type": "integer",
+            "description": "total time"
+          }
+        }
+      },
+      "v0.0.38_diag_rpcu": {
+        "type": "object",
+        "properties": {
+          "user": {
+            "type": "string",
+            "description": "user"
+          },
+          "user_id": {
+            "type": "integer",
+            "description": "user id"
+          },
+          "count": {
+            "type": "integer",
+            "description": "rpc count"
+          },
+          "average_time": {
+            "type": "integer",
+            "description": "average time"
+          },
+          "total_time": {
+            "type": "integer",
+            "description": "total time"
+          }
+        }
+      },
+      "v0.0.38_licenses": {
+        "type": "object",
+        "properties": {
+          "errors": {
+            "type": "array",
+            "description": "slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/v0.0.38_error"
+            }
+          },
+          "licenses": {
+            "type": "array",
+            "description": "licenses info",
+            "items": {
+              "$ref": "#/components/schemas/v0.0.38_license"
+            }
+          }
+        }
+      },
+      "v0.0.38_license": {
+        "type": "object",
+        "properties": {
+          "LicenseName": {
+            "type": "string",
+            "description": "name of license"
+          },
+          "Total": {
+            "type": "integer",
+            "description": "total number of licenses"
+          },
+          "Used": {
+            "type": "integer",
+            "description": "number of licenses in use"
+          },
+          "Free": {
+            "type": "integer",
+            "description": "number of licenses available"
+          },
+          "Reserved": {
+            "type": "integer",
+            "description": "number of licenses reserved"
+          },
+          "Remote": {
+            "type": "boolean",
+            "description": "license is remote"
+          }
+        }
+      },
+      "v0.0.38_pings": {
+        "type": "object",
+        "properties": {
+          "meta": {
+            "$ref": "#/components/schemas/v0.0.38_meta"
+          },
+          "errors": {
+            "type": "array",
+            "description": "slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/v0.0.38_error"
+            }
+          },
+          "pings": {
+            "type": "array",
+            "description": "slurm controller pings",
+            "items": {
+              "$ref": "#/components/schemas/v0.0.38_ping"
+            }
+          }
+        }
+      },
+      "v0.0.38_ping": {
+        "type": "object",
+        "properties": {
+          "hostname": {
+            "type": "string",
+            "description": "slurm controller hostname"
+          },
+          "ping": {
+            "type": "string",
+            "description": "slurm controller host up",
+            "enum": [
+              "UP",
+              "DOWN"
+            ]
+          },
+          "mode": {
+            "type": "string",
+            "description": "slurm controller mode"
+          },
+          "status": {
+            "type": "integer",
+            "description": "slurm controller status"
+          }
+        }
+      },
+      "v0.0.38_partition": {
+        "type": "object",
+        "properties": {
+          "flags": {
+            "type": "array",
+            "description": "partition options",
+            "items": {
+              "type": "string"
+            }
+          },
+          "preemption_mode": {
+            "type": "array",
+            "description": "preemption type",
+            "items": {
+              "type": "string"
+            }
+          },
+          "allowed_allocation_nodes": {
+            "type": "string",
+            "description": "list names of allowed allocating nodes"
+          },
+          "allowed_accounts": {
+            "type": "string",
+            "description": "comma delimited list of accounts"
+          },
+          "allowed_groups": {
+            "type": "string",
+            "description": "comma delimited list of groups"
+          },
+          "allowed_qos": {
+            "type": "string",
+            "description": "comma delimited list of qos"
+          },
+          "alternative": {
+            "type": "string",
+            "description": "name of alternate partition"
+          },
+          "billing_weights": {
+            "type": "string",
+            "description": "TRES billing weights"
+          },
+          "default_memory_per_cpu": {
+            "type": "integer",
+            "format": "int64",
+            "description": "default MB memory per allocated CPU"
+          },
+          "default_time_limit": {
+            "type": "integer",
+            "format": "int64",
+            "description": "default time limit (minutes)"
+          },
+          "denied_accounts": {
+            "type": "string",
+            "description": "comma delimited list of denied accounts"
+          },
+          "denied_qos": {
+            "type": "string",
+            "description": "comma delimited list of denied qos"
+          },
+          "preemption_grace_time": {
+            "type": "integer",
+            "format": "int64",
+            "description": "preemption grace time (seconds)"
+          },
+          "maximum_cpus_per_node": {
+            "type": "integer",
+            "description": "maximum allocated CPUs per node"
+          },
+          "maximum_memory_per_node": {
+            "type": "integer",
+            "format": "int64",
+            "description": "maximum memory per allocated CPU (MiB)"
+          },
+          "maximum_nodes_per_job": {
+            "type": "integer",
+            "description": "Max nodes per job"
+          },
+          "max_time_limit": {
+            "type": "integer",
+            "format": "int64",
+            "description": "Max time limit per job"
+          },
+          "min_nodes_per_job": {
+            "type": "integer",
+            "description": "Min number of nodes per job"
+          },
+          "name": {
+            "type": "string",
+            "description": "Partition name"
+          },
+          "nodes": {
+            "type": "string",
+            "description": "list names of nodes in partition"
+          },
+          "over_time_limit": {
+            "type": "integer",
+            "description": "job's time limit can be exceeded by this number of minutes before cancellation"
+          },
+          "priority_job_factor": {
+            "type": "integer",
+            "description": "job priority weight factor"
+          },
+          "priority_tier": {
+            "type": "integer",
+            "description": "tier for scheduling and preemption"
+          },
+          "qos": {
+            "type": "string",
+            "description": "partition QOS name"
+          },
+          "state": {
+            "type": "string",
+            "description": "Partition state"
+          },
+          "total_cpus": {
+            "type": "integer",
+            "description": "Total cpus in partition"
+          },
+          "total_nodes": {
+            "type": "integer",
+            "description": "Total number of nodes in partition"
+          },
+          "tres": {
+            "type": "string",
+            "description": "configured TRES in partition"
+          }
+        }
+      },
+      "v0.0.38_partitions_response": {
+        "type": "object",
+        "properties": {
+          "meta": {
+            "$ref": "#/components/schemas/v0.0.38_meta"
+          },
+          "errors": {
+            "type": "array",
+            "description": "slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/v0.0.38_error"
+            }
+          },
+          "partitions": {
+            "type": "array",
+            "description": "partition info",
+            "items": {
+              "$ref": "#/components/schemas/v0.0.38_partition"
+            }
+          }
+        }
+      },
+      "v0.0.38_reservation": {
+        "type": "object",
+        "properties": {
+          "accounts": {
+            "type": "string",
+            "description": "Allowed accounts"
+          },
+          "burst_buffer": {
+            "type": "string",
+            "description": "Reserved burst buffer"
+          },
+          "core_count": {
+            "type": "integer",
+            "description": "Number of reserved cores"
+          },
+          "core_spec_cnt": {
+            "type": "integer",
+            "description": "Number of reserved specialized cores"
+          },
+          "end_time": {
+            "type": "integer",
+            "description": "End time of the reservation"
+          },
+          "features": {
+            "type": "string",
+            "description": "List of features"
+          },
+          "flags": {
+            "type": "array",
+            "description": "Reservation options",
+            "items": {
+              "type": "string"
+            }
+          },
+          "groups": {
+            "type": "string",
+            "description": "List of groups permitted to use the reserved nodes"
+          },
+          "licenses": {
+            "type": "string",
+            "description": "List of licenses"
+          },
+          "max_start_delay": {
+            "type": "integer",
+            "description": "Maximum delay in which jobs outside of the reservation will be permitted to overlap once any jobs are queued for the reservation"
+          },
+          "name": {
+            "type": "string",
+            "description": "Reservationn name"
+          },
+          "node_count": {
+            "type": "integer",
+            "description": "Count of nodes reserved"
+          },
+          "node_list": {
+            "type": "string",
+            "description": "List of reserved nodes"
+          },
+          "partition": {
+            "type": "string",
+            "description": "Partition"
+          },
+          "purge_completed": {
+            "type": "object",
+            "description": "If PURGE_COMP flag is set the amount of seconds this reservation will sit idle until it is revoked",
+            "properties": {
+              "time": {
+                "type": "integer",
+                "description": "amount of seconds this reservation will sit idle until it is revoked"
+              }
+            }
+          },
+          "start_time": {
+            "type": "integer",
+            "description": "Start time of reservation"
+          },
+          "watts": {
+            "type": "integer",
+            "description": "amount of power to reserve in watts"
+          },
+          "tres": {
+            "type": "string",
+            "description": "List of TRES"
+          },
+          "users": {
+            "type": "string",
+            "description": "List of users"
+          }
+        }
+      },
+      "v0.0.38_reservations_response": {
+        "type": "object",
+        "properties": {
+          "meta": {
+            "$ref": "#/components/schemas/v0.0.38_meta"
+          },
+          "errors": {
+            "type": "array",
+            "description": "slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/v0.0.38_error"
+            }
+          },
+          "reservations": {
+            "type": "array",
+            "description": "reservation info",
+            "items": {
+              "$ref": "#/components/schemas/v0.0.38_reservation"
+            }
+          }
+        }
+      },
+      "v0.0.38_errors": {
+        "type": "array",
+        "description": "Slurm errors",
+        "items": {
+          "$ref": "#/components/schemas/v0.0.38_error"
+        }
+      },
+      "v0.0.38_error": {
+        "type": "object",
+        "properties": {
+          "error": {
+            "type": "string",
+            "description": "error message"
+          },
+          "error_number": {
+            "description": "Slurm internal error number",
+            "type": "integer"
+          }
+        }
+      },
+      "v0.0.38_signal": {
+        "type": "string",
+        "description": "POSIX signal name",
+        "format": "int32",
+        "enum": [
+          "HUP",
+          "INT",
+          "QUIT",
+          "ABRT",
+          "KILL",
+          "ALRM",
+          "TERM",
+          "USR1",
+          "USR2",
+          "URG",
+          "CONT",
+          "STOP",
+          "TSTP",
+          "TTIN",
+          "TTOU"
+        ]
+      },
+      "v0.0.38_job_submission_response": {
+        "type": "object",
+        "properties": {
+          "meta": {
+            "$ref": "#/components/schemas/v0.0.38_meta"
+          },
+          "errors": {
+            "type": "array",
+            "description": "slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/v0.0.38_error"
+            }
+          },
+          "job_id": {
+            "description": "new job ID",
+            "type": "integer"
+          },
+          "step_id": {
+            "description": "new job step ID",
+            "type": "string"
+          },
+          "job_submit_user_msg": {
+            "description": "Message to user from job_submit plugin",
+            "type": "string"
+          }
+        }
+      },
+      "v0.0.38_job_submission": {
+        "required": [
+          "script"
+        ],
+        "properties": {
+          "script": {
+            "type": "string",
+            "description": "Executable script (full contents) to run in batch step"
+          },
+          "job": {
+            "$ref": "#/components/schemas/v0.0.38_job_properties"
+          },
+          "jobs": {
+            "description": "Properties of an HetJob",
+            "type": "array",
+            "items": {
+              "$ref": "#/components/schemas/v0.0.38_job_properties"
+            }
+          }
+        }
+      },
+      "v0.0.38_jobs_response": {
+        "type": "object",
+        "properties": {
+          "meta": {
+            "$ref": "#/components/schemas/v0.0.38_meta"
+          },
+          "errors": {
+            "type": "array",
+            "description": "slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/v0.0.38_error"
+            }
+          },
+          "jobs": {
+            "type": "array",
+            "description": "job descriptions",
+            "items": {
+              "$ref": "#/components/schemas/v0.0.38_job_response_properties"
+            }
+          }
+        }
+      },
+      "v0.0.38_job_response_properties": {
+        "type": "object",
+        "properties": {
+          "account": {
+            "type": "string",
+            "description": "Charge resources used by this job to specified account"
+          },
+          "accrue_time": {
+            "type": "integer",
+            "format": "int64",
+            "description": "time job is eligible for running"
+          },
+          "admin_comment": {
+            "type": "string",
+            "description": "administrator's arbitrary comment"
+          },
+          "array_job_id": {
+            "type": "integer",
+            "description": "job_id of a job array or 0 if N/A"
+          },
+          "array_task_id": {
+            "type": "integer",
+            "description": "task_id of a job array"
+          },
+          "array_max_tasks": {
+            "type": "integer",
+            "description": "Maximum number of running array tasks"
+          },
+          "array_task_string": {
+            "type": "string",
+            "description": "string expression of task IDs in this record"
+          },
+          "association_id": {
+            "type": "integer",
+            "description": "association id for job"
+          },
+          "batch_features": {
+            "type": "string",
+            "description": "features required for batch script's node"
+          },
+          "batch_flag": {
+            "type": "boolean",
+            "description": "if batch: queued job with script"
+          },
+          "batch_host": {
+            "type": "string",
+            "description": "name of host running batch script"
+          },
+          "flags": {
+            "type": "array",
+            "description": "Job flags",
+            "items": {
+              "type": "string"
+            }
+          },
+          "burst_buffer": {
+            "type": "string",
+            "description": "burst buffer specifications"
+          },
+          "burst_buffer_state": {
+            "type": "string",
+            "description": "burst buffer state info"
+          },
+          "cluster": {
+            "type": "string",
+            "description": "name of cluster that the job is on"
+          },
+          "cluster_features": {
+            "type": "string",
+            "description": "comma separated list of required cluster features"
+          },
+          "command": {
+            "type": "string",
+            "description": "command to be executed"
+          },
+          "comment": {
+            "type": "string",
+            "description": "arbitrary comment"
+          },
+          "container": {
+            "type": "string",
+            "description": "absolute path to OCI container bundle"
+          },
+          "contiguous": {
+            "type": "boolean",
+            "description": "job requires contiguous nodes"
+          },
+          "core_spec": {
+            "type": "string",
+            "description": "specialized core count"
+          },
+          "thread_spec": {
+            "type": "string",
+            "description": "specialized thread count"
+          },
+          "cores_per_socket": {
+            "type": "string",
+            "description": "cores per socket required by job"
+          },
+          "billable_tres": {
+            "type": "number",
+            "description": "billable TRES"
+          },
+          "cpus_per_task": {
+            "type": "string",
+            "description": "number of processors required for each task"
+          },
+          "cpu_frequency_minimum": {
+            "type": "string",
+            "description": "Minimum cpu frequency"
+          },
+          "cpu_frequency_maximum": {
+            "type": "string",
+            "description": "Maximum cpu frequency"
+          },
+          "cpu_frequency_governor": {
+            "type": "string",
+            "description": "cpu frequency governor"
+          },
+          "cpus_per_tres": {
+            "type": "string",
+            "description": "semicolon delimited list of TRES=# values"
+          },
+          "deadline": {
+            "type": "integer",
+            "description": "job start deadline "
+          },
+          "delay_boot": {
+            "type": "integer",
+            "description": "command to be executed"
+          },
+          "dependency": {
+            "type": "string",
+            "description": "synchronize job execution with other jobs"
+          },
+          "derived_exit_code": {
+            "type": "integer",
+            "description": "highest exit code of all job steps"
+          },
+          "eligible_time": {
+            "type": "integer",
+            "format": "int64",
+            "description": "time job is eligible for running"
+          },
+          "end_time": {
+            "type": "integer",
+            "format": "int64",
+            "description": "time of termination, actual or expected"
+          },
+          "excluded_nodes": {
+            "type": "string",
+            "description": "comma separated list of excluded nodes"
+          },
+          "exit_code": {
+            "type": "integer",
+            "description": "exit code for job"
+          },
+          "features": {
+            "type": "string",
+            "description": "comma separated list of required features"
+          },
+          "federation_origin": {
+            "type": "string",
+            "description": "Origin cluster's name"
+          },
+          "federation_siblings_active": {
+            "type": "string",
+            "description": "string of active sibling names"
+          },
+          "federation_siblings_viable": {
+            "type": "string",
+            "description": "string of viable sibling names"
+          },
+          "gres_detail": {
+            "type": "array",
+            "description": "Job flags",
+            "items": {
+              "type": "string"
+            }
+          },
+          "group_id": {
+            "type": "integer",
+            "description": "group job submitted as"
+          },
+          "job_id": {
+            "type": "integer",
+            "description": "job ID"
+          },
+          "job_resources": {
+            "$ref": "#/components/schemas/v0.0.38_job_resources"
+          },
+          "job_state": {
+            "type": "string",
+            "description": "state of the job"
+          },
+          "last_sched_evaluation": {
+            "type": "integer",
+            "description": "last time job was evaluated for scheduling"
+          },
+          "licenses": {
+            "type": "string",
+            "description": "licenses required by the job"
+          },
+          "max_cpus": {
+            "type": "integer",
+            "description": "maximum number of cpus usable by job"
+          },
+          "max_nodes": {
+            "type": "integer",
+            "description": "maximum number of nodes usable by job"
+          },
+          "mcs_label": {
+            "type": "string",
+            "description": "mcs_label if mcs plugin in use"
+          },
+          "memory_per_tres": {
+            "type": "string",
+            "description": "semicolon delimited list of TRES=# values"
+          },
+          "name": {
+            "type": "string",
+            "description": "name of the job"
+          },
+          "nodes": {
+            "type": "string",
+            "description": "list of nodes allocated to job"
+          },
+          "nice": {
+            "type": "integer",
+            "description": "requested priority change"
+          },
+          "tasks_per_core": {
+            "type": "integer",
+            "description": "number of tasks to invoke on each core"
+          },
+          "tasks_per_socket": {
+            "type": "integer",
+            "description": "number of tasks to invoke on each socket"
+          },
+          "tasks_per_board": {
+            "type": "integer",
+            "description": "number of tasks to invoke on each board"
+          },
+          "cpus": {
+            "type": "integer",
+            "description": "minimum number of cpus required by job"
+          },
+          "node_count": {
+            "type": "integer",
+            "description": "minimum number of nodes required by job"
+          },
+          "tasks": {
+            "type": "integer",
+            "description": "requested task count"
+          },
+          "het_job_id": {
+            "type": "integer",
+            "description": "job ID of hetjob leader"
+          },
+          "het_job_id_set": {
+            "type": "string",
+            "description": "job IDs for all components"
+          },
+          "het_job_offset": {
+            "type": "integer",
+            "description": "HetJob component offset from leader"
+          },
+          "partition": {
+            "type": "string",
+            "description": "name of assigned partition"
+          },
+          "memory_per_node": {
+            "type": "integer",
+            "description": "minimum real memory per node"
+          },
+          "memory_per_cpu": {
+            "type": "integer",
+            "description": "minimum real memory per cpu"
+          },
+          "minimum_cpus_per_node": {
+            "type": "integer",
+            "description": "minimum # CPUs per node"
+          },
+          "minimum_tmp_disk_per_node": {
+            "type": "integer",
+            "description": "minimum tmp disk per node"
+          },
+          "preempt_time": {
+            "type": "integer",
+            "format": "int64",
+            "description": "preemption signal time"
+          },
+          "pre_sus_time": {
+            "type": "integer",
+            "format": "int64",
+            "description": "time job ran prior to last suspend"
+          },
+          "priority": {
+            "type": "integer",
+            "description": "relative priority of the job"
+          },
+          "profile": {
+            "type": "array",
+            "description": "Job profiling requested",
+            "items": {
+              "type": "string"
+            }
+          },
+          "qos": {
+            "type": "string",
+            "description": "Quality of Service"
+          },
+          "reboot": {
+            "type": "boolean",
+            "description": "node reboot requested before start"
+          },
+          "required_nodes": {
+            "type": "string",
+            "description": "comma separated list of required nodes"
+          },
+          "requeue": {
+            "type": "boolean",
+            "description": "enable or disable job requeue option"
+          },
+          "resize_time": {
+            "type": "integer",
+            "format": "int64",
+            "description": "time of latest size change"
+          },
+          "restart_cnt": {
+            "type": "integer",
+            "description": "count of job restarts"
+          },
+          "resv_name": {
+            "type": "string",
+            "description": "reservation name"
+          },
+          "shared": {
+            "type": "string",
+            "description": "type and if job can share nodes with other jobs"
+          },
+          "show_flags": {
+            "type": "array",
+            "description": "details requested",
+            "items": {
+              "type": "string"
+            }
+          },
+          "sockets_per_board": {
+            "type": "integer",
+            "description": "sockets per board required by job"
+          },
+          "sockets_per_node": {
+            "type": "integer",
+            "description": "sockets per node required by job"
+          },
+          "start_time": {
+            "type": "integer",
+            "format": "int64",
+            "description": "time execution begins, actual or expected"
+          },
+          "state_description": {
+            "type": "string",
+            "description": "optional details for state_reason"
+          },
+          "state_reason": {
+            "type": "string",
+            "description": "reason job still pending or failed"
+          },
+          "standard_error": {
+            "type": "string",
+            "description": "pathname of job's stderr file"
+          },
+          "standard_input": {
+            "type": "string",
+            "description": "pathname of job's stdin file"
+          },
+          "standard_output": {
+            "type": "string",
+            "description": "pathname of job's stdout file"
+          },
+          "submit_time": {
+            "type": "integer",
+            "format": "int64",
+            "description": "time of job submission"
+          },
+          "suspend_time": {
+            "type": "integer",
+            "format": "int64",
+            "description": "time job last suspended or resumed"
+          },
+          "system_comment": {
+            "type": "string",
+            "description": "slurmctld's arbitrary comment"
+          },
+          "time_limit": {
+            "type": "integer",
+            "format": "int64",
+            "description": "maximum run time in minutes"
+          },
+          "time_minimum": {
+            "type": "integer",
+            "format": "int64",
+            "description": "minimum run time in minutes"
+          },
+          "threads_per_core": {
+            "type": "integer",
+            "description": "threads per core required by job"
+          },
+          "tres_bind": {
+            "type": "string",
+            "description": "Task to TRES binding directives"
+          },
+          "tres_freq": {
+            "type": "string",
+            "description": "TRES frequency directives"
+          },
+          "tres_per_job": {
+            "type": "string",
+            "description": "semicolon delimited list of TRES=# values"
+          },
+          "tres_per_node": {
+            "type": "string",
+            "description": "semicolon delimited list of TRES=# values"
+          },
+          "tres_per_socket": {
+            "type": "string",
+            "description": "semicolon delimited list of TRES=# values"
+          },
+          "tres_per_task": {
+            "type": "string",
+            "description": "semicolon delimited list of TRES=# values"
+          },
+          "tres_req_str": {
+            "type": "string",
+            "description": "tres reqeusted in the job"
+          },
+          "tres_alloc_str": {
+            "type": "string",
+            "description": "tres used in the job"
+          },
+          "user_id": {
+            "type": "integer",
+            "format": "int64",
+            "description": "user id the job runs as"
+          },
+          "user_name": {
+            "type": "string",
+            "description": "user the job runs as"
+          },
+          "wckey": {
+            "type": "string",
+            "description": "wckey for job"
+          },
+          "current_working_directory": {
+            "type": "string",
+            "description": "pathname of working directory"
+          }
+        }
+      },
+      "v0.0.38_job_resources": {
+        "type": "object",
+        "properties": {
+          "nodes": {
+            "type": "string",
+            "description": "list of assigned job nodes"
+          },
+          "allocated_cpus": {
+            "type": "integer",
+            "description": "number of assigned job cpus"
+          },
+          "allocated_hosts": {
+            "type": "integer",
+            "description": "number of assigned job hosts"
+          },
+          "allocated_nodes": {
+            "type": "array",
+            "items": {
+              "$ref": "#/components/schemas/v0.0.38_node_allocation"
+            },
+            "description": "array of allocated nodes"
+          }
+        }
+      },
+      "v0.0.38_node_allocation": {
+        "type": "object",
+        "properties": {
+          "memory": {
+            "type": "integer",
+            "description": "amount of assigned job memory"
+          },
+          "cpus": {
+            "type": "integer",
+            "description": "number of assigned job CPUs"
+          },
+          "sockets": {
+            "type": "object",
+            "description": "assignment status of each socket by numeric socket id",
+            "properties": {
+              "cores": {
+                "type": "object",
+                "description": "assignment status of each core by core id in each socket"
+              }
+            }
+          },
+          "nodename": {
+            "type": "string",
+            "description": "node name"
+          }
+        }
+      },
+      "v0.0.38_job_properties": {
+        "type": "object",
+        "required": [
+          "environment"
+        ],
+        "properties": {
+          "account": {
+            "type": "string",
+            "description": "Charge resources used by this job to specified account."
+          },
+          "account_gather_frequency": {
+            "type": "string",
+            "description": "Define the job accounting and profiling sampling intervals."
+          },
+          "argv": {
+            "type": "array",
+            "description": "Arguments to the script.",
+            "items": {
+              "type": "string"
+            }
+          },
+          "array": {
+            "type": "string",
+            "description": "Submit a job array, multiple jobs to be executed with identical parameters. The indexes specification identifies what array index values should be used."
+          },
+          "batch_features": {
+            "type": "string",
+            "description": "features required for batch script's node"
+          },
+          "begin_time": {
+            "type": "integer",
+            "format": "int64",
+            "description": "Submit the batch script to the Slurm controller immediately, like normal, but tell the controller to defer the allocation of the job until the specified time."
+          },
+          "burst_buffer": {
+            "type": "string",
+            "description": "Burst buffer specification."
+          },
+          "cluster_constraint": {
+            "type": "string",
+            "description": "Specifies features that a federated cluster must have to have a sibling job submitted to it."
+          },
+          "comment": {
+            "type": "string",
+            "description": "An arbitrary comment."
+          },
+          "constraints": {
+            "type": "string",
+            "description": "node features required by job."
+          },
+          "container": {
+            "type": "string",
+            "description": "absolute path to OCI container bundle"
+          },
+          "core_specification": {
+            "type": "integer",
+            "description": "Count of specialized threads per node reserved by the job for system operations and not used by the application."
+          },
+          "cores_per_socket": {
+            "type": "integer",
+            "description": "Restrict node selection to nodes with at least the specified number of cores per socket."
+          },
+          "cpu_binding": {
+            "type": "string",
+            "description": "Cpu binding"
+          },
+          "cpu_binding_hint": {
+            "type": "string",
+            "description": "Cpu binding hint"
+          },
+          "cpu_frequency": {
+            "type": "string",
+            "description": "Request that job steps initiated by srun commands inside this sbatch script be run at some requested frequency if possible, on the CPUs selected for the step on the compute node(s)."
+          },
+          "cpus_per_gpu": {
+            "type": "string",
+            "description": "Number of CPUs requested per allocated GPU."
+          },
+          "cpus_per_task": {
+            "type": "integer",
+            "description": "Advise the Slurm controller that ensuing job steps will require ncpus number of processors per task."
+          },
+          "current_working_directory": {
+            "type": "string",
+            "description": "Instruct Slurm to connect the batch script's standard output directly to the file name."
+          },
+          "deadline": {
+            "type": "string",
+            "description": "Remove the job if no ending is possible before this deadline (start > (deadline - time[-min]))."
+          },
+          "delay_boot": {
+            "type": "integer",
+            "description": "Do not reboot nodes in order to satisfied this job's feature specification if the job has been eligible to run for less than this time period."
+          },
+          "dependency": {
+            "type": "string",
+            "description": "Defer the start of this job until the specified dependencies have been satisfied completed."
+          },
+          "distribution": {
+            "type": "string",
+            "description": "Specify alternate distribution methods for remote processes."
+          },
+          "environment": {
+            "type": "object",
+            "description": "Dictionary of environment entries."
+          },
+          "exclusive": {
+            "type": "string",
+            "description": "The job allocation can share nodes just other users with the \"user\" option or with the \"mcs\" option).",
+            "enum": [
+              "user",
+              "mcs",
+              "true",
+              "false"
+            ]
+          },
+          "get_user_environment": {
+            "type": "boolean",
+            "description": "Load new login environment for user on job node."
+          },
+          "gres": {
+            "type": "string",
+            "description": "Specifies a comma delimited list of generic consumable resources."
+          },
+          "gres_flags": {
+            "type": "string",
+            "description": "Specify generic resource task binding options.",
+            "enum": [
+              "disable-binding",
+              "enforce-binding"
+            ]
+          },
+          "gpu_binding": {
+            "type": "string",
+            "description": "Requested binding of tasks to GPU."
+          },
+          "gpu_frequency": {
+            "type": "string",
+            "description": "Requested GPU frequency."
+          },
+          "gpus": {
+            "type": "string",
+            "description": "GPUs per job."
+          },
+          "gpus_per_node": {
+            "type": "string",
+            "description": "GPUs per node."
+          },
+          "gpus_per_socket": {
+            "type": "string",
+            "description": "GPUs per socket."
+          },
+          "gpus_per_task": {
+            "type": "string",
+            "description": "GPUs per task."
+          },
+          "hold": {
+            "type": "boolean",
+            "description": "Specify the job is to be submitted in a held state (priority of zero)."
+          },
+          "kill_on_invalid_dependency": {
+            "type": "boolean",
+            "description": "If a job has an invalid dependency, then Slurm is to terminate it."
+          },
+          "licenses": {
+            "type": "string",
+            "description": "Specification of licenses (or other resources available on all nodes of the cluster) which must be allocated to this job."
+          },
+          "mail_type": {
+            "type": "string",
+            "description": "Notify user by email when certain event types occur."
+          },
+          "mail_user": {
+            "type": "string",
+            "description": "User to receive email notification of state changes as defined by mail_type."
+          },
+          "mcs_label": {
+            "type": "string",
+            "description": "This parameter is a group among the groups of the user."
+          },
+          "memory_binding": {
+            "type": "string",
+            "description": "Bind tasks to memory."
+          },
+          "memory_per_cpu": {
+            "type": "integer",
+            "description": "Minimum real memory per cpu (MB)."
+          },
+          "memory_per_gpu": {
+            "type": "integer",
+            "description": "Minimum memory required per allocated GPU."
+          },
+          "memory_per_node": {
+            "type": "integer",
+            "description": "Minimum real memory per node (MB)."
+          },
+          "minimum_cpus_per_node": {
+            "type": "integer",
+            "description": "Minimum number of CPUs per node."
+          },
+          "minimum_nodes": {
+            "type": "boolean",
+            "description": "If a range of node counts is given, prefer the smaller count."
+          },
+          "name": {
+            "type": "string",
+            "description": "Specify a name for the job allocation."
+          },
+          "nice": {
+            "type": "string",
+            "description": "Run the job with an adjusted scheduling priority within Slurm."
+          },
+          "no_kill": {
+            "type": "boolean",
+            "description": "Do not automatically terminate a job if one of the nodes it has been allocated fails."
+          },
+          "nodes": {
+            "maxItems": 2,
+            "minItems": 1,
+            "type": "array",
+            "description": "Request that a minimum of minnodes nodes and a maximum node count.",
+            "items": {
+              "type": "integer"
+            }
+          },
+          "open_mode": {
+            "type": "string",
+            "description": "Open the output and error files using append or truncate mode as specified.",
+            "default": "append",
+            "enum": [
+              "append",
+              "truncate"
+            ]
+          },
+          "partition": {
+            "type": "string",
+            "description": "Request a specific partition for the resource allocation."
+          },
+          "prefer": {
+            "type": "string",
+            "description": "Comma delimited list of features for scheduler to prefer but not a strict requirement like a constraint. This value is not dumped but can be used for job submission."
+          },
+          "priority": {
+            "type": "string",
+            "description": "Request a specific job priority."
+          },
+          "qos": {
+            "type": "string",
+            "description": "Request a quality of service for the job."
+          },
+          "requeue": {
+            "type": "boolean",
+            "description": "Specifies that the batch job should eligible to being requeue."
+          },
+          "reservation": {
+            "type": "string",
+            "description": "Allocate resources for the job from the named reservation."
+          },
+          "signal": {
+            "pattern": "(B:|)sig_num(@sig_time|)",
+            "type": "string",
+            "description": "When a job is within sig_time seconds of its end time, send it the signal sig_num."
+          },
+          "sockets_per_node": {
+            "type": "integer",
+            "description": "Restrict node selection to nodes with at least the specified number of sockets."
+          },
+          "spread_job": {
+            "type": "boolean",
+            "description": "Spread the job allocation over as many nodes as possible and attempt to evenly distribute tasks across the allocated nodes."
+          },
+          "standard_error": {
+            "type": "string",
+            "description": "Instruct Slurm to connect the batch script's standard error directly to the file name."
+          },
+          "standard_input": {
+            "type": "string",
+            "description": "Instruct Slurm to connect the batch script's standard input directly to the file name specified."
+          },
+          "standard_output": {
+            "type": "string",
+            "description": "Instruct Slurm to connect the batch script's standard output directly to the file name."
+          },
+          "tasks": {
+            "type": "integer",
+            "description": "Advises the Slurm controller that job steps run within the allocation will launch a maximum of number tasks and to provide for sufficient resources."
+          },
+          "tasks_per_core": {
+            "type": "integer",
+            "description": "Request the maximum ntasks be invoked on each core."
+          },
+          "tasks_per_node": {
+            "type": "integer",
+            "description": "Request the maximum ntasks be invoked on each node."
+          },
+          "tasks_per_socket": {
+            "type": "integer",
+            "description": "Request the maximum ntasks be invoked on each socket."
+          },
+          "thread_specification": {
+            "type": "integer",
+            "description": "Count of specialized threads per node reserved by the job for system operations and not used by the application."
+          },
+          "threads_per_core": {
+            "type": "integer",
+            "description": "Restrict node selection to nodes with at least the specified number of threads per core."
+          },
+          "time_limit": {
+            "type": "integer",
+            "description": "Step time limit in minutes."
+          },
+          "time_minimum": {
+            "type": "integer",
+            "description": "Minimum run time in minutes."
+          },
+          "wait_all_nodes": {
+            "type": "boolean",
+            "description": "Do not begin execution until all nodes are ready for use."
+          },
+          "wckey": {
+            "type": "string",
+            "description": "Specify wckey to be used with job."
+          }
+        }
+      },
+      "v0.0.38_node": {
+        "type": "object",
+        "properties": {
+          "architecture": {
+            "type": "string",
+            "description": "computer architecture"
+          },
+          "burstbuffer_network_address": {
+            "type": "string",
+            "description": "BcastAddr"
+          },
+          "boards": {
+            "type": "integer",
+            "description": "total number of boards per node"
+          },
+          "boot_time": {
+            "type": "integer",
+            "format": "int64",
+            "description": "timestamp of node boot"
+          },
+          "cores": {
+            "type": "integer",
+            "description": "number of cores per socket"
+          },
+          "cpu_binding": {
+            "type": "integer",
+            "description": "Default task binding"
+          },
+          "cpu_load": {
+            "type": "integer",
+            "format": "int64",
+            "description": "CPU load * 100"
+          },
+          "free_memory": {
+            "type": "integer",
+            "description": "free memory in MiB"
+          },
+          "cpus": {
+            "type": "integer",
+            "description": "configured count of cpus running on the node"
+          },
+          "features": {
+            "type": "string",
+            "description": ""
+          },
+          "active_features": {
+            "type": "string",
+            "description": "list of a node's available features"
+          },
+          "gres": {
+            "type": "string",
+            "description": "list of a node's generic resources"
+          },
+          "gres_drained": {
+            "type": "string",
+            "description": "list of drained GRES"
+          },
+          "gres_used": {
+            "type": "string",
+            "description": "list of GRES in current use"
+          },
+          "mcs_label": {
+            "type": "string",
+            "description": "mcs label if mcs plugin in use"
+          },
+          "name": {
+            "type": "string",
+            "description": "node name to slurm"
+          },
+          "next_state_after_reboot": {
+            "type": "string",
+            "description": "state after reboot"
+          },
+          "next_state_after_reboot_flags": {
+            "type": "array",
+            "description": "node state flags",
+            "items": {
+              "type": "string"
+            }
+          },
+          "address": {
+            "type": "string",
+            "description": "state after reboot"
+          },
+          "hostname": {
+            "type": "string",
+            "description": "node's hostname"
+          },
+          "state": {
+            "type": "string",
+            "description": "current node state"
+          },
+          "state_flags": {
+            "type": "array",
+            "description": "node state flags",
+            "items": {
+              "type": "string"
+            }
+          },
+          "operating_system": {
+            "type": "string",
+            "description": "operating system"
+          },
+          "owner": {
+            "type": "string",
+            "description": "User allowed to use this node"
+          },
+          "partitions": {
+            "type": "array",
+            "description": "assigned partitions",
+            "items": {
+              "type": "string"
+            }
+          },
+          "port": {
+            "type": "integer",
+            "description": "TCP port number of the slurmd"
+          },
+          "real_memory": {
+            "type": "integer",
+            "description": "configured MB of real memory on the node"
+          },
+          "reason": {
+            "type": "string",
+            "description": "reason for node being DOWN or DRAINING"
+          },
+          "reason_changed_at": {
+            "type": "integer",
+            "description": "Time stamp when reason was set"
+          },
+          "reason_set_by_user": {
+            "type": "string",
+            "description": "User that set the reason"
+          },
+          "slurmd_start_time": {
+            "type": "integer",
+            "format": "int64",
+            "description": "timestamp of slurmd startup"
+          },
+          "sockets": {
+            "type": "integer",
+            "description": "total number of sockets per node"
+          },
+          "threads": {
+            "type": "integer",
+            "description": "number of threads per core"
+          },
+          "temporary_disk": {
+            "type": "integer",
+            "description": "configured MB of total disk in TMP_FS"
+          },
+          "weight": {
+            "type": "integer",
+            "description": "arbitrary priority of node for scheduling"
+          },
+          "tres": {
+            "type": "string",
+            "description": "TRES on node"
+          },
+          "tres_used": {
+            "type": "string",
+            "description": "TRES used on node"
+          },
+          "tres_weighted": {
+            "type": "number",
+            "format": "double",
+            "description": "TRES weight used on node"
+          },
+          "slurmd_version": {
+            "type": "string",
+            "description": "Slurmd version"
+          },
+          "alloc_cpus": {
+            "type": "integer",
+            "format": "int64",
+            "description": "Allocated CPUs"
+          },
+          "idle_cpus": {
+            "type": "integer",
+            "format": "int64",
+            "description": "Idle CPUs"
+          },
+          "alloc_memory": {
+            "type": "integer",
+            "format": "int64",
+            "description": "Allocated memory (MB)"
+          }
+        }
+      },
+      "v0.0.38_nodes_response": {
+        "type": "object",
+        "properties": {
+          "meta": {
+            "$ref": "#/components/schemas/v0.0.38_meta"
+          },
+          "errors": {
+            "type": "array",
+            "description": "slurm errors",
+            "items": {
+              "$ref": "#/components/schemas/v0.0.38_error"
+            }
+          },
+          "nodes": {
+            "type": "array",
+            "description": "nodes info",
+            "items": {
+              "$ref": "#/components/schemas/v0.0.38_node"
+            }
+          }
+        }
+      },
+      "v0.0.38_meta": {
+        "type": "object",
+        "properties": {
+          "plugin": {
+            "type": "object",
+            "properties": {
+              "type": {
+                "type": "string",
+                "description": ""
+              },
+              "name": {
+                "type": "string",
+                "description": ""
+              }
+            }
+          },
+          "Slurm": {
+            "type": "object",
+            "description": "Slurm information",
+            "properties": {
+              "version": {
+                "type": "object",
+                "properties": {
+                  "major": {
+                    "type": "string",
+                    "description": ""
+                  },
+                  "micro": {
+                    "type": "string",
+                    "description": ""
+                  },
+                  "minor": {
+                    "type": "string",
+                    "description": ""
+                  }
+                }
+              },
+              "release": {
+                "type": "string",
+                "description": "version specifier"
+              }
+            }
+          }
+        }
+      }
+    }
+  }
+}
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/v0.0.38/partitions.c slurm-20.11.9/src/slurmrestd/plugins/openapi/v0.0.38/partitions.c
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/v0.0.38/partitions.c	1970-01-01 01:00:00.000000000 +0100
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/v0.0.38/partitions.c	2022-11-25 15:16:56.544071286 +0100
@@ -0,0 +1,254 @@
+/*****************************************************************************\
+ *  partitions.c - Slurm REST API partitions http operations handlers
+ *****************************************************************************
+ *  Copyright (C) 2019-2020 SchedMD LLC.
+ *  Written by Nathan Rini <nate@schedmd.com>
+ *
+ *  This file is part of Slurm, a resource management program.
+ *  For details, see <https://slurm.schedmd.com/>.
+ *  Please also read the included file: DISCLAIMER.
+ *
+ *  Slurm is free software; you can redistribute it and/or modify it under
+ *  the terms of the GNU General Public License as published by the Free
+ *  Software Foundation; either version 2 of the License, or (at your option)
+ *  any later version.
+ *
+ *  In addition, as a special exception, the copyright holders give permission
+ *  to link the code of portions of this program with the OpenSSL library under
+ *  certain conditions as described in each individual source file, and
+ *  distribute linked combinations including the two. You must obey the GNU
+ *  General Public License in all respects for all of the code used other than
+ *  OpenSSL. If you modify file(s) with this exception, you may extend this
+ *  exception to your version of the file(s), but you are not obligated to do
+ *  so. If you do not wish to do so, delete this exception statement from your
+ *  version.  If you delete this exception statement from all source files in
+ *  the program, then also delete it here.
+ *
+ *  Slurm is distributed in the hope that it will be useful, but WITHOUT ANY
+ *  WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+ *  FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
+ *  details.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with Slurm; if not, write to the Free Software Foundation, Inc.,
+ *  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301  USA.
+\*****************************************************************************/
+
+#include "config.h"
+
+#define _GNU_SOURCE
+
+#include <search.h>
+#include <stdint.h>
+#include <unistd.h>
+
+#include "slurm/slurm.h"
+
+#include "src/common/ref.h"
+#include "src/common/xassert.h"
+#include "src/common/xmalloc.h"
+#include "src/common/xstring.h"
+
+#include "src/slurmrestd/operations.h"
+
+#include "src/slurmrestd/plugins/openapi/v0.0.38/api.h"
+
+typedef enum {
+	URL_TAG_UNKNOWN = 0,
+	URL_TAG_PARTITION,
+	URL_TAG_PARTITIONS,
+} url_tag_t;
+
+static int _dump_part(data_t *p, partition_info_t *part)
+{
+	data_t *d = data_set_dict(data_list_append(p));
+	data_t *flags = data_set_list(data_key_set(d, "flags"));
+	data_t *pm = data_set_list(data_key_set(d, "preemption_mode"));
+
+	data_set_string(data_key_set(d, "allowed_allocation_nodes"),
+			part->allow_alloc_nodes);
+	data_set_string(data_key_set(d, "allowed_accounts"),
+			part->allow_accounts);
+	data_set_string(data_key_set(d, "allowed_groups"), part->allow_groups);
+	data_set_string(data_key_set(d, "allowed_qos"), part->allow_qos);
+	data_set_string(data_key_set(d, "alternative"), part->alternate);
+	data_set_string(data_key_set(d, "billing_weights"),
+			part->billing_weights_str);
+
+	data_set_int(data_key_set(d, "default_memory_per_cpu"),
+		     part->def_mem_per_cpu);
+	if (part->default_time == INFINITE)
+		data_set_int(data_key_set(d, "default_time_limit"), -1);
+	if (part->default_time == NO_VAL)
+		data_set_null(data_key_set(d, "default_time_limit"));
+	else
+		data_set_int(data_key_set(d, "default_time_limit"),
+			     part->default_time);
+
+	data_set_string(data_key_set(d, "denied_accounts"),
+			part->deny_accounts);
+	data_set_string(data_key_set(d, "denied_qos"), part->deny_qos);
+
+	if (part->flags & PART_FLAG_DEFAULT)
+		data_set_string(data_list_append(flags), "default");
+	if (part->flags & PART_FLAG_HIDDEN)
+		data_set_string(data_list_append(flags), "hidden");
+	if (part->flags & PART_FLAG_NO_ROOT)
+		data_set_string(data_list_append(flags), "no_root");
+	if (part->flags & PART_FLAG_ROOT_ONLY)
+		data_set_string(data_list_append(flags), "root_only");
+	if (part->flags & PART_FLAG_REQ_RESV)
+		data_set_string(data_list_append(flags),
+				"reservation_required");
+	if (part->flags & PART_FLAG_LLN)
+		data_set_string(data_list_append(flags), "least_loaded_nodes");
+	if (part->flags & PART_FLAG_EXCLUSIVE_USER)
+		data_set_string(data_list_append(flags), "exclusive_user");
+
+	data_set_int(data_key_set(d, "preemption_grace_time"),
+		     part->grace_time);
+
+	if (part->max_cpus_per_node == INFINITE)
+		data_set_int(data_key_set(d, "maximum_cpus_per_node"), -1);
+	else if (part->max_cpus_per_node == NO_VAL)
+		data_set_null(data_key_set(d, "maximum_cpus_per_node"));
+	else
+		data_set_int(data_key_set(d, "maximum_cpus_per_node"),
+			     part->max_cpus_per_node);
+
+	data_set_int(data_key_set(d, "maximum_memory_per_node"),
+		     part->max_mem_per_cpu);
+
+	if (part->max_nodes == INFINITE)
+		data_set_int(data_key_set(d, "maximum_nodes_per_job"), -1);
+	else
+		data_set_int(data_key_set(d, "maximum_nodes_per_job"),
+			     part->max_nodes);
+
+	if (part->max_time == INFINITE)
+		data_set_int(data_key_set(d, "max_time_limit"), -1);
+	else
+		data_set_int(data_key_set(d, "max_time_limit"), part->max_time);
+	data_set_int(data_key_set(d, "min nodes per job"), part->min_nodes);
+	data_set_string(data_key_set(d, "name"), part->name);
+	// TODO: int32_t *node_inx;	/* list index pairs into node_table:
+	// 			 * start_range_1, end_range_1,
+	// 			 * start_range_2, .., -1  */
+	data_set_string(data_key_set(d, "nodes"), part->nodes);
+	if (part->over_time_limit == NO_VAL16)
+		data_set_null(data_key_set(d, "over_time_limit"));
+	else
+		data_set_int(data_key_set(d, "over_time_limit"),
+			     part->over_time_limit);
+
+	if ((part->preempt_mode == PREEMPT_MODE_OFF) ||
+	    (part->preempt_mode == NO_VAL16)) {
+		data_set_string(data_list_append(pm), "disabled");
+	} else {
+		if (part->preempt_mode & PREEMPT_MODE_SUSPEND)
+			data_set_string(data_list_append(pm), "suspend");
+		if (part->preempt_mode & PREEMPT_MODE_REQUEUE)
+			data_set_string(data_list_append(pm), "requeue");
+		if (part->preempt_mode & PREEMPT_MODE_GANG)
+			data_set_string(data_list_append(pm), "gang_schedule");
+		//if (part->preempt_mode & PREEMPT_MODE_WITHIN)
+		//	data_set_string(data_list_append(pm), "within");
+	}
+
+	data_set_int(data_key_set(d, "priority_job_factor"),
+		     part->priority_job_factor);
+	data_set_int(data_key_set(d, "priority_tier"), part->priority_tier);
+	data_set_string(data_key_set(d, "qos"), part->qos_char);
+	if (part->state_up == PARTITION_UP)
+		data_set_string(data_key_set(d, "state"), "UP");
+	else if (part->state_up == PARTITION_DOWN)
+		data_set_string(data_key_set(d, "state"), "DOWN");
+	else if (part->state_up == PARTITION_INACTIVE)
+		data_set_string(data_key_set(d, "state"), "INACTIVE");
+	else if (part->state_up == PARTITION_DRAIN)
+		data_set_string(data_key_set(d, "state"), "DRAIN");
+	else
+		data_set_string(data_key_set(d, "state"), "UNKNOWN");
+
+	data_set_int(data_key_set(d, "total_cpus"), part->total_cpus);
+	data_set_int(data_key_set(d, "total_nodes"), part->total_nodes);
+	data_set_string(data_key_set(d, "tres"), part->tres_fmt_str);
+
+	return SLURM_SUCCESS;
+}
+
+static int _op_handler_partitions(const char *context_id,
+				  http_request_method_t method,
+				  data_t *parameters, data_t *query,
+				  int tag, data_t *d, rest_auth_context_t *auth)
+{
+	int rc = SLURM_SUCCESS;
+	data_t *errors = populate_response_format(d);
+	data_t *partitions = data_set_list(data_key_set(d, "partitions"));
+	char *name = NULL;
+	partition_info_msg_t *part_info_ptr = NULL;
+	time_t update_time = 0;
+
+	if ((rc = get_date_param(query, "update_time", &update_time)))
+		goto done;
+
+	if (tag == URL_TAG_PARTITION) {
+		const data_t *part_name = data_key_get_const(parameters,
+							     "partition_name");
+		if (!part_name || data_get_string_converted(part_name, &name) ||
+		    !name)
+			rc = ESLURM_INVALID_PARTITION_NAME;
+	}
+
+	if (!rc)
+		rc = slurm_load_partitions(update_time, &part_info_ptr,
+					   SHOW_ALL);
+	if (errno == SLURM_NO_CHANGE_IN_DATA) {
+		/* no-op: nothing to do here */
+		rc = errno;
+		goto done;
+	} else if (!rc && part_info_ptr) {
+		int found = 0;
+		for (int i = 0; !rc && i < part_info_ptr->record_count; i++) {
+			if (tag == URL_TAG_PARTITIONS ||
+			    !xstrcasecmp(
+				    name,
+				    part_info_ptr->partition_array[i].name)) {
+				rc = _dump_part(
+					partitions,
+					&part_info_ptr->partition_array[i]);
+				found++;
+			}
+		}
+
+		if (!found)
+			rc = ESLURM_INVALID_PARTITION_NAME;
+	}
+
+	if (!rc && (!part_info_ptr || part_info_ptr->record_count == 0))
+		rc = ESLURM_INVALID_PARTITION_NAME;
+
+	if (rc) {
+		data_t *e = data_set_dict(data_list_append(errors));
+		data_set_string(data_key_set(e, "error"), slurm_strerror(rc));
+		data_set_int(data_key_set(e, "errno"), rc);
+	}
+
+done:
+	slurm_free_partition_info_msg(part_info_ptr);
+	xfree(name);
+	return rc;
+}
+
+extern void init_op_partitions(void)
+{
+	bind_operation_handler("/slurm/v0.0.38/partitions/",
+			       _op_handler_partitions, URL_TAG_PARTITIONS);
+	bind_operation_handler("/slurm/v0.0.38/partition/{partition_name}",
+			       _op_handler_partitions, URL_TAG_PARTITION);
+}
+
+extern void destroy_op_partitions(void)
+{
+	unbind_operation_handler(_op_handler_partitions);
+}
diff -N '--exclude=#*' '--exclude=*.bino' '--exclude=*~' '--exclude=*.l*' '--exclude=*.o' '--exclude=*.in' '--exclude=Makefile' '--exclude=*.deps' '--exclude=slurmrestd' -ur slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/v0.0.38/reservations.c slurm-20.11.9/src/slurmrestd/plugins/openapi/v0.0.38/reservations.c
--- slurm-20.11.9.orig/src/slurmrestd/plugins/openapi/v0.0.38/reservations.c	1970-01-01 01:00:00.000000000 +0100
+++ slurm-20.11.9/src/slurmrestd/plugins/openapi/v0.0.38/reservations.c	2022-11-25 15:16:56.544071286 +0100
@@ -0,0 +1,216 @@
+/*****************************************************************************\
+ *  reservations.c - Slurm REST API reservations http operations handlers
+ *****************************************************************************
+ *  Copyright (C) 2020 UT-Battelle, LLC.
+ *  Written by Matt Ezell <ezellma@ornl.gov>
+ *
+ *  This file is part of Slurm, a resource management program.
+ *  For details, see <https://slurm.schedmd.com/>.
+ *  Please also read the included file: DISCLAIMER.
+ *
+ *  Slurm is free software; you can redistribute it and/or modify it under
+ *  the terms of the GNU General Public License as published by the Free
+ *  Software Foundation; either version 2 of the License, or (at your option)
+ *  any later version.
+ *
+ *  In addition, as a special exception, the copyright holders give permission
+ *  to link the code of portions of this program with the OpenSSL library under
+ *  certain conditions as described in each individual source file, and
+ *  distribute linked combinations including the two. You must obey the GNU
+ *  General Public License in all respects for all of the code used other than
+ *  OpenSSL. If you modify file(s) with this exception, you may extend this
+ *  exception to your version of the file(s), but you are not obligated to do
+ *  so. If you do not wish to do so, delete this exception statement from your
+ *  version.  If you delete this exception statement from all source files in
+ *  the program, then also delete it here.
+ *
+ *  Slurm is distributed in the hope that it will be useful, but WITHOUT ANY
+ *  WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+ *  FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
+ *  details.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with Slurm; if not, write to the Free Software Foundation, Inc.,
+ *  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301  USA.
+\*****************************************************************************/
+
+#include "config.h"
+
+#define _GNU_SOURCE
+
+#include <search.h>
+#include <stdint.h>
+#include <unistd.h>
+
+#include "slurm/slurm.h"
+
+#include "src/common/ref.h"
+#include "src/common/xassert.h"
+#include "src/common/xmalloc.h"
+#include "src/common/xstring.h"
+
+#include "src/slurmrestd/operations.h"
+
+#include "src/slurmrestd/plugins/openapi/v0.0.38/api.h"
+
+typedef enum {
+	URL_TAG_UNKNOWN = 0,
+	URL_TAG_RESERVATION,
+	URL_TAG_RESERVATIONS,
+} url_tag_t;
+
+typedef struct {
+	uint64_t flag;
+	char *name;
+} res_flags_t;
+
+/* based on strings in reservation_flags_string() */
+static const res_flags_t res_flags[] = {
+	{ RESERVE_FLAG_MAINT, "MAINT" },
+	{ RESERVE_FLAG_NO_MAINT, "NO_MAINT" },
+	{ RESERVE_FLAG_FLEX, "FLEX" },
+	{ RESERVE_FLAG_OVERLAP, "OVERLAP" },
+	{ RESERVE_FLAG_IGN_JOBS, "IGNORE_JOBS" },
+	//{ RESERVE_FLAG_HOURLY, "HOURLY" },
+	//{ RESERVE_FLAG_NO_HOURLY, "NO_HOURLY" },
+	{ RESERVE_FLAG_DAILY, "DAILY" },
+	{ RESERVE_FLAG_NO_DAILY, "NO_DAILY" },
+	{ RESERVE_FLAG_WEEKDAY, "WEEKDAY" },
+	{ RESERVE_FLAG_WEEKEND, "WEEKEND" },
+	{ RESERVE_FLAG_WEEKLY, "WEEKLY" },
+	{ RESERVE_FLAG_NO_WEEKLY, "NO_WEEKLY" },
+	{ RESERVE_FLAG_SPEC_NODES, "SPEC_NODES" },
+	{ RESERVE_FLAG_ALL_NODES, "ALL_NODES" },
+	{ RESERVE_FLAG_ANY_NODES, "ANY_NODES" },
+	{ RESERVE_FLAG_NO_ANY_NODES, "NO_ANY_NODES" },
+	{ RESERVE_FLAG_STATIC, "STATIC" },
+	{ RESERVE_FLAG_NO_STATIC, "NO_STATIC" },
+	{ RESERVE_FLAG_PART_NODES, "PART_NODES" },
+	{ RESERVE_FLAG_NO_PART_NODES, "NO_PART_NODES" },
+	{ RESERVE_FLAG_FIRST_CORES, "FIRST_CORES" },
+	{ RESERVE_FLAG_TIME_FLOAT, "TIME_FLOAT" },
+	{ RESERVE_FLAG_REPLACE, "REPLACE" },
+	{ RESERVE_FLAG_REPLACE_DOWN, "REPLACE_DOWN" },
+	/* skipping RESERVE_FLAG_PURGE_COMP due to setting */
+	{ RESERVE_FLAG_NO_HOLD_JOBS, "NO_HOLD_JOBS_AFTER_END" },
+	{ RESERVE_FLAG_MAGNETIC, "MAGNETIC" },
+	{ RESERVE_FLAG_NO_MAGNETIC, "NO_MAGNETIC" },
+};
+
+static int _dump_res(data_t *p, reserve_info_t *res)
+{
+	data_t *d = data_set_dict(data_list_append(p));
+
+	data_t *flags = data_set_list(data_key_set(d, "flags"));
+	data_set_string(data_key_set(d, "accounts"), res->accounts);
+	data_set_string(data_key_set(d, "burst_buffer"), res->burst_buffer);
+	data_set_int(data_key_set(d, "core_count"), res->core_cnt);
+	data_set_int(data_key_set(d, "core_spec_cnt"), res->core_spec_cnt);
+	data_set_int(data_key_set(d, "end_time"), res->end_time);
+	data_set_string(data_key_set(d, "features"), res->features);
+
+	for (int i = 0; i < ARRAY_SIZE(res_flags); i++)
+		if (res->flags & res_flags[i].flag)
+			data_set_string(data_list_append(flags),
+					res_flags[i].name);
+
+	data_set_string(data_key_set(d, "groups"), res->groups);
+	data_set_string(data_key_set(d, "licenses"), res->licenses);
+	data_set_int(data_key_set(d, "max_start_delay"), res->max_start_delay);
+	data_set_string(data_key_set(d, "name"), res->name);
+	data_set_int(data_key_set(d, "node_count"), res->node_cnt);
+	/* skipping node_inx */
+	data_set_string(data_key_set(d, "node_list"), res->node_list);
+	data_set_string(data_key_set(d, "partition"), res->partition);
+
+	/* purgecomp is a flag with a time setting */
+	if (res->flags & RESERVE_FLAG_PURGE_COMP) {
+		data_t *pd = data_set_dict(data_key_set(d, "purge_completed"));
+		data_set_int(data_key_set(pd, "time"), res->purge_comp_time);
+	}
+
+	data_set_int(data_key_set(d, "start_time"), res->start_time);
+	data_set_int(data_key_set(d, "watts"), res->resv_watts);
+	data_set_string(data_key_set(d, "tres"), res->tres_str);
+	data_set_string(data_key_set(d, "users"), res->users);
+
+	return SLURM_SUCCESS;
+}
+
+static int _op_handler_reservations(const char *context_id,
+				    http_request_method_t method,
+				    data_t *parameters, data_t *query, int tag,
+				    data_t *d, rest_auth_context_t *auth)
+{
+	int rc = SLURM_SUCCESS;
+	data_t *errors = populate_response_format(d);
+	data_t *reservations = data_set_list(data_key_set(d, "reservations"));
+	char *name = NULL;
+	reserve_info_msg_t *res_info_ptr = NULL;
+	time_t update_time = 0;
+
+	if ((rc = get_date_param(query, "update_time", &update_time)))
+		goto done;
+
+	if (tag == URL_TAG_RESERVATION) {
+		const data_t *res_name = data_key_get_const(parameters,
+							    "reservation_name");
+		if (!res_name || data_get_string_converted(res_name, &name) ||
+		    !name)
+			rc = ESLURM_RESERVATION_INVALID;
+	}
+
+	if (!rc)
+		rc = slurm_load_reservations(update_time, &res_info_ptr);
+
+	if ((tag == URL_TAG_RESERVATION) &&
+	    (!res_info_ptr || (res_info_ptr->record_count == 0)))
+		rc = ESLURM_RESERVATION_INVALID;
+
+	if (errno == SLURM_NO_CHANGE_IN_DATA) {
+		/* no-op: nothing to do here */
+		rc = errno;
+		goto done;
+	} else if (!rc && res_info_ptr) {
+		int found = 0;
+
+		for (int i = 0; !rc && (i < res_info_ptr->record_count); i++) {
+			if ((tag == URL_TAG_RESERVATIONS) ||
+			    !xstrcasecmp(
+				    name,
+				    res_info_ptr->reservation_array[i].name)) {
+				rc = _dump_res(
+					reservations,
+					&res_info_ptr->reservation_array[i]);
+				found++;
+			}
+		}
+
+		if (!found && (tag == URL_TAG_RESERVATION))
+			rc = ESLURM_RESERVATION_INVALID;
+	}
+
+	if (rc) {
+		data_t *e = data_set_dict(data_list_append(errors));
+		data_set_string(data_key_set(e, "error"), slurm_strerror(rc));
+		data_set_int(data_key_set(e, "errno"), rc);
+	}
+
+done:
+	slurm_free_reservation_info_msg(res_info_ptr);
+	xfree(name);
+	return rc;
+}
+
+extern void init_op_reservations(void)
+{
+	bind_operation_handler("/slurm/v0.0.38/reservations/",
+			       _op_handler_reservations, URL_TAG_RESERVATIONS);
+	bind_operation_handler("/slurm/v0.0.38/reservation/{reservation_name}",
+			       _op_handler_reservations, URL_TAG_RESERVATION);
+}
+
+extern void destroy_op_reservations(void)
+{
+	unbind_operation_handler(_op_handler_reservations);
+}
--- slurm-20.11.9.orig/configure.ac	2022-05-04 21:32:38.000000000 +0200
+++ slurm-20.11.9/configure.ac	2022-11-22 13:44:25.255049849 +0100
@@ -454,6 +454,8 @@
 		 src/slurmrestd/plugins/openapi/v0.0.35/Makefile
 		 src/slurmrestd/plugins/openapi/v0.0.36/Makefile
 		 src/slurmrestd/plugins/openapi/dbv0.0.36/Makefile
+		 src/slurmrestd/plugins/openapi/v0.0.38/Makefile
+		 src/slurmrestd/plugins/openapi/dbv0.0.38/Makefile
 		 src/sprio/Makefile
 		 src/squeue/Makefile
 		 src/srun/Makefile
